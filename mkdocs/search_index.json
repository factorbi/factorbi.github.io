{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nBipost is a simple database synchronization tool for continually moving data from on-premises to AWS Aurora MySQL and back forward.\n\n\nCreated to keep your Windows databases on-premises while providing a way to extract, load \n transform specific sets of data to AWS Aurora. Two-way database synchronization also available, from AWS back to on-prem.\n\n\nSources:\n\n\n\n\nMicrosoft SQL Server\u00ae\n\n\nFirebird SQL\n\n\nDBF dBase III\n\n\n\n\nDestination:\n\n\n\n\n\n\nAmazon Aurora MySQL\n\n\n\n\n\n\n\n\n\n\n\n\nFully deployed on AWS with serverless technologies to scale at demand. Delivered as Software as a Service (SaaS).\n\n\n\n\n\n\nHow it works\n\n\n\n\nOn every sync Bipost Sync reads table schema's and data and uploads it to AWS.\n\n\nFully customize data sets to upload using \ncustomData.json\n\n\nDatabase and tables are created/altered if they don't exist, and data is loaded to Aurora-MySQL.\n\n\nBefore and after data is loaded to Aurora-MySQL you can transform data with stored procedures.\n\n\nUpload the same data multiple times and avoid duplicates. Data is replaced using primary keys.\n\n\nRun manually or automatically with a \nWindows Task schedule.\n\n\nUpload big datasets using \nRecursive Sync.\n\n\n\n\nData is also available as CSV files on S3 so you can use other AWS services like \nAthena\n and \nGlue\n to build your data lake.\n\n\nTwo-way synchronization\n\n\n\n\nSynchronize from Aurora-MySQL to on-premises SQL Server or Firebird SQL.\n\n\nSpecify data to download using \noutData.json\n\n\nInsert/update the returned data to your on-prem DB or just save the CSV files on Windows.\n\n\nTables schemas are created/altered on your on-prem DB if they don't exist.\n\n\nPrimary keys set on Aurora-MySQL are used against on-prem DB to avoid duplicates.\n\n\nBefore and after data is loaded to on-prem DB you can transform data with stored procedures.\n\n\nNot available for DBF dBase III on-prem.\n\n\nLearn more \nhere.\n\n\n\n\n\n\nUse Cases\n\n\n\n\nGreat way to consolidate information from separate databases and locations, e.g. merge your sales and inventory information from different branches.\n\n\nIdeal to extend your on-premises ERP to the cloud and build web applications, web services and API's on top of AWS cloud platform, with services such as API Gateway and Lambda.\n\n\nBuild Business Intelligence dashboards using \nGoogle Data Studio\n or \nAWS QuickSight\n, with a direct connection to Aurora-MySQL.\n\n\n\n\n\n\nPrivate Cloud\n\n\nWe care deeply about privacy.\n\n\nOur API links to your RDS instance on your AWS account, so you have full control of your databases.\n\n\nEach RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.\n\n\nArchitecture\n\n\n\n\nAurora\n is a MySQL compatible, fully managed database service built for the cloud with the performance and scalability of high-end commercial databases at 1/10th the cost.\n\n\n\n\nStart Using\n\n\n\n\n30 days free.\n\n\nUnlimited databases and synchronizations.\n\n\nNo need to provide credit card information.\n\n\n\n\n--\n Start here\n\n\nOr email us: \ninfo@factorbi.com\n\n\n\n\nPrices\n\n\nPricing here: \nwww.factorbi.com\n\n\nFirebird Community, \nMembers to Members Offer available.\n\n\nMembers of \nComunidad AWS en Espa\u00f1ol\n, ask for special deal.\n\n\n\n\nStaff Stories\n\n\nA journey from on-premises to Cloud Business Intelligence\n\n\n\n\nWhy we dropped Microsoft Power BI and embraced AWS QuickSight\n\n\n\n\n\n\nRelease Notes\n\n\n1.1.0 (GA) 2018-08-12\n\n\n\n\nDBF dBase III on-prem connection added.\n\n\nRun unlimited time stored procedures after data is uploaded to Aurora (previously limited to 5 minutes).\n\n\nImproved security for two-way synchronization.\n\n\nImproved support for special characters, e.g. now able to sync XML documents inside BLOB (Firebird) and TEXT (SQL Server) fields.\n\n\nFirst time customers: Automated creation of AWS services via \nCloudFormation template.\n\n\nBug fixes.\n\n\n\n\n1.0.0 (General Availability) 2018-03-02\n\n\n\n\nData upload is now done through secure HTTPS.\n\n\nJOIN parameter now supported for Firebird SQL.\n\n\nRecursive sync\n now supported for Firebird SQL.\n\n\nConnection information is now encrypted.\n\n\nBug fixes.\n\n\n\n\n0.5.6 (Beta) 2017-12-02\n\n\n\n\nBidirectional syncing is here!\n\n\nSynchronize to any AWS Region.\n\n\nPerformance improvements to API, now able to load nearly 1.5 million rows (or 280 MB uncompressed files) on a single call. Future releases will support much more since \nAWS Lambda recently doubled maximum memory capacity to 3 GB.\n\n\nFirebird transaction READ UNCOMMITTED to prevent Bipost Sync from being stopped while other transactions are still not committed.\n\n\nInitial and final statements on Aurora MySQL are disabled on recursive sync. This prevents excessive workload on your RDS instance.\n\n\nBug fixes to API and Bipost Sync.\n\n\n\n\n0.4.2 (Beta) 2017-09-16\n\n\n\n\nTable schemas are now synchronized against source definition on every sync, details \nhere.\n\n\nBipost Sync bug fixes.\n\n\nBipost API bug fixes.\n\n\n\n\n0.4.0 (Beta) 2017-08-20\n\n\n\n\nCustom connections added.\n\n\nInitial statement\n added to API.\n\n\nSpecial characters are deleted on string columns of 100 characters length or up.\n\n\n\n\n\n\nContact\n\n\nWe are always happy to hear about you.\n\n\nPlease send us an email to: \ninfo@factorbi.com", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Bipost is a simple database synchronization tool for continually moving data from on-premises to AWS Aurora MySQL and back forward.  Created to keep your Windows databases on-premises while providing a way to extract, load   transform specific sets of data to AWS Aurora. Two-way database synchronization also available, from AWS back to on-prem.  Sources:   Microsoft SQL Server\u00ae  Firebird SQL  DBF dBase III   Destination:    Amazon Aurora MySQL       Fully deployed on AWS with serverless technologies to scale at demand. Delivered as Software as a Service (SaaS).", 
            "title": "Overview"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "On every sync Bipost Sync reads table schema's and data and uploads it to AWS.  Fully customize data sets to upload using  customData.json  Database and tables are created/altered if they don't exist, and data is loaded to Aurora-MySQL.  Before and after data is loaded to Aurora-MySQL you can transform data with stored procedures.  Upload the same data multiple times and avoid duplicates. Data is replaced using primary keys.  Run manually or automatically with a  Windows Task schedule.  Upload big datasets using  Recursive Sync.   Data is also available as CSV files on S3 so you can use other AWS services like  Athena  and  Glue  to build your data lake.", 
            "title": "How it works"
        }, 
        {
            "location": "/#two-way-synchronization", 
            "text": "Synchronize from Aurora-MySQL to on-premises SQL Server or Firebird SQL.  Specify data to download using  outData.json  Insert/update the returned data to your on-prem DB or just save the CSV files on Windows.  Tables schemas are created/altered on your on-prem DB if they don't exist.  Primary keys set on Aurora-MySQL are used against on-prem DB to avoid duplicates.  Before and after data is loaded to on-prem DB you can transform data with stored procedures.  Not available for DBF dBase III on-prem.  Learn more  here.", 
            "title": "Two-way synchronization"
        }, 
        {
            "location": "/#use-cases", 
            "text": "Great way to consolidate information from separate databases and locations, e.g. merge your sales and inventory information from different branches.  Ideal to extend your on-premises ERP to the cloud and build web applications, web services and API's on top of AWS cloud platform, with services such as API Gateway and Lambda.  Build Business Intelligence dashboards using  Google Data Studio  or  AWS QuickSight , with a direct connection to Aurora-MySQL.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/#private-cloud", 
            "text": "We care deeply about privacy.  Our API links to your RDS instance on your AWS account, so you have full control of your databases.  Each RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.", 
            "title": "Private Cloud"
        }, 
        {
            "location": "/#architecture", 
            "text": "Aurora  is a MySQL compatible, fully managed database service built for the cloud with the performance and scalability of high-end commercial databases at 1/10th the cost.", 
            "title": "Architecture"
        }, 
        {
            "location": "/#start-using", 
            "text": "30 days free.  Unlimited databases and synchronizations.  No need to provide credit card information.   --  Start here  Or email us:  info@factorbi.com", 
            "title": "Start Using"
        }, 
        {
            "location": "/#prices", 
            "text": "Pricing here:  www.factorbi.com  Firebird Community,  Members to Members Offer available.  Members of  Comunidad AWS en Espa\u00f1ol , ask for special deal.", 
            "title": "Prices"
        }, 
        {
            "location": "/#staff-stories", 
            "text": "A journey from on-premises to Cloud Business Intelligence   Why we dropped Microsoft Power BI and embraced AWS QuickSight", 
            "title": "Staff Stories"
        }, 
        {
            "location": "/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/#110-ga-2018-08-12", 
            "text": "DBF dBase III on-prem connection added.  Run unlimited time stored procedures after data is uploaded to Aurora (previously limited to 5 minutes).  Improved security for two-way synchronization.  Improved support for special characters, e.g. now able to sync XML documents inside BLOB (Firebird) and TEXT (SQL Server) fields.  First time customers: Automated creation of AWS services via  CloudFormation template.  Bug fixes.", 
            "title": "1.1.0 (GA) 2018-08-12"
        }, 
        {
            "location": "/#100-general-availability-2018-03-02", 
            "text": "Data upload is now done through secure HTTPS.  JOIN parameter now supported for Firebird SQL.  Recursive sync  now supported for Firebird SQL.  Connection information is now encrypted.  Bug fixes.", 
            "title": "1.0.0 (General Availability) 2018-03-02"
        }, 
        {
            "location": "/#056-beta-2017-12-02", 
            "text": "Bidirectional syncing is here!  Synchronize to any AWS Region.  Performance improvements to API, now able to load nearly 1.5 million rows (or 280 MB uncompressed files) on a single call. Future releases will support much more since  AWS Lambda recently doubled maximum memory capacity to 3 GB.  Firebird transaction READ UNCOMMITTED to prevent Bipost Sync from being stopped while other transactions are still not committed.  Initial and final statements on Aurora MySQL are disabled on recursive sync. This prevents excessive workload on your RDS instance.  Bug fixes to API and Bipost Sync.", 
            "title": "0.5.6 (Beta) 2017-12-02"
        }, 
        {
            "location": "/#042-beta-2017-09-16", 
            "text": "Table schemas are now synchronized against source definition on every sync, details  here.  Bipost Sync bug fixes.  Bipost API bug fixes.", 
            "title": "0.4.2 (Beta) 2017-09-16"
        }, 
        {
            "location": "/#040-beta-2017-08-20", 
            "text": "Custom connections added.  Initial statement  added to API.  Special characters are deleted on string columns of 100 characters length or up.", 
            "title": "0.4.0 (Beta) 2017-08-20"
        }, 
        {
            "location": "/#contact", 
            "text": "We are always happy to hear about you.  Please send us an email to:  info@factorbi.com", 
            "title": "Contact"
        }, 
        {
            "location": "/easyawssetup/", 
            "text": "Link your AWS Account\n\n\nThis is the easiest and recommended way to link your AWS Account to Bipost API.\n\n\nIMPORTANT NOTICE: If you are planning to use Bipost Sync for production you may want to follow your IT department policies and use AWS security according to your needs.\n\n\n\n\nStep 1: Have an AWS Account?\n\n\nIf you don't have an AWS Account please proceed:\n\n\n\n\n\n\nCreate an AWS Account here \naws.amazon.com\n\n\n\n\n\n\n\n\nAWS usually makes an automated verification phone call, we suggest to provide a land line.\n\n\n\n\nProvide payment information.\n\n\nSelect Basic Support (free plan).\n\n\nCongrats you have an AWS account!\n\n\n\n\nNeed Help? \n--\n Write us.\n\n\n\n\nStep 2: Canonical User ID\n\n\nLogged in to AWS Account:\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nExpand Account Identifiers.\n\n\n\n\nCopy AWS Account ID (12-digit) and Canonical User ID (64-digit).\n\n\n\n\n\n\n\n\nEmail these numbers to \ninfo@factorbi.com\n so we can setup your dedicated Bucket.\n\n\n\n\n\n\nPlease stop here until you get a reply email from Factor BI.\n We will provide your \nbucket name\n which will be used on further steps.\n\n\n\n\nStep 3: Closest AWS Region\n\n\ncloudping.info\n\n\n\n\nClick the above link and hit \nHTTP Ping\n and look for the lowest latency.\n\n\nMaybe you want to try this at different times of the day.\n\n\nTake note of the closest region.\n\n\n\n\n\n\n\n\nStep 4: CloudFormation\n\n\nBased on the result from previous step, click the icon that is the closest Region to your location.\n\n\n\n\n\n\n\n\nAWS Region\n\n\nShort name\n\n\n\n\n\n\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\n\n\nUS East (Ohio)\n\n\nus-east-2\n\n\n\n\n\n\n\n\nUS West (California)\n\n\nus-west-1\n\n\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\n\n\nCanada (Central)\n\n\nca-central-1\n\n\n\n\n\n\n\n\nEurope (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\n\n\nEurope (London)\n\n\neu-west-2\n\n\n\n\n\n\n\n\nEurope (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\n\n\nEurope (Paris)\n\n\neu-west-3\n\n\n\n\n\n\n\n\nAsia Pacific (Mumbai)\n\n\nap-south-1\n\n\n\n\n\n\n\n\nAsia Pacific (Seoul)\n\n\nap-northeast-2\n\n\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\n\n\n\n\n4.1. Select Template\n\n\n\n\nThe template must be already selected, click \nNext\n lower-right blue button.\n\n\n\n\n\n\n4.2. Specify Details\n\n\n\n\nStack Name:\n this will be the prefix of all provisioned services.  Example: \nmycompany-prod\n\n\nBucketName:\n Paste the S3 bucket name that your received from Factor BI over email. It must look like this: \nbipostdata-123456789012\n\n\n\n\nDBAdminPassword:\n Type a complex password. Must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.\n\n\n\n\nPassword must be at least eight characters long. Can be any printable ASCII character except \"/\", \"\"\", or \"@\".\n\n\n\n\n\n\n\n\nDBAdminUsername:\n Database Admin Username, example: \nroot\n\n\n\n\nDBInstanceClass:\n for testing purposes select the smallest available, currently \ndb.t2.small\n\n\nEnvironment:\n Text to be included in the database cluster name.\n\n\nPublicSubnetACIDR:\n Leave default. Only modify the subnet address if multiple environments are needed, example: \n10.20.10.0/24\n\n\nPublicSubnetBCIDR:\n Leave default. Only modify the subnet address if multiple environments are needed, example: \n10.20.20.0/24\n\n\nSubnetsAZ:\n Select two availability zones to create the resources.\n\n\nVPCCIDR:\n Leave default. Only modify the address if multiple environment are needed, example: \n10.20.0.0/16\n\n\nClick \nNext\n, blue button blue button.\n\n\n\n\n\n\n4.3. Options\n\n\n\n\nLeave all defaults, many in blank.\n\n\nClick \nNext\n, blue button blue button lower right.\n\n\n\n\n4.4. Review\n\n\n\n\nCheck \nI acknowledge that AWS CloudFormation might create IAM resources with custom names.\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button. This will redirect to CloudFormation console home.\n\n\nClick refresh icon, upper right corner.\n\n\n\n\n\n\n4.5. Resources created\n\n\n\n\nCheck Stack Name.\n\n\nReview \nEvents\n tab while Status is CREATE_IN_PROGRESS.\n\n\nOnce Status is CREATE_COMPLETE review \nOutputs\n tab.\n\n\nYou may want to copy and save on a secure place all Outputs, as you will use them for further configuration.\n\n\n\n\nCome back any time and open again the Outputs tab.\n --\n From AWS Console Home, search for CloudFormation.\n\n\n\n\n\n\nStep 5: Add Role to Cluster\n\n\n\n\nOpen \nRDS console.\n\n\nClick \nClusters\n on left pane.\n\n\nClick radio button on your cluster.\n\n\n\n\nClick \nActions\n then \nManage IAM roles.\n\n\n\n\n\n\n\n\nUnder \nAdd IAM roles to this cluster\n select the DBRole created on Outputs tab, Step 4.5, and click \nAdd role\n button.\n\n\n\n\n\n\n\n\nWait until you see Status \nactive\n.\n\n\n\n\nClick \nDone\n.\n\n\n\n\n\n\nStep 6: Test MySQL Connection\n\n\n\n\n\n\nDownload MySQL Workbench\n and install on your machine.\n\n\n\n\n\n\nOpen MySQL Workbench and setup a new connection.\n\n\n\n\n\n\n\n\nCopy and paste from the \nOutputs\n tab (Step 4.5):\n\n\n\n\nConnection Name:\n type any name of your preference.\n\n\nConnection Method:\n \nStandard (TCP/IP)\n\n\nHostname:\n Paste the \nAuroraEndpoint\n string.\n\n\nPort:\n \n3306\n\n\nUsername:\n Paste the \nDBUser\n string\n\n\nClick \nTest Connection\n and when prompt type the \nDBAdminPassword\n you used on Step 4.2.\n\n\nIf you have a successful connection then you are good to go!\n\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Register at Factor BI\n\n\nClick and follow steps to \ncreate your account with Factor BI.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/easyawssetup/#link-your-aws-account", 
            "text": "This is the easiest and recommended way to link your AWS Account to Bipost API.  IMPORTANT NOTICE: If you are planning to use Bipost Sync for production you may want to follow your IT department policies and use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/easyawssetup/#step-1-have-an-aws-account", 
            "text": "If you don't have an AWS Account please proceed:    Create an AWS Account here  aws.amazon.com     AWS usually makes an automated verification phone call, we suggest to provide a land line.   Provide payment information.  Select Basic Support (free plan).  Congrats you have an AWS account!   Need Help?  --  Write us.", 
            "title": "Step 1: Have an AWS Account?"
        }, 
        {
            "location": "/easyawssetup/#step-2-canonical-user-id", 
            "text": "Logged in to AWS Account:   Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Expand Account Identifiers.   Copy AWS Account ID (12-digit) and Canonical User ID (64-digit).     Email these numbers to  info@factorbi.com  so we can setup your dedicated Bucket.    Please stop here until you get a reply email from Factor BI.  We will provide your  bucket name  which will be used on further steps.", 
            "title": "Step 2: Canonical User ID"
        }, 
        {
            "location": "/easyawssetup/#step-3-closest-aws-region", 
            "text": "cloudping.info   Click the above link and hit  HTTP Ping  and look for the lowest latency.  Maybe you want to try this at different times of the day.  Take note of the closest region.", 
            "title": "Step 3: Closest AWS Region"
        }, 
        {
            "location": "/easyawssetup/#step-4-cloudformation", 
            "text": "Based on the result from previous step, click the icon that is the closest Region to your location.     AWS Region  Short name       US East (N. Virginia)  us-east-1     US East (Ohio)  us-east-2     US West (California)  us-west-1     US West (Oregon)  us-west-2     Canada (Central)  ca-central-1     Europe (Ireland)  eu-west-1     Europe (London)  eu-west-2     Europe (Frankfurt)  eu-central-1     Europe (Paris)  eu-west-3     Asia Pacific (Mumbai)  ap-south-1     Asia Pacific (Seoul)  ap-northeast-2     Asia Pacific (Singapore)  ap-southeast-1     Asia Pacific (Sydney)  ap-southeast-2     Asia Pacific (Tokyo)  ap-northeast-1     South America (S\u00e3o Paulo)  sa-east-1", 
            "title": "Step 4: CloudFormation"
        }, 
        {
            "location": "/easyawssetup/#41-select-template", 
            "text": "The template must be already selected, click  Next  lower-right blue button.", 
            "title": "4.1. Select Template"
        }, 
        {
            "location": "/easyawssetup/#42-specify-details", 
            "text": "Stack Name:  this will be the prefix of all provisioned services.  Example:  mycompany-prod  BucketName:  Paste the S3 bucket name that your received from Factor BI over email. It must look like this:  bipostdata-123456789012   DBAdminPassword:  Type a complex password. Must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.   Password must be at least eight characters long. Can be any printable ASCII character except \"/\", \"\"\", or \"@\".     DBAdminUsername:  Database Admin Username, example:  root   DBInstanceClass:  for testing purposes select the smallest available, currently  db.t2.small  Environment:  Text to be included in the database cluster name.  PublicSubnetACIDR:  Leave default. Only modify the subnet address if multiple environments are needed, example:  10.20.10.0/24  PublicSubnetBCIDR:  Leave default. Only modify the subnet address if multiple environments are needed, example:  10.20.20.0/24  SubnetsAZ:  Select two availability zones to create the resources.  VPCCIDR:  Leave default. Only modify the address if multiple environment are needed, example:  10.20.0.0/16  Click  Next , blue button blue button.", 
            "title": "4.2. Specify Details"
        }, 
        {
            "location": "/easyawssetup/#43-options", 
            "text": "Leave all defaults, many in blank.  Click  Next , blue button blue button lower right.", 
            "title": "4.3. Options"
        }, 
        {
            "location": "/easyawssetup/#44-review", 
            "text": "Check  I acknowledge that AWS CloudFormation might create IAM resources with custom names.     Click  Create  blue button. This will redirect to CloudFormation console home.  Click refresh icon, upper right corner.", 
            "title": "4.4. Review"
        }, 
        {
            "location": "/easyawssetup/#45-resources-created", 
            "text": "Check Stack Name.  Review  Events  tab while Status is CREATE_IN_PROGRESS.  Once Status is CREATE_COMPLETE review  Outputs  tab.  You may want to copy and save on a secure place all Outputs, as you will use them for further configuration.   Come back any time and open again the Outputs tab.  --  From AWS Console Home, search for CloudFormation.", 
            "title": "4.5. Resources created"
        }, 
        {
            "location": "/easyawssetup/#step-5-add-role-to-cluster", 
            "text": "Open  RDS console.  Click  Clusters  on left pane.  Click radio button on your cluster.   Click  Actions  then  Manage IAM roles.     Under  Add IAM roles to this cluster  select the DBRole created on Outputs tab, Step 4.5, and click  Add role  button.     Wait until you see Status  active .   Click  Done .", 
            "title": "Step 5: Add Role to Cluster"
        }, 
        {
            "location": "/easyawssetup/#step-6-test-mysql-connection", 
            "text": "Download MySQL Workbench  and install on your machine.    Open MySQL Workbench and setup a new connection.     Copy and paste from the  Outputs  tab (Step 4.5):   Connection Name:  type any name of your preference.  Connection Method:   Standard (TCP/IP)  Hostname:  Paste the  AuroraEndpoint  string.  Port:   3306  Username:  Paste the  DBUser  string  Click  Test Connection  and when prompt type the  DBAdminPassword  you used on Step 4.2.  If you have a successful connection then you are good to go!", 
            "title": "Step 6: Test MySQL Connection"
        }, 
        {
            "location": "/easyawssetup/#step-7-register-at-factor-bi", 
            "text": "Click and follow steps to  create your account with Factor BI.", 
            "title": "Step 7: Register at Factor BI"
        }, 
        {
            "location": "/registration/", 
            "text": "Create your Account\n\n\nOpen \nFactor BI Console.\n \n\n\nClick \nCreate Your Account\n and follow steps.\n\n\nPassword must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.\n\n\n\n\n\n\nConfigure your first service\n\n\n\n\nLog in to the \nConsole.\n\n\nOn the left pane, go to \nService Numbers\n, click \nEdit\n then \nNew\n.\n\n\n\n\nEdit \nDatabase\n and \nDescription\n, then click \nEdit\n again to save.\n\n\nDatabase must be all lowercase. Can't contain special characters.\n\n\n\n\n\n\n\n\nGo to left pane \nRDS Instances\n and click under \nHostname\n.\n\n\nFill the following fields with the information you have on \nOutputs\n tab, \nStep 4.5\n: \n\n\n\n\nHostname or Endpoint\n\n\nUsername\n\n\nPassword\n\n\nPort\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Bipost Sync\n\n\nDownload Bipost Sync for Windows.", 
            "title": "Factor BI Console"
        }, 
        {
            "location": "/registration/#create-your-account", 
            "text": "Open  Factor BI Console.    Click  Create Your Account  and follow steps.  Password must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.", 
            "title": "Create your Account"
        }, 
        {
            "location": "/registration/#configure-your-first-service", 
            "text": "Log in to the  Console.  On the left pane, go to  Service Numbers , click  Edit  then  New .   Edit  Database  and  Description , then click  Edit  again to save.  Database must be all lowercase. Can't contain special characters.     Go to left pane  RDS Instances  and click under  Hostname .  Fill the following fields with the information you have on  Outputs  tab,  Step 4.5 :    Hostname or Endpoint  Username  Password  Port", 
            "title": "Configure your first service"
        }, 
        {
            "location": "/registration/#download-bipost-sync", 
            "text": "Download Bipost Sync for Windows.", 
            "title": "Download Bipost Sync"
        }, 
        {
            "location": "/installation/", 
            "text": "Prerequisites\n\n\nWindow XP \n Vista\n\n\nDownload and install: \n\n\n\n\nWindowsInstaller.\n\n\nPrerequisites for Windows 7 and 8.\n\n\n\n\nWindows 7 and 8\n\n\nFor most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.\n\n\n\n\nMicrosoft .NET Framework 4\n\n\nMicrosoft Visual C++ 2010\n\n\nVisual FoxPro OLEDB\n \n-- Only for DBF connection.\n\n\n\n\nDownload \n Install\n\n\n\n\n\n\nDownload latest version here.\n\n\n\n\n\n\nUnzip \nbiPost.zip\n to any folder on your Windows.\n\n\n\n\n\n\nRun \nbiPost.exe\n\n\n\n\n\n\nConfigure Bipost Sync", 
            "title": "Download Bipost"
        }, 
        {
            "location": "/installation/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/installation/#window-xp-vista", 
            "text": "Download and install:    WindowsInstaller.  Prerequisites for Windows 7 and 8.", 
            "title": "Window XP &amp; Vista"
        }, 
        {
            "location": "/installation/#windows-7-and-8", 
            "text": "For most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.   Microsoft .NET Framework 4  Microsoft Visual C++ 2010  Visual FoxPro OLEDB   -- Only for DBF connection.", 
            "title": "Windows 7 and 8"
        }, 
        {
            "location": "/installation/#download-install", 
            "text": "Download latest version here.    Unzip  biPost.zip  to any folder on your Windows.    Run  biPost.exe    Configure Bipost Sync", 
            "title": "Download &amp; Install"
        }, 
        {
            "location": "/bipostexe/", 
            "text": "Bipost Sync Settings\n\n\nIMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat these as sensitive information.\n\n\nFrom this point on you need your \nService No.\n and \nActivation No.\n available on your \nFactor BI Console.\n\n\n\n\nClick \nConfiguration\n and set:\n\n\n\n\nService No.:\n 36 digit hex number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit hex number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n Select \nFirebird\n or \nSQL\n (Microsoft SQL Server).\n\n\n\n\nSystem:\n Select \nCustom...\n\n\nIf you use \nMicrosip ERP\n or \nAspel SAE\n then select the System according.\n\n\n\n\n\n\n\n\nFirebird Connection\n\n\n\n\nDatabase:\n Location of your \n.FDB\n file.\n\n\nPassword:\n Set your Firebird password, sometimes \nmasterkey\n\n\n\n\nUse the following only when Bipost Sync is not located on the same server as the Firebird server.\n\n\n\n\nRemote Connection:\n Enable when biPost.exe is on a remote location on your LAN.\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nSQL Connection\n\n\n\n\n\n\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nUser:\n Login for your SQL server. It only needs read permissions.\n\n\n\n\n\n\nPassword:\n Password for the Login provided.\n\n\n\n\n\n\nDatabase:\n Name of your database.\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\n\n\n\n\nSpecific Bucket:\n Enable to use your own AWS Account.\n\n\n\n\nEnter your \nBucket Name\n that we provided over email.\n\n\nRemove \narn:aws:s3:::\n and just leave \nbipostdata-xx_my_number_xx\n, example:\n\n\n\n\nbipostdata-123456789012\n\n\n\n\n\n\n\nDownload Data:\n Enable to download data from AWS Aurora-MySQL to your on-premises.\n\n\n\n\n\n\nRecursive Sync\n\n\nWhen enabled it optimizes upload by extracting and uploading one day at a time for the given date range.\n\n\nVery useful to upload historic data or big data sets.\n\n\nIt is always used in combination with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\nWhen \nturned off\n it automatically sets \ntoday\n as start and end date, using your system clock.\n\n\n\n\ncustomData.json\n\n\nThis file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data sets from your on-prem DB.\n\n\nExample 1, using recursiveDateField:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente\",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]\n\n\n\nExample 2, for catalogs:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]\n\n\n\n\n\nThe above JSON will send all data for the specified tables, as \njoin\n and \nfilter\n are not in use.\n\n\nNote that you can leave \n\"active\": \"\",\n empty.\n\n\n\n\nExample 3, using special filter when no datetime is available:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym\",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0)))\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]\n\n\n\n\n\njoin\n and \nfilter\n can use any syntax supported on SQL Server/Firebird SQL.\n\n\n\n\nExample 4 with Microsip ERP:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"DOCTOS_ENTRE_SIS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]\n\n\n\n\n\nIn the above example Bipost Sync is using \nSystem: Microsip\n and on every sync it sends all the factory embedded tables plus the tables set in customData.json\n\n\n\n\nTables \n Primary Keys\n\n\nTake note of the following:\n\n\n\n\nEvery table specified in \ncustomData.json\n must have a \nPrimary Key\n and these must be listed on \n\"fields\"\n\n\nYou can only setup customData.json with \nbase tables.\n\n\nUse \n*\n on \nfields\n to retrieve all table columns.\n\n\n\"fields\"\n parameter must only include fields that exist on \n\"table\":\n\n\n\n\nrecursiveDateField\n\n\nThis parameter is used when \nRecursive Sync\n check box is enabled.\n\n\nIt only supports date fields without hour.\n\n\nOn SQL Server using \ndatetime\n data type, it only supports dates ending in \n00:00:00.000\n\n\nHandle multiple customData.json settings\n\n\nIt is very common to make changes to customData.json to upload different sets of data.\n\n\nUse Cases:\n\n\n\n\n\n\nSome tables may be uploaded once since that data is rarely changed, e.g. config \n company tables.\n\n\n\n\n\n\nHistoric data may be uploaded once, e.g. transactions from previous years.\n\n\n\n\n\n\nRecently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.\n\n\n\n\n\n\nRecently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.\n\n\n\n\n\n\nFor all these reasons it may be very useful to make copies of Bipost Sync folder and just change \ncustomData.json.\n Moreover you may want to have different sync schedules, which are explained next.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.", 
            "title": "Sync to AWS"
        }, 
        {
            "location": "/bipostexe/#bipost-sync-settings", 
            "text": "IMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat these as sensitive information.  From this point on you need your  Service No.  and  Activation No.  available on your  Factor BI Console.   Click  Configuration  and set:   Service No.:  36 digit hex number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit hex number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:  Select  Firebird  or  SQL  (Microsoft SQL Server).   System:  Select  Custom...  If you use  Microsip ERP  or  Aspel SAE  then select the System according.", 
            "title": "Bipost Sync Settings"
        }, 
        {
            "location": "/bipostexe/#firebird-connection", 
            "text": "Database:  Location of your  .FDB  file.  Password:  Set your Firebird password, sometimes  masterkey   Use the following only when Bipost Sync is not located on the same server as the Firebird server.   Remote Connection:  Enable when biPost.exe is on a remote location on your LAN.  Server:  IP or name of the server on your LAN network.", 
            "title": "Firebird Connection"
        }, 
        {
            "location": "/bipostexe/#sql-connection", 
            "text": "Server:  IP or name of the server on your LAN network.    User:  Login for your SQL server. It only needs read permissions.    Password:  Password for the Login provided.    Database:  Name of your database.", 
            "title": "SQL Connection"
        }, 
        {
            "location": "/bipostexe/#general-settings", 
            "text": "Specific Bucket:  Enable to use your own AWS Account.   Enter your  Bucket Name  that we provided over email.  Remove  arn:aws:s3:::  and just leave  bipostdata-xx_my_number_xx , example:   bipostdata-123456789012    Download Data:  Enable to download data from AWS Aurora-MySQL to your on-premises.", 
            "title": "General Settings"
        }, 
        {
            "location": "/bipostexe/#recursive-sync", 
            "text": "When enabled it optimizes upload by extracting and uploading one day at a time for the given date range.  Very useful to upload historic data or big data sets.  It is always used in combination with  customData.json  so you can configure the date field to use for each table.  When  turned off  it automatically sets  today  as start and end date, using your system clock.", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/bipostexe/#customdatajson", 
            "text": "This file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data sets from your on-prem DB.  Example 1, using recursiveDateField:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente\",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]  Example 2, for catalogs:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]   The above JSON will send all data for the specified tables, as  join  and  filter  are not in use.  Note that you can leave  \"active\": \"\",  empty.   Example 3, using special filter when no datetime is available:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym\",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0)))\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]   join  and  filter  can use any syntax supported on SQL Server/Firebird SQL.   Example 4 with Microsip ERP:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"DOCTOS_ENTRE_SIS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]   In the above example Bipost Sync is using  System: Microsip  and on every sync it sends all the factory embedded tables plus the tables set in customData.json", 
            "title": "customData.json"
        }, 
        {
            "location": "/bipostexe/#tables-primary-keys", 
            "text": "Take note of the following:   Every table specified in  customData.json  must have a  Primary Key  and these must be listed on  \"fields\"  You can only setup customData.json with  base tables.  Use  *  on  fields  to retrieve all table columns.  \"fields\"  parameter must only include fields that exist on  \"table\":", 
            "title": "Tables &amp; Primary Keys"
        }, 
        {
            "location": "/bipostexe/#recursivedatefield", 
            "text": "This parameter is used when  Recursive Sync  check box is enabled.  It only supports date fields without hour.  On SQL Server using  datetime  data type, it only supports dates ending in  00:00:00.000", 
            "title": "recursiveDateField"
        }, 
        {
            "location": "/bipostexe/#handle-multiple-customdatajson-settings", 
            "text": "It is very common to make changes to customData.json to upload different sets of data.  Use Cases:    Some tables may be uploaded once since that data is rarely changed, e.g. config   company tables.    Historic data may be uploaded once, e.g. transactions from previous years.    Recently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.    Recently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.    For all these reasons it may be very useful to make copies of Bipost Sync folder and just change  customData.json.  Moreover you may want to have different sync schedules, which are explained next.", 
            "title": "Handle multiple customData.json settings"
        }, 
        {
            "location": "/bipostexe/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/bipostexe/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/bipostexe/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/synctowindows/", 
            "text": "Download data sets to your on-prem\n\n\nIn many cases you may want to use Aurora-MySQL as a cloud database and make transactions with web applications. In this case it may be useful to download data sets from Aurora-MySQL to your on-premises.\n\n\n\n\n\n\n\n\nWith this option you are able to download data from Aurora to your on-premises Windows.\n\n\n\n\n\n\nData to retrieve from Aurora is specified on \noutData.json\n\n\n\n\n\n\nProcess Data is used to insert retuned data to SQL Server or Firebird SQL.\n\n\n\n\n\n\nIMPORTANT: Follow \nthis security steps\n to enable the downloading process.\n\n\n\n\noutData.json\n\n\n\n\n\n\nWith this file you are able to specify the tables, fields and filter criteria to query data to Aurora.\n\n\n\n\n\n\nTables are case-sensitive on Aurora MySQL, so this is important on \n\"table\":\n parameter.\n\n\n\n\n\n\nDownloaded data will be available on \n%localappdata%/biPost/out_\n Windows folder.\n\n\n\n\n\n\nExcept for \nrecursiveDateField\n all parameters are supported.\n\n\n\n\n\n\nWe highly recomend to prepare your data set on separate non-transactional tables, and process them with \nBipost API Final Statement.\n\n\nExample 1, using outData.json\n\n\n{\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}\n\n\n\nExample 2, using outData.json\n\n\n{\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}\n\n\n\n\n\nNote that you can leave \n\"active\": \"\",\n empty.\n\n\n\n\nExample of downloaded data on \n%localappdata%/bipost\n folder:\n\n\n\n\n\n\nProcess Data\n\n\n\n\n\n\nWhen this option is on, table schema's are created/altered and data uploaded to your SQL Server or Firebird SQL.\n\n\n\n\n\n\nSame names of output tables on Aurora are going to be created on SQL Server/Firebird.\n\n\n\n\n\n\nWe highly recommend to create specific output tables on Aurora and give them a name that is not in use on your SQL Server/Firebird database.\n\n\n\n\n\n\nPrimary keys on Aurora are used on SQL Server/Firebird to avoid duplicates.\n\n\n\n\n\n\nYou are able to query views from Aurora as output, in which case tables on SQL Server/Firebird are always deleted before any new data load.\n\n\n\n\n\n\nSchema changes on Aurora are applied to SQL Server/Firebird, except for deleting and renaming columns.\n\n\n\n\n\n\nYou're able to run queries before and after data is loaded to SQL Server/Firebird, more instructions below.\n\n\n\n\n\n\nFirebird SQL does not naturally support creating a column name starting with underscore,\n so avoid that on Aurora if your on-prem DB is Firebird.\n\n\n\n\n\n\nExample of processed data: tables were created and data loaded.\n\n\n\n\n\n\nInitial and Final Query\n\n\nBefore and after data is loaded to SQL Server/Firebird you're able to run queries, example:\n\n\n{\n      \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n      \"initialQuery\": \"execute procedure spInitial;\",\n      \"finalQuery\": \"execute procedure spFinal;\"\n}\n\n\n\ninitialQuery\n and \nfinalQuery\n are used to transform your data in any way you want.", 
            "title": "Sync back to Windows"
        }, 
        {
            "location": "/synctowindows/#download-data-sets-to-your-on-prem", 
            "text": "In many cases you may want to use Aurora-MySQL as a cloud database and make transactions with web applications. In this case it may be useful to download data sets from Aurora-MySQL to your on-premises.     With this option you are able to download data from Aurora to your on-premises Windows.    Data to retrieve from Aurora is specified on  outData.json    Process Data is used to insert retuned data to SQL Server or Firebird SQL.    IMPORTANT: Follow  this security steps  to enable the downloading process.", 
            "title": "Download data sets to your on-prem"
        }, 
        {
            "location": "/synctowindows/#outdatajson", 
            "text": "With this file you are able to specify the tables, fields and filter criteria to query data to Aurora.    Tables are case-sensitive on Aurora MySQL, so this is important on  \"table\":  parameter.    Downloaded data will be available on  %localappdata%/biPost/out_  Windows folder.    Except for  recursiveDateField  all parameters are supported.    We highly recomend to prepare your data set on separate non-transactional tables, and process them with  Bipost API Final Statement.  Example 1, using outData.json  {\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}  Example 2, using outData.json  {\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}   Note that you can leave  \"active\": \"\",  empty.   Example of downloaded data on  %localappdata%/bipost  folder:", 
            "title": "outData.json"
        }, 
        {
            "location": "/synctowindows/#process-data", 
            "text": "When this option is on, table schema's are created/altered and data uploaded to your SQL Server or Firebird SQL.    Same names of output tables on Aurora are going to be created on SQL Server/Firebird.    We highly recommend to create specific output tables on Aurora and give them a name that is not in use on your SQL Server/Firebird database.    Primary keys on Aurora are used on SQL Server/Firebird to avoid duplicates.    You are able to query views from Aurora as output, in which case tables on SQL Server/Firebird are always deleted before any new data load.    Schema changes on Aurora are applied to SQL Server/Firebird, except for deleting and renaming columns.    You're able to run queries before and after data is loaded to SQL Server/Firebird, more instructions below.    Firebird SQL does not naturally support creating a column name starting with underscore,  so avoid that on Aurora if your on-prem DB is Firebird.    Example of processed data: tables were created and data loaded.", 
            "title": "Process Data"
        }, 
        {
            "location": "/synctowindows/#initial-and-final-query", 
            "text": "Before and after data is loaded to SQL Server/Firebird you're able to run queries, example:  {\n      \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n      \"initialQuery\": \"execute procedure spInitial;\",\n      \"finalQuery\": \"execute procedure spFinal;\"\n}  initialQuery  and  finalQuery  are used to transform your data in any way you want.", 
            "title": "Initial and Final Query"
        }, 
        {
            "location": "/bipostapi/", 
            "text": "Bipost API\n\n\nThe API runs on AWS and has 4 main stages which are described next.\n\n\n1. Create-Alter Schemas\n\n\n\n\n\n\nIf it doesn't exist, database is created with:\n\n\n\n\n\n\nName: The one you specified on \nFactor BI Console.\n\n\n\n\n\n\nEncoding: \ncp1252 West European (latin1)\n\n\n\n\n\n\nCollation: \nlatin1_spanish_ci\n\n\n\n\n\n\n\n\n\n\nTables are created with the full set of fields found on source db.\n\n\n\n\n\n\nAlter tables to match source schemas. Columns are never deleted.\n\n\n\n\n\n\nFields will appear on a different position as the source db.\n\n\n\n\n\n\nOnly the fields specified in \ncustomData.json\n will be populated.\n\n\n\n\n\n\n\n\n2. Initial Statement\n\n\nAfter schemas are created/altered and \nbefore\n new data is loaded, you can specify to run a query against your database.\n\n\nThis is very useful if you need to delete, truncate or make any changes before data is loaded.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostInitial\n and must not have parameters.\n\n\nPlease send us an email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  INSERT INTO logPostInitial (message) VALUES ('auto');\n\n  TRUNCATE TABLE `mytesttable`;\n  TRUNCATE TABLE `bipostlog`;\n  TRUNCATE TABLE `movtipo`;\n\nEND$$\n\n\n\n\n\n3. Load Data\n\n\nData loading is performed by Aurora with a \nREPLACE\n parameter. Rows with the same primary key are updated and the rest inserted.\n\n\nYou can verify which tables where loaded by querying \naurora_s3_load_history\n table like this:\n\n\nselect * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';\n\n\n\nOptionally convert \nload_timestamp\n to your local time, e.g.: \nCONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')\n\n\n\n\n4. Final Statement\n\n\nAfter new data is loaded to your Aurora-MySQL database, and \nbefore\n it begins the downloading process to your on-prem, you can specify to run a query.\n\n\nThis is very handy if you need to execute several routines and, for example, populate new tables.\n\n\nIt is also very useful to prepare tables with data sets for the downloading process back to on-premises.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostFinal\n and must not have parameters.\n\n\nPlease send us an email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  REPLACE INTO dateInfo (tag, cDate)\n  SELECT 'yesterday', fnDateInfo('yesterday',fnServiceDate());\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\n  call spPopulateMyOtherTables;\n\n  INSERT INTO logPostFinal (message) VALUES ('auto');\n\nEND$$", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#bipost-api", 
            "text": "The API runs on AWS and has 4 main stages which are described next.", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#1-create-alter-schemas", 
            "text": "If it doesn't exist, database is created with:    Name: The one you specified on  Factor BI Console.    Encoding:  cp1252 West European (latin1)    Collation:  latin1_spanish_ci      Tables are created with the full set of fields found on source db.    Alter tables to match source schemas. Columns are never deleted.    Fields will appear on a different position as the source db.    Only the fields specified in  customData.json  will be populated.", 
            "title": "1. Create-Alter Schemas"
        }, 
        {
            "location": "/bipostapi/#2-initial-statement", 
            "text": "After schemas are created/altered and  before  new data is loaded, you can specify to run a query against your database.  This is very useful if you need to delete, truncate or make any changes before data is loaded.  Currently you need to include all desired statements in a stored procedure with the name  spPostInitial  and must not have parameters.  Please send us an email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  INSERT INTO logPostInitial (message) VALUES ('auto');\n\n  TRUNCATE TABLE `mytesttable`;\n  TRUNCATE TABLE `bipostlog`;\n  TRUNCATE TABLE `movtipo`;\n\nEND$$", 
            "title": "2. Initial Statement"
        }, 
        {
            "location": "/bipostapi/#3-load-data", 
            "text": "Data loading is performed by Aurora with a  REPLACE  parameter. Rows with the same primary key are updated and the rest inserted.  You can verify which tables where loaded by querying  aurora_s3_load_history  table like this:  select * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';  Optionally convert  load_timestamp  to your local time, e.g.:  CONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')", 
            "title": "3. Load Data"
        }, 
        {
            "location": "/bipostapi/#4-final-statement", 
            "text": "After new data is loaded to your Aurora-MySQL database, and  before  it begins the downloading process to your on-prem, you can specify to run a query.  This is very handy if you need to execute several routines and, for example, populate new tables.  It is also very useful to prepare tables with data sets for the downloading process back to on-premises.  Currently you need to include all desired statements in a stored procedure with the name  spPostFinal  and must not have parameters.  Please send us an email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  REPLACE INTO dateInfo (tag, cDate)\n  SELECT 'yesterday', fnDateInfo('yesterday',fnServiceDate());\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\n  call spPopulateMyOtherTables;\n\n  INSERT INTO logPostFinal (message) VALUES ('auto');\n\nEND$$", 
            "title": "4. Final Statement"
        }, 
        {
            "location": "/dbase/", 
            "text": "POSitouch Use Case\n\n\nYou can export DBF to MySQL on AWS using Bipost continuous synchronization.\n\n\nA popular use case is \nPOSitouch\n Point of Sale system that works with .DBF files. You may extract DBF data from several POSitouch restaurants and consolidate them on AWS at scale.\n\n\n\n\nPOSitouch DBF database documentation download.\n\n\nMake POSitouch reporting simple. Create KPI's, Dashboards and Business Intelligence analytics using \nGoogle Data Studio\n or \nAWS QuickSight\n with a seamless connection to Aurora-MySQL.", 
            "title": "dBase III to MySQL"
        }, 
        {
            "location": "/dbase/#positouch-use-case", 
            "text": "You can export DBF to MySQL on AWS using Bipost continuous synchronization.  A popular use case is  POSitouch  Point of Sale system that works with .DBF files. You may extract DBF data from several POSitouch restaurants and consolidate them on AWS at scale.   POSitouch DBF database documentation download.  Make POSitouch reporting simple. Create KPI's, Dashboards and Business Intelligence analytics using  Google Data Studio  or  AWS QuickSight  with a seamless connection to Aurora-MySQL.", 
            "title": "POSitouch Use Case"
        }, 
        {
            "location": "/aspel/", 
            "text": "Aspel SAE\n\n\nAspel SAE es un sistema administrativo que fabrica la empresa \nAspel de M\u00e9xico, S.A. de C.V.\n\n\nBipost Sync ofrece una conexi\u00f3n simple a \nGoogle Data Studio\n para crear tus tableros de mando, dashboards e indicadores de gesti\u00f3n.\n\n\n\n\nCasos de Uso\n\n\nFolleto Casos de Uso: \nTableros de Mando\n\n\n\n\n\n\nTablas Aspel\n\n\n\n\nAl configurar Bipost Sync con \nSystem: SAE\n, de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:\n\n\nalmacenes\nclie\nclin\ncolor\ncompc\ncompd\ncompo\nconc\nconm\nconp\ncuen_det\ncuen_m\nfactd\nfactf\nfactp\ninve\nminve\nmoned\npaga_det\npaga_m\npar_compc\npar_compd\npar_compo\npar_factd\npar_factf\npar_factp\nprov\nprvprod\ntalla\nvend\n\n\n\nEn la base de datos del sistema Aspel SAE cada tabla tiene un sufijo \n01\n, \n02\n, etc., de acuerdo a la empresa que se ha creado. Por ejemplo la tabla de las facturas podr\u00edas encontrarla como \nfactf01\n y para otra raz\u00f3n social como \nfactf02\n.\n\n\nBipost Sync autom\u00e1ticamente elimina estos sufijos por lo que las tablas en Aurora-MySQL ser\u00e1n creadas sin el sufijo. Esta funcionalidad aplica incluso cuando se a\u00f1aden tablas con \ncustomData.json\n.\n\n\n\n\nExcel Tablas Aspel SAE\n\n\nAbre aqu\u00ed \nExcel tablas Aspel SAE.\n\n\nNOTA:\n La lista del link anterior puede no estar completa.\n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nA\u00f1adir tablas a la Sincronizaci\u00f3n\n\n\nPara incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo \ncustomData.json\n, por ejemplo:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"CLIE_CLIB\",\n    \"fields\": \"CVE_CLIE, CAMPLIB1, CAMPLIB2, CAMPLIB3, CAMPLIB4, CAMPLIB5, CAMPLIB6\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"AFACT\",\n    \"fields\": \"* \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]\n\n\n\nNotar que no se incluye el sufijo \n01\n, \n02\n, etc. de las tablas.\n\n\nPara m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver \naqu\u00ed.\n\n\n\n\nAspel SAE en GitHub\n\n\nRepositorio de GitHub Aspel SAE\n para crear estructuras del Business Intelligence.\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?\n\n\nEscr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Aspel"
        }, 
        {
            "location": "/aspel/#aspel-sae", 
            "text": "Aspel SAE es un sistema administrativo que fabrica la empresa  Aspel de M\u00e9xico, S.A. de C.V.  Bipost Sync ofrece una conexi\u00f3n simple a  Google Data Studio  para crear tus tableros de mando, dashboards e indicadores de gesti\u00f3n.", 
            "title": "Aspel SAE"
        }, 
        {
            "location": "/aspel/#casos-de-uso", 
            "text": "Folleto Casos de Uso:  Tableros de Mando", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/aspel/#tablas-aspel", 
            "text": "Al configurar Bipost Sync con  System: SAE , de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:  almacenes\nclie\nclin\ncolor\ncompc\ncompd\ncompo\nconc\nconm\nconp\ncuen_det\ncuen_m\nfactd\nfactf\nfactp\ninve\nminve\nmoned\npaga_det\npaga_m\npar_compc\npar_compd\npar_compo\npar_factd\npar_factf\npar_factp\nprov\nprvprod\ntalla\nvend  En la base de datos del sistema Aspel SAE cada tabla tiene un sufijo  01 ,  02 , etc., de acuerdo a la empresa que se ha creado. Por ejemplo la tabla de las facturas podr\u00edas encontrarla como  factf01  y para otra raz\u00f3n social como  factf02 .  Bipost Sync autom\u00e1ticamente elimina estos sufijos por lo que las tablas en Aurora-MySQL ser\u00e1n creadas sin el sufijo. Esta funcionalidad aplica incluso cuando se a\u00f1aden tablas con  customData.json .", 
            "title": "Tablas Aspel"
        }, 
        {
            "location": "/aspel/#excel-tablas-aspel-sae", 
            "text": "Abre aqu\u00ed  Excel tablas Aspel SAE.  NOTA:  La lista del link anterior puede no estar completa.  Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Excel Tablas Aspel SAE"
        }, 
        {
            "location": "/aspel/#anadir-tablas-a-la-sincronizacion", 
            "text": "Para incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo  customData.json , por ejemplo:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"CLIE_CLIB\",\n    \"fields\": \"CVE_CLIE, CAMPLIB1, CAMPLIB2, CAMPLIB3, CAMPLIB4, CAMPLIB5, CAMPLIB6\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"AFACT\",\n    \"fields\": \"* \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\",\n    \"order\": \"\",\n    \"limit\": \"\"\n  }\n]  Notar que no se incluye el sufijo  01 ,  02 , etc. de las tablas.  Para m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver  aqu\u00ed.", 
            "title": "A\u00f1adir tablas a la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/aspel/#aspel-sae-en-github", 
            "text": "Repositorio de GitHub Aspel SAE  para crear estructuras del Business Intelligence.", 
            "title": "Aspel SAE en GitHub"
        }, 
        {
            "location": "/aspel/#contacto", 
            "text": "\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?  Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/microsip/", 
            "text": "Microsip ERP\n\n\nMicrosip es un sistema administrativo ERP que fabrica la empresa \nAplicaciones y Proyectos Computacionales S.A. de C.V.\n\n\nBipost Sync ofrece una conexi\u00f3n simple a \nGoogle Data Studio\n para crear tus tableros de mando, dashboards e indicadores de gesti\u00f3n.\n\n\n\n\nCasos de Uso\n\n\nFolleto Casos de Uso: \nTableros de Mando\n\n\n\n\n\n\nTablas Microsip\n\n\n\n\nAl configurar Bipost Sync con \nSystem: Microsip\n, de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:\n\n\nagentes\nalmacenes\nanticipos_cc\narticulos\nbancos\nbeneficiarios\ncajas\ncajeros\ncentros_costo\nciudades\nclaves_articulos\nclientes\ncobradores\ncomprom_articulos\nconceptos_ba\nconceptos_cc\nconceptos_cp\nconceptos_in\ncondiciones_pago\ncondiciones_pago_cp\ncuentas_bancarias\ncuentas_co\ndepositos_cc\ndepositos_cc_det\ndeptos_co\ndirs_clientes\ndoctos_ba\ndoctos_cc\ndoctos_cm\ndoctos_cm_det\ndoctos_cp\ndoctos_in\ndoctos_in_det\ndoctos_ve\ndoctos_ve_det\nestados\nexis_discretos\nformas_cobro_cc\nformas_cobro_doctos\ngrupos_lineas\nhistoria_cambiaria\nimportes_doctos_cc\nimportes_doctos_cp\nimpuestos\nimpuestos_doctos_ve\nlibres_articulos\nlibres_cargos_cc\nlibres_cargos_cp\nlibres_clientes\nlibres_com_cm\nlibres_cot_ve\nlibres_creditos_cc\nlibres_creditos_cp\nlibres_ctas_ban\nlibres_cuentas_co\nlibres_devcom_cm\nlibres_devfac_ve\nlibres_fac_ve\nlibres_ped_ve\nlibres_pol_co\nlibres_proveedor\nlineas_articulos\nmonedas\npaises\nplazos_cond_pag\nplazos_cond_pag_cp\npoliticas_comisiones_vendedores\nproveedores\nroles_claves_articulos\nsaldos_ba\nsaldos_cc\nsaldos_co\nsaldos_cp\nsaldos_in\nsucursales\ntipos_clientes\ntipos_impuestos\ntipos_polizas\ntipos_prov\ntraspasos_ba\nusos_anticipos_cc\nvencimientos_cargos_cc\nvencimientos_cargos_cm\nvencimientos_cargos_cp\nvencimientos_cargos_ve\nvendedores\nvias_embarque\nzonas_clientes\n\n\n\n\n\nExcel Tablas Microsip\n\n\nAbre aqu\u00ed \nExcel tablas Microsip.\n\n\nNOTA:\n La lista del link anterior puede no estar completa.\n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nA\u00f1adir tablas a la Sincronizaci\u00f3n\n\n\nPara incluir tablas adicionales en la sincronizaci\u00f3n se utiliza el archivo \ncustomData.json\n, ejemplo:\n\n\n    [\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_CFDI\",\n        \"fields\": \"DOCTOS_CO_CFDI.*\",\n        \"join\": \"DOCTOS_CO ON DOCTOS_CO_CFDI.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET\",\n        \"fields\": \"DOCTOS_CO_DET.*\",\n        \"join\": \"DOCTOS_CO ON DOCTOS_CO_DET.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET_CFDI\",\n        \"fields\": \"DOCTOS_CO_DET_CFDI.*\",\n        \"join\": \"DOCTOS_CO_DET ON DOCTOS_CO_DET_CFDI.DOCTO_CO_DET_ID = DOCTOS_CO_DET.DOCTO_CO_DET_ID JOIN DOCTOS_CO ON DOCTOS_CO_DET.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET_INFO_BAN\",\n        \"fields\": \"DOCTOS_CO_DET_INFO_BAN.*\",\n        \"join\": \"DOCTOS_CO_DET ON DOCTOS_CO_DET_INFO_BAN.DOCTO_CO_DET_ID = DOCTOS_CO_DET.DOCTO_CO_DET_ID JOIN DOCTOS_CO ON DOCTOS_CO_DET.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"SALDOS_CO\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"ANO \n= EXTRACT(YEAR from CAST('Now' as date))-1\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"USOS_FOLIOS_FISCALES\",\n        \"fields\": \"USOS_FOLIOS_FISCALES.USO_FOLIO_ID, USOS_FOLIOS_FISCALES.FOLIOS_FISCALES_ID, USOS_FOLIOS_FISCALES.FOLIO, USOS_FOLIOS_FISCALES.FECHA, USOS_FOLIOS_FISCALES.SISTEMA, USOS_FOLIOS_FISCALES.DOCTO_ID, USOS_FOLIOS_FISCALES.PROV_CERT, USOS_FOLIOS_FISCALES.FECHA_HORA_TIMBRADO, USOS_FOLIOS_FISCALES.UUID, USOS_FOLIOS_FISCALES.CFDI_ID \",\n        \"join\": \"REPOSITORIO_CFDI ON USOS_FOLIOS_FISCALES.CFDI_ID = REPOSITORIO_CFDI.CFDI_ID\",\n        \"filter\": \"REPOSITORIO_CFDI.FECHA BETWEEN DATEADD(-20 day to CAST('Now' as date)) AND CAST('Now' as date)\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"USOS_FOLIOS_FISCALES_CANCELADOS\",\n        \"fields\": \"USO_FOLIO_ID, FECHA_CANCELACION, PROV_CANCELACION, FECHA_HORA_CANCELACION_CFDI \",\n        \"join\": \"\",\n        \"filter\": \"(FECHA_CANCELACION BETWEEN DATEADD(-20 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(FECHA_HORA_CANCELACION_CFDI AS date) BETWEEN DATEADD(-20 day to CAST('Now' as date)) AND CAST('Now' as date))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_ENTRE_SIS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"DOCTO_DEST_ID DESC\",\n        \"limit\": \"12500\"\n      }\n    ]\n\n\n\nPara m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver \naqu\u00ed.\n\n\n\n\nSincronizaci\u00f3n Doble V\u00eda\n\n\n\n\n\n\nAl usar la sincronizaci\u00f3n con \nDownload Data\n es posible traer datos desde AWS Aurora-MySQL hacia tu base de Microsip. \n\n\n\n\n\n\nCuando activas la opci\u00f3n \nProcess Data\n se insertan los datos en Firebird.\n\n\n\n\n\n\nUsando la opci\u00f3n \nFinal Query\n puedes correr cualquier stored procedure para que termines de calcular y procesar hacia otras tablas. \n\n\n\n\n\n\nPor ejemplo, despu\u00e9s que se han recibido e insertado los datos en Firebird, queremos correr el store \nspFinal\n as\u00ed que usamos el par\u00e1metro \n\"finalQuery\": \"execute procedure spFinal;\"\n dentro de \noutData.json\n. Aqu\u00ed ejemplo del contenido de \nspFinal\n:\n\n\n    SET TERM ^ ;\n\n    RECREATE PROCEDURE SPFINAL\n    AS\n    DECLARE VARIABLE id            VARCHAR(50);\n    DECLARE VARIABLE idd           VARCHAR(50);\n    DECLARE VARIABLE tipod         VARCHAR(8);\n    DECLARE VARIABLE docto_id      BIGINT;\n    DECLARE VARIABLE new_docto_id  BIGINT;\n    DECLARE VARIABLE new_doctod_id BIGINT;\n    DECLARE VARIABLE estatus       VARCHAR(1);\n    DECLARE VARIABLE new_posicion  INT;\n    DECLARE VARIABLE posicion      INT;\n    BEGIN\n      -- DOCTOS_VE\n      FOR SELECT id FROM EXPORT_DOCTOS_VE INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_VE WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_VE_ID FROM DOCTOS_VE WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_VE(\n                   DOCTO_VE_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CONSIG_ID, ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CLI_ID,    ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, ID\n              FROM EXPORT_DOCTOS_VE\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_VE_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_VE_DET(\n                     DOCTO_VE_DET_ID, DOCTO_VE_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_VE_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_VE SET ESTATUS = :estatus WHERE DOCTO_VE_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_CM\n      FOR SELECT id FROM EXPORT_DOCTOS_CM INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_CM WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_CM_ID FROM DOCTOS_CM WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_CM(\n                   DOCTO_CM_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , ID\n              FROM EXPORT_DOCTOS_CM\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_CM_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_CM_DET(\n                     DOCTO_CM_DET_ID, DOCTO_CM_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_CM_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_CM SET ESTATUS = :estatus WHERE DOCTO_CM_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_IN\n      FOR SELECT id FROM EXPORT_DOCTOS_IN INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT CANCELADO   FROM EXPORT_DOCTOS_IN WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_IN_ID FROM DOCTOS_IN WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_IN(\n                   DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, WEBID)\n            SELECT :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, ID\n              FROM EXPORT_DOCTOS_IN\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_IN_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_IN_DET(\n                     DOCTO_IN_DET_ID, DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID)\n              SELECT :new_doctod_id,  :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID\n                FROM EXPORT_DOCTOS_IN_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_IN SET CANCELADO = :estatus WHERE DOCTO_IN_ID = :docto_id;\n        END\n      END    \n      SUSPEND;\n    END^\n\n    SET TERM ; ^\n\n\n\n\n\nMicrosip en GitHub\n\n\nRepositorio de GitHub Microsip\n para crear estructuras del Business Intelligence.\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?\n\n\nEscr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Microsip"
        }, 
        {
            "location": "/microsip/#microsip-erp", 
            "text": "Microsip es un sistema administrativo ERP que fabrica la empresa  Aplicaciones y Proyectos Computacionales S.A. de C.V.  Bipost Sync ofrece una conexi\u00f3n simple a  Google Data Studio  para crear tus tableros de mando, dashboards e indicadores de gesti\u00f3n.", 
            "title": "Microsip ERP"
        }, 
        {
            "location": "/microsip/#casos-de-uso", 
            "text": "Folleto Casos de Uso:  Tableros de Mando", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/microsip/#tablas-microsip", 
            "text": "Al configurar Bipost Sync con  System: Microsip , de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:  agentes\nalmacenes\nanticipos_cc\narticulos\nbancos\nbeneficiarios\ncajas\ncajeros\ncentros_costo\nciudades\nclaves_articulos\nclientes\ncobradores\ncomprom_articulos\nconceptos_ba\nconceptos_cc\nconceptos_cp\nconceptos_in\ncondiciones_pago\ncondiciones_pago_cp\ncuentas_bancarias\ncuentas_co\ndepositos_cc\ndepositos_cc_det\ndeptos_co\ndirs_clientes\ndoctos_ba\ndoctos_cc\ndoctos_cm\ndoctos_cm_det\ndoctos_cp\ndoctos_in\ndoctos_in_det\ndoctos_ve\ndoctos_ve_det\nestados\nexis_discretos\nformas_cobro_cc\nformas_cobro_doctos\ngrupos_lineas\nhistoria_cambiaria\nimportes_doctos_cc\nimportes_doctos_cp\nimpuestos\nimpuestos_doctos_ve\nlibres_articulos\nlibres_cargos_cc\nlibres_cargos_cp\nlibres_clientes\nlibres_com_cm\nlibres_cot_ve\nlibres_creditos_cc\nlibres_creditos_cp\nlibres_ctas_ban\nlibres_cuentas_co\nlibres_devcom_cm\nlibres_devfac_ve\nlibres_fac_ve\nlibres_ped_ve\nlibres_pol_co\nlibres_proveedor\nlineas_articulos\nmonedas\npaises\nplazos_cond_pag\nplazos_cond_pag_cp\npoliticas_comisiones_vendedores\nproveedores\nroles_claves_articulos\nsaldos_ba\nsaldos_cc\nsaldos_co\nsaldos_cp\nsaldos_in\nsucursales\ntipos_clientes\ntipos_impuestos\ntipos_polizas\ntipos_prov\ntraspasos_ba\nusos_anticipos_cc\nvencimientos_cargos_cc\nvencimientos_cargos_cm\nvencimientos_cargos_cp\nvencimientos_cargos_ve\nvendedores\nvias_embarque\nzonas_clientes", 
            "title": "Tablas Microsip"
        }, 
        {
            "location": "/microsip/#excel-tablas-microsip", 
            "text": "Abre aqu\u00ed  Excel tablas Microsip.  NOTA:  La lista del link anterior puede no estar completa.  Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Excel Tablas Microsip"
        }, 
        {
            "location": "/microsip/#anadir-tablas-a-la-sincronizacion", 
            "text": "Para incluir tablas adicionales en la sincronizaci\u00f3n se utiliza el archivo  customData.json , ejemplo:      [\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_CFDI\",\n        \"fields\": \"DOCTOS_CO_CFDI.*\",\n        \"join\": \"DOCTOS_CO ON DOCTOS_CO_CFDI.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET\",\n        \"fields\": \"DOCTOS_CO_DET.*\",\n        \"join\": \"DOCTOS_CO ON DOCTOS_CO_DET.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET_CFDI\",\n        \"fields\": \"DOCTOS_CO_DET_CFDI.*\",\n        \"join\": \"DOCTOS_CO_DET ON DOCTOS_CO_DET_CFDI.DOCTO_CO_DET_ID = DOCTOS_CO_DET.DOCTO_CO_DET_ID JOIN DOCTOS_CO ON DOCTOS_CO_DET.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET_INFO_BAN\",\n        \"fields\": \"DOCTOS_CO_DET_INFO_BAN.*\",\n        \"join\": \"DOCTOS_CO_DET ON DOCTOS_CO_DET_INFO_BAN.DOCTO_CO_DET_ID = DOCTOS_CO_DET.DOCTO_CO_DET_ID JOIN DOCTOS_CO ON DOCTOS_CO_DET.DOCTO_CO_ID = DOCTOS_CO.DOCTO_CO_ID\",\n        \"filter\": \"(DOCTOS_CO.ESTATUS = 'P' AND DOCTOS_CO.APLICADO = 'N') OR ((DOCTOS_CO.FECHA BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CREACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_ULT_MODIF AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(DOCTOS_CO.FECHA_HORA_CANCELACION AS date) BETWEEN DATEADD(-2 day to CAST('Now' as date)) AND CAST('Now' as date)))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"SALDOS_CO\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"ANO  = EXTRACT(YEAR from CAST('Now' as date))-1\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"USOS_FOLIOS_FISCALES\",\n        \"fields\": \"USOS_FOLIOS_FISCALES.USO_FOLIO_ID, USOS_FOLIOS_FISCALES.FOLIOS_FISCALES_ID, USOS_FOLIOS_FISCALES.FOLIO, USOS_FOLIOS_FISCALES.FECHA, USOS_FOLIOS_FISCALES.SISTEMA, USOS_FOLIOS_FISCALES.DOCTO_ID, USOS_FOLIOS_FISCALES.PROV_CERT, USOS_FOLIOS_FISCALES.FECHA_HORA_TIMBRADO, USOS_FOLIOS_FISCALES.UUID, USOS_FOLIOS_FISCALES.CFDI_ID \",\n        \"join\": \"REPOSITORIO_CFDI ON USOS_FOLIOS_FISCALES.CFDI_ID = REPOSITORIO_CFDI.CFDI_ID\",\n        \"filter\": \"REPOSITORIO_CFDI.FECHA BETWEEN DATEADD(-20 day to CAST('Now' as date)) AND CAST('Now' as date)\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"USOS_FOLIOS_FISCALES_CANCELADOS\",\n        \"fields\": \"USO_FOLIO_ID, FECHA_CANCELACION, PROV_CANCELACION, FECHA_HORA_CANCELACION_CFDI \",\n        \"join\": \"\",\n        \"filter\": \"(FECHA_CANCELACION BETWEEN DATEADD(-20 day to CAST('Now' as date)) AND CAST('Now' as date)) OR (CAST(FECHA_HORA_CANCELACION_CFDI AS date) BETWEEN DATEADD(-20 day to CAST('Now' as date)) AND CAST('Now' as date))\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_ENTRE_SIS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"DOCTO_DEST_ID DESC\",\n        \"limit\": \"12500\"\n      }\n    ]  Para m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver  aqu\u00ed.", 
            "title": "A\u00f1adir tablas a la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#sincronizacion-doble-via", 
            "text": "Al usar la sincronizaci\u00f3n con  Download Data  es posible traer datos desde AWS Aurora-MySQL hacia tu base de Microsip.     Cuando activas la opci\u00f3n  Process Data  se insertan los datos en Firebird.    Usando la opci\u00f3n  Final Query  puedes correr cualquier stored procedure para que termines de calcular y procesar hacia otras tablas.     Por ejemplo, despu\u00e9s que se han recibido e insertado los datos en Firebird, queremos correr el store  spFinal  as\u00ed que usamos el par\u00e1metro  \"finalQuery\": \"execute procedure spFinal;\"  dentro de  outData.json . Aqu\u00ed ejemplo del contenido de  spFinal :      SET TERM ^ ;\n\n    RECREATE PROCEDURE SPFINAL\n    AS\n    DECLARE VARIABLE id            VARCHAR(50);\n    DECLARE VARIABLE idd           VARCHAR(50);\n    DECLARE VARIABLE tipod         VARCHAR(8);\n    DECLARE VARIABLE docto_id      BIGINT;\n    DECLARE VARIABLE new_docto_id  BIGINT;\n    DECLARE VARIABLE new_doctod_id BIGINT;\n    DECLARE VARIABLE estatus       VARCHAR(1);\n    DECLARE VARIABLE new_posicion  INT;\n    DECLARE VARIABLE posicion      INT;\n    BEGIN\n      -- DOCTOS_VE\n      FOR SELECT id FROM EXPORT_DOCTOS_VE INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_VE WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_VE_ID FROM DOCTOS_VE WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_VE(\n                   DOCTO_VE_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CONSIG_ID, ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CLI_ID,    ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, ID\n              FROM EXPORT_DOCTOS_VE\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_VE_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_VE_DET(\n                     DOCTO_VE_DET_ID, DOCTO_VE_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_VE_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_VE SET ESTATUS = :estatus WHERE DOCTO_VE_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_CM\n      FOR SELECT id FROM EXPORT_DOCTOS_CM INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_CM WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_CM_ID FROM DOCTOS_CM WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_CM(\n                   DOCTO_CM_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , ID\n              FROM EXPORT_DOCTOS_CM\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_CM_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_CM_DET(\n                     DOCTO_CM_DET_ID, DOCTO_CM_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_CM_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_CM SET ESTATUS = :estatus WHERE DOCTO_CM_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_IN\n      FOR SELECT id FROM EXPORT_DOCTOS_IN INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT CANCELADO   FROM EXPORT_DOCTOS_IN WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_IN_ID FROM DOCTOS_IN WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_IN(\n                   DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, WEBID)\n            SELECT :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, ID\n              FROM EXPORT_DOCTOS_IN\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_IN_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_IN_DET(\n                     DOCTO_IN_DET_ID, DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID)\n              SELECT :new_doctod_id,  :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID\n                FROM EXPORT_DOCTOS_IN_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_IN SET CANCELADO = :estatus WHERE DOCTO_IN_ID = :docto_id;\n        END\n      END    \n      SUSPEND;\n    END^\n\n    SET TERM ; ^", 
            "title": "Sincronizaci\u00f3n Doble V\u00eda"
        }, 
        {
            "location": "/microsip/#microsip-en-github", 
            "text": "Repositorio de GitHub Microsip  para crear estructuras del Business Intelligence.", 
            "title": "Microsip en GitHub"
        }, 
        {
            "location": "/microsip/#contacto", 
            "text": "\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?  Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/intelisis/", 
            "text": "Intelisis ERP\n\n\nIntelisis es un ERP que fabrica la empresa \nIntelisis Software, S.A. de C.V.\n\n\nBipost Sync ofrece una conexi\u00f3n simple a \nGoogle Data Studio\n para crear tus tableros de mando, dashboards e indicadores de gesti\u00f3n.\n\n\n\n\nCasos de Uso\n\n\nFolleto Casos de Uso: \nTableros de Mando\n\n\n\n\n\n\nEjemplos de configuraci\u00f3n\n\n\nEjemplos de informaci\u00f3n a sincronizar usando \ncustomData.json\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"Venta\",\n        \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"VentaD\",\n        \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n        \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    }\n]\n\n\n\n\n\nRecomendaciones\n\n\nCrear dos carpetas\n\n\nCrea al menos dos carpetas para Bipost Sync, esto te permitir\u00e1 sincronizar cat\u00e1logos y movimientos en forma independiente.\n\n\nCarpeta Cat\u00e1logos\n\n\nTu primer carpeta donde est\u00e1 Bipost Sync la puedes nombrar \nBipost_catalogos\n y utilizar\u00e1s \ncustomData.json\n para enviar \u00fanicamente cat\u00e1logos. Ejemplo:\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"Sucursal\",\n        \"fields\": \"Sucursal, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Agente\",\n        \"fields\": \"Agente, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"MovTipo\",\n        \"fields\": \"Modulo, Mov, Clave\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Cta\",\n        \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"CentroCostos\",\n        \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    }\n]\n\n\n\nAbre biPost.exe y aseg\u00farate de quitar el check \nRecursive Sync\n.\n\n\nUsando el JSON del ejemplo anterior, se enviar\u00e1 en cada sincronizaci\u00f3n \ntodos los renglones\n de las tablas \nSucursal\n, \nAgente\n, \nMovTipo\n, \nCta\n y \nCentroCostos\n.\n\n\nNOTA:\n Si la tabla que vas a enviar tiene m\u00e1s de un mill\u00f3n de registros, es recomendable que filtres el query utilizando la opci\u00f3n \n\"filter\":\n.\n\n\nCarpeta Recursivo\n\n\nLa otra carpeta de Bipost Sync puedes nombrarla \nBipost_diario\n. Abre el archivo \ncustomData.json\n y especifica las tablas de movimientos que vas a enviar, por ejemplo:\n\n\n[\n  {\n        \"active\": \"true\",\n        \"table\": \"Cont\",\n        \"fields\": \"Cont.ID, Cont.Empresa, Cont.Mov, Cont.MovID, Cont.FechaEmision, Cont.FechaContable, Cont.Proyecto, Cont.Moneda, Cont.TipoCambio, Cont.Estatus, Cont.Ejercicio, Cont.Periodo, Cont.Moneda2, Cont.TipoCambio2\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n        \"filter\": \"Cont.Estatus \n 'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n        \"recursiveDateField\": \"Cont.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n  },\n  {\n        \"active\": \"true\",\n        \"table\": \"ContD\",\n        \"fields\": \"ContD.ID, ContD.Renglon, ContD.RenglonSub, ContD.Cuenta, ContD.SubCuenta, ContD.SubCuenta2, ContD.SubCuenta3, ContD.Debe, ContD.Debe2, ContD.Haber, ContD.Haber2, ContD.Sucursal, ContD.SucursalContable \",\n        \"join\": \"Cont WITH (NOLOCK) ON Cont.ID = ContD.ID JOIN MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n        \"filter\": \"Cont.Estatus \n 'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n        \"recursiveDateField\": \"Cont.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n  }\n]\n\n\n\nEs importante incluir el par\u00e1metro \n\"recursiveDateField\":\n para optimizar la subida de informaci\u00f3n.\n\n\n\n\nMediante esta opci\u00f3n se pueden cargar hist\u00f3ricos de varios a\u00f1os.\n\n\nNOTA: Al apagar el check \nRecursive Sync\n autom\u00e1ticamente la fecha de inicio y fin se fija en el d\u00eda de hoy utilizando la fecha de la m\u00e1quina Windows.\n\n\nUso del Filter\n\n\nEn el ejemplo customData.json anterior, se utiliz\u00f3 el siguiente filtro para la tabla \nContD\n:\n\n\n\"filter\": \"Cont.Estatus \n 'SINAFECTAR'\n\n\nSi un registro del m\u00f3dulo Contabilidad se cancela, cambia su estatus a \nCANCELADO\n por tanto el query a la base de datos incluye los estatus \nCANCELADO\n para que estos cambios se vean reflejados en la base de Aurora-MySQL.\n\n\n\n\nIntelisis en GitHub\n\n\nVer m\u00e1s ejemplos en nuestro \nrepositorio de GitHub.\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?\n\n\nEscr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Intelisis"
        }, 
        {
            "location": "/intelisis/#intelisis-erp", 
            "text": "Intelisis es un ERP que fabrica la empresa  Intelisis Software, S.A. de C.V.  Bipost Sync ofrece una conexi\u00f3n simple a  Google Data Studio  para crear tus tableros de mando, dashboards e indicadores de gesti\u00f3n.", 
            "title": "Intelisis ERP"
        }, 
        {
            "location": "/intelisis/#casos-de-uso", 
            "text": "Folleto Casos de Uso:  Tableros de Mando", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/intelisis/#ejemplos-de-configuracion", 
            "text": "Ejemplos de informaci\u00f3n a sincronizar usando  customData.json  [\n    {\n        \"active\": \"true\",\n        \"table\": \"Venta\",\n        \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"VentaD\",\n        \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n        \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    }\n]", 
            "title": "Ejemplos de configuraci\u00f3n"
        }, 
        {
            "location": "/intelisis/#recomendaciones", 
            "text": "", 
            "title": "Recomendaciones"
        }, 
        {
            "location": "/intelisis/#crear-dos-carpetas", 
            "text": "Crea al menos dos carpetas para Bipost Sync, esto te permitir\u00e1 sincronizar cat\u00e1logos y movimientos en forma independiente.", 
            "title": "Crear dos carpetas"
        }, 
        {
            "location": "/intelisis/#carpeta-catalogos", 
            "text": "Tu primer carpeta donde est\u00e1 Bipost Sync la puedes nombrar  Bipost_catalogos  y utilizar\u00e1s  customData.json  para enviar \u00fanicamente cat\u00e1logos. Ejemplo:  [\n    {\n        \"active\": \"true\",\n        \"table\": \"Sucursal\",\n        \"fields\": \"Sucursal, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Agente\",\n        \"fields\": \"Agente, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"MovTipo\",\n        \"fields\": \"Modulo, Mov, Clave\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Cta\",\n        \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"CentroCostos\",\n        \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\"\n    }\n]  Abre biPost.exe y aseg\u00farate de quitar el check  Recursive Sync .  Usando el JSON del ejemplo anterior, se enviar\u00e1 en cada sincronizaci\u00f3n  todos los renglones  de las tablas  Sucursal ,  Agente ,  MovTipo ,  Cta  y  CentroCostos .  NOTA:  Si la tabla que vas a enviar tiene m\u00e1s de un mill\u00f3n de registros, es recomendable que filtres el query utilizando la opci\u00f3n  \"filter\": .", 
            "title": "Carpeta Cat\u00e1logos"
        }, 
        {
            "location": "/intelisis/#carpeta-recursivo", 
            "text": "La otra carpeta de Bipost Sync puedes nombrarla  Bipost_diario . Abre el archivo  customData.json  y especifica las tablas de movimientos que vas a enviar, por ejemplo:  [\n  {\n        \"active\": \"true\",\n        \"table\": \"Cont\",\n        \"fields\": \"Cont.ID, Cont.Empresa, Cont.Mov, Cont.MovID, Cont.FechaEmision, Cont.FechaContable, Cont.Proyecto, Cont.Moneda, Cont.TipoCambio, Cont.Estatus, Cont.Ejercicio, Cont.Periodo, Cont.Moneda2, Cont.TipoCambio2\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n        \"filter\": \"Cont.Estatus   'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n        \"recursiveDateField\": \"Cont.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n  },\n  {\n        \"active\": \"true\",\n        \"table\": \"ContD\",\n        \"fields\": \"ContD.ID, ContD.Renglon, ContD.RenglonSub, ContD.Cuenta, ContD.SubCuenta, ContD.SubCuenta2, ContD.SubCuenta3, ContD.Debe, ContD.Debe2, ContD.Haber, ContD.Haber2, ContD.Sucursal, ContD.SucursalContable \",\n        \"join\": \"Cont WITH (NOLOCK) ON Cont.ID = ContD.ID JOIN MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n        \"filter\": \"Cont.Estatus   'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n        \"recursiveDateField\": \"Cont.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\"\n  }\n]  Es importante incluir el par\u00e1metro  \"recursiveDateField\":  para optimizar la subida de informaci\u00f3n.   Mediante esta opci\u00f3n se pueden cargar hist\u00f3ricos de varios a\u00f1os.  NOTA: Al apagar el check  Recursive Sync  autom\u00e1ticamente la fecha de inicio y fin se fija en el d\u00eda de hoy utilizando la fecha de la m\u00e1quina Windows.", 
            "title": "Carpeta Recursivo"
        }, 
        {
            "location": "/intelisis/#uso-del-filter", 
            "text": "En el ejemplo customData.json anterior, se utiliz\u00f3 el siguiente filtro para la tabla  ContD :  \"filter\": \"Cont.Estatus   'SINAFECTAR'  Si un registro del m\u00f3dulo Contabilidad se cancela, cambia su estatus a  CANCELADO  por tanto el query a la base de datos incluye los estatus  CANCELADO  para que estos cambios se vean reflejados en la base de Aurora-MySQL.", 
            "title": "Uso del Filter"
        }, 
        {
            "location": "/intelisis/#intelisis-en-github", 
            "text": "Ver m\u00e1s ejemplos en nuestro  repositorio de GitHub.", 
            "title": "Intelisis en GitHub"
        }, 
        {
            "location": "/intelisis/#contacto", 
            "text": "\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?  Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/businessintelligence/", 
            "text": "Use Case\n\n\nAt \nFactor BI\n we use Bipost Sync to power small and medium companies with Dashboards using \nGoogle Data Studio\n and \nAWS QuickSight\n\n\nWant to see a demo? Go to --\n \nGoogle Data Studio Demo.\n\n\nHere is an example of the architected solution.\n\n\n\n\nIn this example some dashboards are built with AWS QuickSight because it has an iPhone App so it is used by high level executives on the go.\n\n\nGoogle Data Studio is used for desktop users and detailed analytical situations, excellent UX, and dashboards are shared using \n\nGoogle accounts,\n which is a great way to avoid managing user accounts.", 
            "title": "Business Intelligence"
        }, 
        {
            "location": "/businessintelligence/#use-case", 
            "text": "At  Factor BI  we use Bipost Sync to power small and medium companies with Dashboards using  Google Data Studio  and  AWS QuickSight  Want to see a demo? Go to --   Google Data Studio Demo.  Here is an example of the architected solution.   In this example some dashboards are built with AWS QuickSight because it has an iPhone App so it is used by high level executives on the go.  Google Data Studio is used for desktop users and detailed analytical situations, excellent UX, and dashboards are shared using  Google accounts,  which is a great way to avoid managing user accounts.", 
            "title": "Use Case"
        }, 
        {
            "location": "/setupaws/", 
            "text": "Link your AWS Account\n\n\nThese instructions are for experienced AWS administrators only.\n\n\nFollow these instructions if you want to manually to link your AWS Account to Bipost API. Strong knowledge of AWS is needed.\n\n\nIMPORTANT NOTICE: If you are planning to use the following AWS resources for production you may want to follow your company policies and understand how to use AWS security according to your needs.\n\n\n\n\nCanonical User ID\n\n\nSign in with the \nroot\n AWS account.\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nExpand Account Identifiers.\n\n\n\n\nCopy AWS Account ID (12-digit) and Canonical User ID (64-digit).\n\n\n\n\n\n\n\n\nEmail these numbers to \ninfo@factorbi.com\n so we can setup your dedicated Bucket.\n\n\n\n\n\n\nWe use your \nCanonical User ID\n to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.\n\n\n\n\nCreate IAM Policy to Grant Access to S3\n\n\nFrom this point on you need the newly S3 bucket ARN that we provided over email on the previous step.\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nPolicies.\n\n\nCreate policy\n blue button.\n\n\nSelect \nPolicy Generator\n\n\n\n\nClick \nJSON\n tab.\n\n\n\n\n\n\n\n\nCopy and paste the following.\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-123456789012\", \n                \"arn:aws:s3:::bipostdata-123456789012/*\"\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\n\nReplace the text \narn:aws:s3:::bipostdata-123456789012\n with the bucket ARN you received from us over email.\n\n\n\n\n\n\nDouble check the JSON text is using your own ARN bucket and click \nReview policy\n blue button on the lower right.\n\n\n\n\n\n\nEnter the following on the Review policy screen.\n\n\n\n\nName: \nAuroraToS3\n\n\nName: \nConnection to Factor BI bucket.\n\n\n\n\n\n\n\n\nClick \nCreate policy\n blue button.\n\n\n\n\n\n\nFurther information from AWS go to: \nAllowing Amazon Aurora to Access Amazon S3 Resources\n\n\n\n\nCreate IAM Role to Allow RDS Access to S3\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nRoles.\n\n\nCreate role\n blue button.\n\n\n\n\nChoose \nAWS service,\n then \nRDS\n\n\n\n\n\n\n\n\nSelect your use case\n click \nRDS - CloudHSM and Directory Service,\n click \nNext: Permissions\n blue button.\n\n\n\n\nClick \nNext:Review.\n\n\nSet \nRole name:\n \nRDSLoadFromS3\n and click \nCreate role.\n\n\nNow from the navigation details, click the role you just created.\n\n\n\n\nUnder permissions tab, detach by clicking \nX\n the following:\n\n\n\n\nAmazonRDSDirectoryServiceAccess\n\n\nRDSCloudHsmAuthorizationRole\n\n\n\n\n\n\n\n\nNow click \nAttach policy\n blue button.\n\n\n\n\n\n\n\n\nSelect \nAuroraToS3\n and click \nAttach policy\n blue button.\n\n\n\n\n\n\nCopy \nRole ARN\n string and save it for further use. It may look like this: \narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\nFurther information from AWS go to: \nCreating an IAM Role to Allow Amazon Aurora to Access AWS Services\n\n\n\n\nClosest AWS Region\n\n\ncloudping.info\n\n\n\n\nClick the above link and hit \nHTTP Ping\n and look for the lowest latency.\n\n\nMaybe you want to try this at different times of the day.\n\n\nTake note of the closest region.\n\n\n\n\n\n\n\n\nCreate Aurora Instance\n\n\nInstance specifications\n\n\n\n\nFrom AWS Console Home, upper right corner (next to you name) be sure to select the \nclosest region to your location.\n\n\nUnder menu \nServices\n search RDS.\n\n\nFrom \nAmazon RDS\n click Instances.\n\n\nLaunch DB Instance, orange button.\n\n\nSelect Engine: \nAmazon Aurora\n, scroll down, Edition: \nMySQL 5.7-compatible\n click \nNext\n orange button.\n\n\nDB instance class: for testing purposes select the smallest available, currently \nt2.small\n\n\nMulti-AZ deployment: for testing purposes select \nNo\n\n\nSettings, DB Instance identifier: set a name, lower-case and no special characters.\n\n\nMaster username: \nroot\n\n\n\n\nMaster Password: set a strong password and store it in a secure place.\n\n\n\n\nPassword can't contain spaces and the following characters: \n\n\n\n\n/\n\"\n@\n\n\n\n\n\n\n\nClick \nNext\n orange button.\n\n\n\n\n\n\n\n\nNetwork \n Security\n\n\n\n\nVirtual Private Cloud (VPC): \nCreate new VPC\n\n\nSubnet group: \nCreate new DB Subnet Group\n\n\nPublicly accessible: \nYes\n\n\nAvailability zone: \nNo Preference\n\n\nVPC security groups: \nCreate new VPC security groups\n\n\n\n\n\n\nDatabase options\n\n\n\n\nDB Cluster Identifier: \nleave blank\n\n\nDatabase name: \nleave blank\n\n\nDatabase port: \n3306\n\n\nDB parameter group: \ndefault.aurora-mysql5.7\n\n\nDB cluster parameter group: \ndefault.aurora-mysql5.7\n\n\nOption group: \nleave default\n\n\n\n\nEncryption\n\n\n\n\nEncryption: \nDisable encryption\n\n\n\n\nFailover\n\n\n\n\nPriority: \ntier-0\n\n\n\n\nBackup\n\n\n\n\nBackup retention period: \n1 day\n\n\n\n\nMonitoring\n\n\n\n\nEnhanced Monitoring: \nDisable enhanced monitoring\n\n\n\n\nMaintenance\n\n\n\n\nAuto minor version upgrade: \nEnable auto minor version upgrade\n\n\nMaintenance windows: \nNo preference\n\n\n\n\nLaunch DB Instance\n\n\n\n\n\n\nClick Launch DB Instance orange button.\n\n\n\n\n\n\nThis process may take a few minutes.\n\n\n\n\n\n\nClick \nView DB Instance details.\n\n\n\n\n\n\n\n\nRDS Instance Security Group\n\n\nClick \nInstances\n left pane.\n\n\nOnce the new instance has \nStatus:\n \navailable\n proceed:\n\n\n\n\nClick your new instance.\n\n\nScroll down to \nConnect\n section. \n\n\n\n\nUnder \nSecurity group rules\n click the blue string that looks like this\n\n\n\n\nrds-launch-wizard (sg-XXXXXXXX)\n\n\n\n\n\n\n\n\n\n\nYou are now on EC2 Management Console and Security Group ID is already selected.\n\n\n\n\nClick \nActions \\ Edit inbound rules\n\n\nRemove the default Custom TCP rule created.\n\n\nClick \nAdd Rule\n, under Type select \nMYSQL/Aurora\n\n\n\n\nSource \nCustom\n and enter this value: \n0.0.0.0/0\n\n\n\n\n\n\nRepeat steps 7 \n 8, and type value \n::/0\n\n\n\n\n\n\n\n\nClick \nSave\n blue button.\n\n\n\n\nClick \nActions \\ Edit outbound rules\n\n\nVerify if Type: \nAll traffic\n, Destination: \nCustom\n and value: \n0.0.0.0/0\n is already set, if not, add the rule.\n\n\nGo back to \nRDS Console\n, select your instance, click \nInstance actions \\ Reboot\n, confirm with orange button on the right.\n\n\n\n\nWait until \nStatus\n is \navailable\n \n\n\n\n\n\n\n\n\nClick your DB Instance, scroll down to \nDetails\n section and check if \nSecurity groups\n are \n( active )\n\n\n\n\n\n\n\n\nAdd IAM Role to Aurora Cluster\n\n\n\n\nOpen \nRDS console.\n\n\nChoose \nClusters\n on left pane.\n\n\nClick radio button of your newly cluster.\n\n\n\n\nClick \nActions\n then \nManage IAM roles.\n\n\n\n\n\n\n\n\nUnder \nAdd IAM roles to this cluster\n select the role you just created: \nRDSLoadFromS3\n and click \nAdd role\n button.\n\n\n\n\n\n\n\n\nWait until you see Status \nactive\n under Current IAM roles for this cluster.\n\n\n\n\nClick \nDone\n.\n\n\n\n\n\n\nCreate Cluster Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate parameter group\n orange button on top.\n\n\n\n\nParameter group family: \naurora-mysql5.7\n\n\nType: \nDB Cluster Parameter Group\n\n\nGroup name: \nAuroraClusterAllowAWSAccess\n\n\nDescription: \nBipost Aurora Database Cluster Parameter Group\n\n\n\n\n\n\n\n\nClick \nCreate\n orange button and refresh browser.\n\n\n\n\nClick check box on your new \nauroraclusterallowawsaccess\n parameter group and click \nParameter group actions\n and then \nEdit\n button on top.\n\n\nMake sure you have your ARN role string (\nstep 12 here\n) and replace it below.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\nExample\n\n\n\n\n\n\n\n\n\n\naurora_load_from_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naurora_select_into_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_lambda_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n orange button.\n\n\n\n\n\n\nClick \nPreview changes\n and it should look like this:\n\n\n\n\n\n\n\n\nFurther information from AWS go to: \nAssociating an IAM Role with a DB Cluster\n\n\n\n\nCreate DB Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate parameter group\n orange button on top.\n\n\n\n\nParameter group family: \naurora-mysql5.7\n\n\nType: \nDB Parameter Group\n\n\nGroup name: \nAuroraInstanceAllowAWSAccess\n\n\nDescription: \nBipost Aurora Parameter Group\n\n\n\n\n\n\n\n\nClick \nCreate\n orange button and refresh browser.\n\n\n\n\nClick check box on your new \naurorainstanceallowawsaccess\n parameter group and click \nParameter group actions\n and then \nEdit\n button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\n\n\n\n\n\n\n\n\nlog_bin_trust_function_creators\n\n\n1\n\n\n\n\n\n\nmax_allowed_packet\n\n\n1073741824\n\n\n\n\n\n\nmax_connections\n\n\n16000\n\n\n\n\n\n\nmax_user_connections\n\n\n4294967295\n\n\n\n\n\n\nevent_scheduler\n\n\nON\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nPreview changes\n and double check.\n\n\n\n\nClick \nSave changes\n orange button.\n\n\n\n\n\n\nSet Cluster Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nClusters.\n\n\nClick radio button on your new cluster.\n\n\nClick \nActions\n then \nModify cluster\n button on top.\n\n\nUnder Database options, set \nDB cluster parameter group\n to \nauroraclusterallowawsaccess\n.\n\n\nScroll down and click \nContinue.\n\n\nClick option \nApply immediately\n and click \nModify cluster.\n\n\n\n\nSet Instance Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick radio button on your new instance.\n\n\nClick \nInstance Actions \\ Modify\n button on top.\n\n\nScroll down and under Database options, set \nDB parameter group\n to \naurorainstanceallowawsaccess\n\n\nYou may also notice that \nDB cluster parameter group\n is already set to \nauroraclusterallowawsaccess\n\n\nClick \nContinue\n orange button.\n\n\nSelect \nApply Immediately\n and click \nModify DB Instance.\n\n\nUse refresh icon and wait until Status is available on your instance.\n\n\nClick Instance actions, select Reboot and confirm.\n\n\nUse refresh icon and wait until Status is available on your instance.\n\n\n\n\n\n\nVerify Instance Configuration\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick your new instance.\n\n\n\n\nVerify the following:\n\n\n\n\nDB instance status: \navailable\n\n\nParameter group: \naurorainstanceallowawsaccess (in-sync)\n\n\nDB cluster parameter group: \nauroraclusterallowawsaccess (in-sync)\n\n\nSecurity groups: \nrds-launch-wizard (sg-XXXXXXXX) ( active )\n\n\nPublicly accessible: \nYes\n\n\n\n\n\n\n\n\n\n\nTest MySQL Connection\n\n\n\n\n\n\nDownload and install any \nMySQL client\n of your preference:\n\n\nFor Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"\n\n\n\n\n\n\n\nOn your AWS Console go to \nRDS Dashboard\n, then Clusters, select your new cluster and copy the \nCluster Endpoint\n, which is a blue string with more than 60 characters.\n\n\n\n\n\n\nLaunch your MySQL client and configure a new connection:\n\n\n\n\nName:\n type any name of your preference.\n\n\nHost:\n Paste the Cluster Endpoint.\n\n\nUsername:\n root\n\n\nPassword:\n type the Master Password\n\n\nPort:\n 3306\n\n\nDatabase:\n Leave blank\n\n\nConnect using SSL:\n No\n\n\n\n\n\n\n\n\nClick Connect and verify that you can successfully connect to your RDS instance.\n\n\n\n\n\n\n\n\nConfigure Instance Connection Details on Factor BI Console\n\n\n\n\nLog in to \nFactor BI Console.\n\n\nGo to RDS Instances and then click under Hostname.\n\n\nComplete all fields on the form with instance connection information.\n\n\n\n\n\n\nSecurity of your RDS Instance for Production\n\n\nIf your are ready to use Bipost API for production, we highly recommend the following:\n\n\n\n\nUse \nMySQL client\n to create a new user.\n\n\nSet a strong password.\n\n\nGran the new user with the following: \nGRANT LOAD FROM S3 ON *.* TO 'your-user-name';\n\n\n\n\nSet the following Global Privileges:\n\n\n\n\n\n\n\n\n\n\nSecurity for Downloading Data\n\n\nIf you plan to download data from Aurora to your on-premises databases, there are some settings to make on your AWS account.\n\n\n\n\nOpen \nIAM console.\n\n\nClick \nAdd user\n blue button on top left corner.\n\n\nUser name: \nauroraToS3\n\n\nAccess type: \nProgrammatic access\n\n\n\n\nClick \nNext: Permissions\n blue button lower right corner.\n\n\n\n\n\n\n\n\nSelect \nAttach existing policies directly\n\n\n\n\nOn the search box type \nS3\n and select \nAmazonS3FullAccess\n\n\n\n\nClick \nNext: Review\n blue button lower right corner.\n\n\n\n\n\n\n\n\nClick \nDownload .csv\n.\n\n\n\n\n\n\nEmail the CSV to \ninfo@factorbi.com\n so we can setup the downloading process.\n\n\n\n\n\n\n\n\n\n\nConsole Access to Bucket\n\n\nBipost synchronization uses S3 to upload the data that is extracted from the on-prem database. The bucket is located within Factor BI AWS account so we can efficiently handle API calls, patches and new releases.\n\n\nRemember, we create a unique S3 bucket for each one of our customers, so nothing gets mixed up.\n\n\nSometimes you may want to access this bucket and review files and folders.\n\n\nTo accomplish this we provide an \nAWS Console access\n with a user, password and a direct link to your bucket.", 
            "title": "Advanced AWS Setup"
        }, 
        {
            "location": "/setupaws/#link-your-aws-account", 
            "text": "These instructions are for experienced AWS administrators only.  Follow these instructions if you want to manually to link your AWS Account to Bipost API. Strong knowledge of AWS is needed.  IMPORTANT NOTICE: If you are planning to use the following AWS resources for production you may want to follow your company policies and understand how to use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#canonical-user-id", 
            "text": "Sign in with the  root  AWS account.   Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Expand Account Identifiers.   Copy AWS Account ID (12-digit) and Canonical User ID (64-digit).     Email these numbers to  info@factorbi.com  so we can setup your dedicated Bucket.    We use your  Canonical User ID  to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.", 
            "title": "Canonical User ID"
        }, 
        {
            "location": "/setupaws/#create-iam-policy-to-grant-access-to-s3", 
            "text": "From this point on you need the newly S3 bucket ARN that we provided over email on the previous step.   Open  IAM Console.  In the left navigation pane choose  Policies.  Create policy  blue button.  Select  Policy Generator   Click  JSON  tab.     Copy and paste the following.  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-123456789012\", \n                \"arn:aws:s3:::bipostdata-123456789012/*\"\n            ]\n        }\n    ]\n}    Replace the text  arn:aws:s3:::bipostdata-123456789012  with the bucket ARN you received from us over email.    Double check the JSON text is using your own ARN bucket and click  Review policy  blue button on the lower right.    Enter the following on the Review policy screen.   Name:  AuroraToS3  Name:  Connection to Factor BI bucket.     Click  Create policy  blue button.    Further information from AWS go to:  Allowing Amazon Aurora to Access Amazon S3 Resources", 
            "title": "Create IAM Policy to Grant Access to S3"
        }, 
        {
            "location": "/setupaws/#create-iam-role-to-allow-rds-access-to-s3", 
            "text": "Open  IAM Console.  In the left navigation pane choose  Roles.  Create role  blue button.   Choose  AWS service,  then  RDS     Select your use case  click  RDS - CloudHSM and Directory Service,  click  Next: Permissions  blue button.   Click  Next:Review.  Set  Role name:   RDSLoadFromS3  and click  Create role.  Now from the navigation details, click the role you just created.   Under permissions tab, detach by clicking  X  the following:   AmazonRDSDirectoryServiceAccess  RDSCloudHsmAuthorizationRole     Now click  Attach policy  blue button.     Select  AuroraToS3  and click  Attach policy  blue button.    Copy  Role ARN  string and save it for further use. It may look like this:  arn:aws:iam::123456789012:role/RDSLoadFromS3    Further information from AWS go to:  Creating an IAM Role to Allow Amazon Aurora to Access AWS Services", 
            "title": "Create IAM Role to Allow RDS Access to S3"
        }, 
        {
            "location": "/setupaws/#closest-aws-region", 
            "text": "cloudping.info   Click the above link and hit  HTTP Ping  and look for the lowest latency.  Maybe you want to try this at different times of the day.  Take note of the closest region.", 
            "title": "Closest AWS Region"
        }, 
        {
            "location": "/setupaws/#create-aurora-instance", 
            "text": "", 
            "title": "Create Aurora Instance"
        }, 
        {
            "location": "/setupaws/#instance-specifications", 
            "text": "From AWS Console Home, upper right corner (next to you name) be sure to select the  closest region to your location.  Under menu  Services  search RDS.  From  Amazon RDS  click Instances.  Launch DB Instance, orange button.  Select Engine:  Amazon Aurora , scroll down, Edition:  MySQL 5.7-compatible  click  Next  orange button.  DB instance class: for testing purposes select the smallest available, currently  t2.small  Multi-AZ deployment: for testing purposes select  No  Settings, DB Instance identifier: set a name, lower-case and no special characters.  Master username:  root   Master Password: set a strong password and store it in a secure place.   Password can't contain spaces and the following characters:    /\n\"\n@    Click  Next  orange button.", 
            "title": "Instance specifications"
        }, 
        {
            "location": "/setupaws/#network-security", 
            "text": "Virtual Private Cloud (VPC):  Create new VPC  Subnet group:  Create new DB Subnet Group  Publicly accessible:  Yes  Availability zone:  No Preference  VPC security groups:  Create new VPC security groups", 
            "title": "Network &amp; Security"
        }, 
        {
            "location": "/setupaws/#database-options", 
            "text": "DB Cluster Identifier:  leave blank  Database name:  leave blank  Database port:  3306  DB parameter group:  default.aurora-mysql5.7  DB cluster parameter group:  default.aurora-mysql5.7  Option group:  leave default", 
            "title": "Database options"
        }, 
        {
            "location": "/setupaws/#encryption", 
            "text": "Encryption:  Disable encryption", 
            "title": "Encryption"
        }, 
        {
            "location": "/setupaws/#failover", 
            "text": "Priority:  tier-0", 
            "title": "Failover"
        }, 
        {
            "location": "/setupaws/#backup", 
            "text": "Backup retention period:  1 day", 
            "title": "Backup"
        }, 
        {
            "location": "/setupaws/#monitoring", 
            "text": "Enhanced Monitoring:  Disable enhanced monitoring", 
            "title": "Monitoring"
        }, 
        {
            "location": "/setupaws/#maintenance", 
            "text": "Auto minor version upgrade:  Enable auto minor version upgrade  Maintenance windows:  No preference", 
            "title": "Maintenance"
        }, 
        {
            "location": "/setupaws/#launch-db-instance", 
            "text": "Click Launch DB Instance orange button.    This process may take a few minutes.    Click  View DB Instance details.", 
            "title": "Launch DB Instance"
        }, 
        {
            "location": "/setupaws/#rds-instance-security-group", 
            "text": "Click  Instances  left pane.  Once the new instance has  Status:   available  proceed:   Click your new instance.  Scroll down to  Connect  section.    Under  Security group rules  click the blue string that looks like this   rds-launch-wizard (sg-XXXXXXXX)      You are now on EC2 Management Console and Security Group ID is already selected.   Click  Actions \\ Edit inbound rules  Remove the default Custom TCP rule created.  Click  Add Rule , under Type select  MYSQL/Aurora   Source  Custom  and enter this value:  0.0.0.0/0    Repeat steps 7   8, and type value  ::/0     Click  Save  blue button.   Click  Actions \\ Edit outbound rules  Verify if Type:  All traffic , Destination:  Custom  and value:  0.0.0.0/0  is already set, if not, add the rule.  Go back to  RDS Console , select your instance, click  Instance actions \\ Reboot , confirm with orange button on the right.   Wait until  Status  is  available       Click your DB Instance, scroll down to  Details  section and check if  Security groups  are  ( active )", 
            "title": "RDS Instance Security Group"
        }, 
        {
            "location": "/setupaws/#add-iam-role-to-aurora-cluster", 
            "text": "Open  RDS console.  Choose  Clusters  on left pane.  Click radio button of your newly cluster.   Click  Actions  then  Manage IAM roles.     Under  Add IAM roles to this cluster  select the role you just created:  RDSLoadFromS3  and click  Add role  button.     Wait until you see Status  active  under Current IAM roles for this cluster.   Click  Done .", 
            "title": "Add IAM Role to Aurora Cluster"
        }, 
        {
            "location": "/setupaws/#create-cluster-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create parameter group  orange button on top.   Parameter group family:  aurora-mysql5.7  Type:  DB Cluster Parameter Group  Group name:  AuroraClusterAllowAWSAccess  Description:  Bipost Aurora Database Cluster Parameter Group     Click  Create  orange button and refresh browser.   Click check box on your new  auroraclusterallowawsaccess  parameter group and click  Parameter group actions  and then  Edit  button on top.  Make sure you have your ARN role string ( step 12 here ) and replace it below.   Set the following:     Name  Edit Values  Example      aurora_load_from_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aurora_select_into_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_lambda_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3       Click  Save Changes  orange button.    Click  Preview changes  and it should look like this:     Further information from AWS go to:  Associating an IAM Role with a DB Cluster", 
            "title": "Create Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#create-db-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create parameter group  orange button on top.   Parameter group family:  aurora-mysql5.7  Type:  DB Parameter Group  Group name:  AuroraInstanceAllowAWSAccess  Description:  Bipost Aurora Parameter Group     Click  Create  orange button and refresh browser.   Click check box on your new  aurorainstanceallowawsaccess  parameter group and click  Parameter group actions  and then  Edit  button on top.   Set the following:     Name  Edit Values      log_bin_trust_function_creators  1    max_allowed_packet  1073741824    max_connections  16000    max_user_connections  4294967295    event_scheduler  ON       Click  Preview changes  and double check.   Click  Save changes  orange button.", 
            "title": "Create DB Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-cluster-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Clusters.  Click radio button on your new cluster.  Click  Actions  then  Modify cluster  button on top.  Under Database options, set  DB cluster parameter group  to  auroraclusterallowawsaccess .  Scroll down and click  Continue.  Click option  Apply immediately  and click  Modify cluster.", 
            "title": "Set Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-instance-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click radio button on your new instance.  Click  Instance Actions \\ Modify  button on top.  Scroll down and under Database options, set  DB parameter group  to  aurorainstanceallowawsaccess  You may also notice that  DB cluster parameter group  is already set to  auroraclusterallowawsaccess  Click  Continue  orange button.  Select  Apply Immediately  and click  Modify DB Instance.  Use refresh icon and wait until Status is available on your instance.  Click Instance actions, select Reboot and confirm.  Use refresh icon and wait until Status is available on your instance.", 
            "title": "Set Instance Parameter Group"
        }, 
        {
            "location": "/setupaws/#verify-instance-configuration", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click your new instance.   Verify the following:   DB instance status:  available  Parameter group:  aurorainstanceallowawsaccess (in-sync)  DB cluster parameter group:  auroraclusterallowawsaccess (in-sync)  Security groups:  rds-launch-wizard (sg-XXXXXXXX) ( active )  Publicly accessible:  Yes", 
            "title": "Verify Instance Configuration"
        }, 
        {
            "location": "/setupaws/#test-mysql-connection", 
            "text": "Download and install any  MySQL client  of your preference:  For Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"    On your AWS Console go to  RDS Dashboard , then Clusters, select your new cluster and copy the  Cluster Endpoint , which is a blue string with more than 60 characters.    Launch your MySQL client and configure a new connection:   Name:  type any name of your preference.  Host:  Paste the Cluster Endpoint.  Username:  root  Password:  type the Master Password  Port:  3306  Database:  Leave blank  Connect using SSL:  No     Click Connect and verify that you can successfully connect to your RDS instance.", 
            "title": "Test MySQL Connection"
        }, 
        {
            "location": "/setupaws/#configure-instance-connection-details-on-factor-bi-console", 
            "text": "Log in to  Factor BI Console.  Go to RDS Instances and then click under Hostname.  Complete all fields on the form with instance connection information.", 
            "title": "Configure Instance Connection Details on Factor BI Console"
        }, 
        {
            "location": "/setupaws/#security-of-your-rds-instance-for-production", 
            "text": "If your are ready to use Bipost API for production, we highly recommend the following:   Use  MySQL client  to create a new user.  Set a strong password.  Gran the new user with the following:  GRANT LOAD FROM S3 ON *.* TO 'your-user-name';   Set the following Global Privileges:", 
            "title": "Security of your RDS Instance for Production"
        }, 
        {
            "location": "/setupaws/#security-for-downloading-data", 
            "text": "If you plan to download data from Aurora to your on-premises databases, there are some settings to make on your AWS account.   Open  IAM console.  Click  Add user  blue button on top left corner.  User name:  auroraToS3  Access type:  Programmatic access   Click  Next: Permissions  blue button lower right corner.     Select  Attach existing policies directly   On the search box type  S3  and select  AmazonS3FullAccess   Click  Next: Review  blue button lower right corner.     Click  Download .csv .    Email the CSV to  info@factorbi.com  so we can setup the downloading process.", 
            "title": "Security for Downloading Data"
        }, 
        {
            "location": "/setupaws/#console-access-to-bucket", 
            "text": "Bipost synchronization uses S3 to upload the data that is extracted from the on-prem database. The bucket is located within Factor BI AWS account so we can efficiently handle API calls, patches and new releases.  Remember, we create a unique S3 bucket for each one of our customers, so nothing gets mixed up.  Sometimes you may want to access this bucket and review files and folders.  To accomplish this we provide an  AWS Console access  with a user, password and a direct link to your bucket.", 
            "title": "Console Access to Bucket"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting\n\n\nWhen you use manual sync and it is completed successfully, the following dialog appears.\n\n\n\n\nIf you face any errors, try looking here.\n\n\n\n\nSyntax error\n\n\nMisspelled fields or tables on customData.json may appear as:\n\n\n\n\n\n\nNo Internet Connection\n\n\nWhen no internet connection available, the following message appears:\n\n\n\n\n\n\nFirewall Restrictions\n\n\nIf your internet connection has a firewall, it may show different errors like:\n\n\n\n\nThe remote name could not be resolved.\n\n\nA WebException with status SendFailure was thrown.\n\n\nA WebException with status NameResolutionFailure was thrown.\n\n\nError making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.\n\n\n\n\n\n\nGrant Firewall to reach Amazon S3\n\n\nCreate a policy to Allow to:\n\n\n\n\n54.230.0.0/15\n\n\n52.192.0.0/11\n\n\n\n\n\n\nAWS can have multiple IP addresses for S3 service, so in case the above IP's don't work check the \nAWS Public IP Address Ranges documentation\n and look for\n\n\n  \"region\": \"GLOBAL\",\n  \"service\": \"AMAZON\"\n\n\n\nand\n\n\n  \"region\": \"us-east-1\",\n  \"service\": \"AMAZON\"\n\n\n\nhttps://ip-ranges.amazonaws.com/ip-ranges.json\n\n\n\n\nNo information to Sync\n\n\nIf \nNo information to Sync\n message appears, verify that customData.json is set to send at least one table.\n\n\n\n\nWaiting time\n\n\nBipost Sync may take from a few seconds to several minutes to extract from on-prem DB and upload to AWS. While this is happening no messages/icons will show that biPost.exe is working and maybe you'll see \n(Not responding)\n on the top of the window, this is normal.\n\n\nIf you launch Windows Task Manager probably you'll see that \nbiPost.exe *32\n is running and consuming a considerable amount of CPU.\n\n\nOnce the information is uploaded to AWS, it usually is available on Aurora-MySQL very fast. If a big data set was uploaded it may take up to 5 minutes to be available on Aurora.\n\n\nIf you need to check which tables where loaded, check \naurora_s3_load_history.\n\n\n\n\nUpload Limit\n\n\nDepending on the number of rows and columns on each table, it is possible that a large amount of data sent on a single sync may not load to Aurora-MySQL.\n\n\nWe have tested up to 1.5 million rows on a single sync and works fine.\n\n\nWe recommend using \nRecursive Sync\n for big tables that have a datetime field available.\n\n\n\n\nSpecial Characters\n\n\nSome special characters on \nchar\n and \nvarchar\n fields are not supported and thus removed by biPost.exe\nFor example:\n\n\n\n\nEnter\n\n\n()\n\n\n\n\nOn \nSQL Server\n, all special characters on strings of 100 length or more are removed, leaving only letters and numbers.\n\n\n\n\nSchema Limitations\n\n\n\n\nTables without a \nPRIMARY KEY\n on SQL Server and Firebird will display an error message.\n\n\n\n\n\n\nAppData\\Local folder\n\n\nSometimes it is necessary to manually delete the content of \n\\AppData\\Local\\biPost\n folder.\n\n\nOpen a new Windows Explorer and enter \n%localappdata%\\bipost\n. Select all and delete.\n\n\n************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.\n\n\n\n\n\nMySQL schemas are created but no data is loaded\n\n\nTwo things might be causing this problem:\n\n\n1. RDS instance cannot reach S3 bucket.\n\n\nWhen we look at our CloudWatch logs, we see \nUnable to initialize S3Stream\n, so do the following:\n\n\n\n\n\n\nCheck if your \nIAM Policy to Grant Access to S3\n is set correctly using the S3 bucket ARN we provided. Also double check the policy document (JSON).\n\n\n\n\n\n\nCheck that \nIAM Role\n has attached the former IAM Policy. Copy ARN Role to a notepad for next steps.\n\n\n\n\n\n\nGo to \nRDS Parameter Groups\n, select the cluster group and click \nCompare Parameters\n, it should show the IAM ARN Role (the one you just copied on a notepad) on the parameters shown \nhere.\n\n\n\n\n\n\nGo to \nRDS Clusters\n and check if \nIAM Role\n is listed and active for your cluster.\n\n\n\n\n\n\nDouble check IAM roles attached to your instance querying \nshow global variables like '%role%'\n\n\n\n\n\n\n\n\nAfter this, if you still experience this error, check out \nManually debugging S3Stream.\n\n\n2. Name of your destination database must be all lower case.\n\n\nWhen we look at our CloudWatch logs, we see:\n\n\nSequelizeConnectionError: ER_BAD_DB_ERROR: Unknown database\n\n\nDouble check that your DB name is all lower case.\n\n\nManually debugging S3Stream\n\n\nIn this section we will manually upload data to Aurora-MySQL. The goal here is to see whether an error is shown while directly importing data from S3 to Aurora-MySQL.\n\n\nUsing MySQL Workbench (or any client of your preference), open a connection to your MySQL instance preferably using \nroot\n account.\n\n\nSyntax\n\n\nLOAD DATA FROM S3 's3-us-east-1://{my-bucket-name}/{my-36-digit-service-number}/process_{my-36-digit-service-number}{YYYY_M_D-of-yesterday}_post/{my-example-table}.csv'\nREPLACE INTO TABLE `{my-database-name}`.`{my-example-table}`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n({column_1},{column_2},...,{column_n});\n\n\n\nReplace curly brackets with your data.\n\n\nParameters\n\n\n\n\nmy-bucket-name\n: Name of your dedicated bucket, assigned by Factor BI.\n\n\nmy-36-digit-service-number\n: Number assigned to your service.\n\n\nYYYY_M_D-of-yesterday\n: Date for yesterday on GMT America/Mexico_City.\n\n\nmy-example-table\n: Pick any of the tables that you are synchronizing.\n\n\nmy-database-name\n: Name of your destination DB on MySQL.\n\n\n{column_1},{column_2},...,{column_n}\n: List all the columns in the same order as they appear in the .CSV file created on \nAppData\\Local folder\n.\n\n\n\n\nExample\n\n\nLOAD DATA FROM S3 's3-us-east-1://bipostdata-f0123abc4567/a1bcd23e-4fa5-67b8-cd9e-f0123abc4567/process_a1bcd23e-4fa5-67b8-cd9e-f0123abc45672017_9_13_post/CLIE.csv'\nREPLACE INTO TABLE `mytestdb`.`clie`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CLAVE,STATUS,NOMBRE,RFC,CALLE,NUMINT,NUMEXT,CRUZAMIENTOS,CRUZAMIENTOS2,COLONIA,CODIGO,LOCALIDAD,MUNICIPIO,ESTADO,NACIONALIDAD,REFERDIR,TELEFONO,CLASIFIC,FAX,PAG_WEB,CURP,CVE_ZONA,IMPRIR,MAIL,NIVELSEC,ENVIOSILEN,EMAILPRED,DIAREV,DIAPAGO,CON_CREDITO,DIASCRED,LIMCRED,SALDO,LISTA_PREC,CVE_BITA,ULT_PAGOD,ULT_PAGOM,ULT_PAGOF,DESCUENTO,ULT_VENTAD,ULT_COMPM,FCH_ULTCOM,VENTAS,CVE_VEND,CVE_OBS,TIPO_EMPRESA,MATRIZ,PROSPECTO,CALLE_ENVIO,NUMINT_ENVIO,NUMEXT_ENVIO,CRUZAMIENTOS_ENVIO,CRUZAMIENTOS_ENVIO2,COLONIA_ENVIO,LOCALIDAD_ENVIO,MUNICIPIO_ENVIO,ESTADO_ENVIO,PAIS_ENVIO,CODIGO_ENVIO,CVE_ZONA_ENVIO,REFERENCIA_ENVIO,CUENTA_CONTABLE,ADDENDAF,ADDENDAD,NAMESPACE,METODODEPAGO,NUMCTAPAGO,MODELO,DES_IMPU1,DES_IMPU2,DES_IMPU3,DES_IMPU4,DES_PER,LAT_GENERAL,LON_GENERAL,LAT_ENVIO,LON_ENVIO,UUID,VERSION_SINC,USO_CFDI,CVE_PAIS_SAT,NUMIDREGFISCAL,FORMADEPAGOSAT);\n\n\n\n\n\nLOAD FROM S3 privileges\n\n\nThe Aurora user that executes \nLOAD DATA FROM S3\n requires the following privilege:\n\n\nGRANT LOAD FROM S3 ON *.* TO 'your-user-name';\n\n\nBy default this privilege is set to your \nMaster Username\n when you \ncreated your Aurora instance.\n\n\nIf you are using a different user and the privilege is not set, the following error appears:\n\n\nAccess denied; you need (at least one of) the LOAD FROM S3 privilege(s) for this operation\n\n\nThe only way to see this error is executing \nLOAD FROM S3\n \nmanually.\n\n\nIf your MySQL user already has this privilege and you see the following error, try \nthese steps.\n\n\nAccess denied for user 'your-user-name'@'xx.xx.xxx.xxx' (using password: YES)\n\n\n\n\nFirebird column name starts with underscore\n\n\nFirebird SQL does not naturally support creating a column starting with underscore, so avoid that on Aurora if your source DB is Firebird.\n\n\nToken unknown - line 1\n\n\n\n\n\n\nNeed more help?\n\n\nPlease send us an email to: \ninfo@factorbi.com", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "When you use manual sync and it is completed successfully, the following dialog appears.   If you face any errors, try looking here.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#syntax-error", 
            "text": "Misspelled fields or tables on customData.json may appear as:", 
            "title": "Syntax error"
        }, 
        {
            "location": "/troubleshooting/#no-internet-connection", 
            "text": "When no internet connection available, the following message appears:", 
            "title": "No Internet Connection"
        }, 
        {
            "location": "/troubleshooting/#firewall-restrictions", 
            "text": "If your internet connection has a firewall, it may show different errors like:   The remote name could not be resolved.  A WebException with status SendFailure was thrown.  A WebException with status NameResolutionFailure was thrown.  Error making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.", 
            "title": "Firewall Restrictions"
        }, 
        {
            "location": "/troubleshooting/#grant-firewall-to-reach-amazon-s3", 
            "text": "Create a policy to Allow to:   54.230.0.0/15  52.192.0.0/11    AWS can have multiple IP addresses for S3 service, so in case the above IP's don't work check the  AWS Public IP Address Ranges documentation  and look for    \"region\": \"GLOBAL\",\n  \"service\": \"AMAZON\"  and    \"region\": \"us-east-1\",\n  \"service\": \"AMAZON\"  https://ip-ranges.amazonaws.com/ip-ranges.json", 
            "title": "Grant Firewall to reach Amazon S3"
        }, 
        {
            "location": "/troubleshooting/#no-information-to-sync", 
            "text": "If  No information to Sync  message appears, verify that customData.json is set to send at least one table.", 
            "title": "No information to Sync"
        }, 
        {
            "location": "/troubleshooting/#waiting-time", 
            "text": "Bipost Sync may take from a few seconds to several minutes to extract from on-prem DB and upload to AWS. While this is happening no messages/icons will show that biPost.exe is working and maybe you'll see  (Not responding)  on the top of the window, this is normal.  If you launch Windows Task Manager probably you'll see that  biPost.exe *32  is running and consuming a considerable amount of CPU.  Once the information is uploaded to AWS, it usually is available on Aurora-MySQL very fast. If a big data set was uploaded it may take up to 5 minutes to be available on Aurora.  If you need to check which tables where loaded, check  aurora_s3_load_history.", 
            "title": "Waiting time"
        }, 
        {
            "location": "/troubleshooting/#upload-limit", 
            "text": "Depending on the number of rows and columns on each table, it is possible that a large amount of data sent on a single sync may not load to Aurora-MySQL.  We have tested up to 1.5 million rows on a single sync and works fine.  We recommend using  Recursive Sync  for big tables that have a datetime field available.", 
            "title": "Upload Limit"
        }, 
        {
            "location": "/troubleshooting/#special-characters", 
            "text": "Some special characters on  char  and  varchar  fields are not supported and thus removed by biPost.exe\nFor example:   Enter  ()   On  SQL Server , all special characters on strings of 100 length or more are removed, leaving only letters and numbers.", 
            "title": "Special Characters"
        }, 
        {
            "location": "/troubleshooting/#schema-limitations", 
            "text": "Tables without a  PRIMARY KEY  on SQL Server and Firebird will display an error message.", 
            "title": "Schema Limitations"
        }, 
        {
            "location": "/troubleshooting/#appdatalocal-folder", 
            "text": "Sometimes it is necessary to manually delete the content of  \\AppData\\Local\\biPost  folder.  Open a new Windows Explorer and enter  %localappdata%\\bipost . Select all and delete.  ************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.", 
            "title": "AppData\\Local folder"
        }, 
        {
            "location": "/troubleshooting/#mysql-schemas-are-created-but-no-data-is-loaded", 
            "text": "Two things might be causing this problem:", 
            "title": "MySQL schemas are created but no data is loaded"
        }, 
        {
            "location": "/troubleshooting/#1-rds-instance-cannot-reach-s3-bucket", 
            "text": "When we look at our CloudWatch logs, we see  Unable to initialize S3Stream , so do the following:    Check if your  IAM Policy to Grant Access to S3  is set correctly using the S3 bucket ARN we provided. Also double check the policy document (JSON).    Check that  IAM Role  has attached the former IAM Policy. Copy ARN Role to a notepad for next steps.    Go to  RDS Parameter Groups , select the cluster group and click  Compare Parameters , it should show the IAM ARN Role (the one you just copied on a notepad) on the parameters shown  here.    Go to  RDS Clusters  and check if  IAM Role  is listed and active for your cluster.    Double check IAM roles attached to your instance querying  show global variables like '%role%'     After this, if you still experience this error, check out  Manually debugging S3Stream.", 
            "title": "1. RDS instance cannot reach S3 bucket."
        }, 
        {
            "location": "/troubleshooting/#2-name-of-your-destination-database-must-be-all-lower-case", 
            "text": "When we look at our CloudWatch logs, we see:  SequelizeConnectionError: ER_BAD_DB_ERROR: Unknown database  Double check that your DB name is all lower case.", 
            "title": "2. Name of your destination database must be all lower case."
        }, 
        {
            "location": "/troubleshooting/#manually-debugging-s3stream", 
            "text": "In this section we will manually upload data to Aurora-MySQL. The goal here is to see whether an error is shown while directly importing data from S3 to Aurora-MySQL.  Using MySQL Workbench (or any client of your preference), open a connection to your MySQL instance preferably using  root  account.  Syntax  LOAD DATA FROM S3 's3-us-east-1://{my-bucket-name}/{my-36-digit-service-number}/process_{my-36-digit-service-number}{YYYY_M_D-of-yesterday}_post/{my-example-table}.csv'\nREPLACE INTO TABLE `{my-database-name}`.`{my-example-table}`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n({column_1},{column_2},...,{column_n});  Replace curly brackets with your data.  Parameters   my-bucket-name : Name of your dedicated bucket, assigned by Factor BI.  my-36-digit-service-number : Number assigned to your service.  YYYY_M_D-of-yesterday : Date for yesterday on GMT America/Mexico_City.  my-example-table : Pick any of the tables that you are synchronizing.  my-database-name : Name of your destination DB on MySQL.  {column_1},{column_2},...,{column_n} : List all the columns in the same order as they appear in the .CSV file created on  AppData\\Local folder .   Example  LOAD DATA FROM S3 's3-us-east-1://bipostdata-f0123abc4567/a1bcd23e-4fa5-67b8-cd9e-f0123abc4567/process_a1bcd23e-4fa5-67b8-cd9e-f0123abc45672017_9_13_post/CLIE.csv'\nREPLACE INTO TABLE `mytestdb`.`clie`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CLAVE,STATUS,NOMBRE,RFC,CALLE,NUMINT,NUMEXT,CRUZAMIENTOS,CRUZAMIENTOS2,COLONIA,CODIGO,LOCALIDAD,MUNICIPIO,ESTADO,NACIONALIDAD,REFERDIR,TELEFONO,CLASIFIC,FAX,PAG_WEB,CURP,CVE_ZONA,IMPRIR,MAIL,NIVELSEC,ENVIOSILEN,EMAILPRED,DIAREV,DIAPAGO,CON_CREDITO,DIASCRED,LIMCRED,SALDO,LISTA_PREC,CVE_BITA,ULT_PAGOD,ULT_PAGOM,ULT_PAGOF,DESCUENTO,ULT_VENTAD,ULT_COMPM,FCH_ULTCOM,VENTAS,CVE_VEND,CVE_OBS,TIPO_EMPRESA,MATRIZ,PROSPECTO,CALLE_ENVIO,NUMINT_ENVIO,NUMEXT_ENVIO,CRUZAMIENTOS_ENVIO,CRUZAMIENTOS_ENVIO2,COLONIA_ENVIO,LOCALIDAD_ENVIO,MUNICIPIO_ENVIO,ESTADO_ENVIO,PAIS_ENVIO,CODIGO_ENVIO,CVE_ZONA_ENVIO,REFERENCIA_ENVIO,CUENTA_CONTABLE,ADDENDAF,ADDENDAD,NAMESPACE,METODODEPAGO,NUMCTAPAGO,MODELO,DES_IMPU1,DES_IMPU2,DES_IMPU3,DES_IMPU4,DES_PER,LAT_GENERAL,LON_GENERAL,LAT_ENVIO,LON_ENVIO,UUID,VERSION_SINC,USO_CFDI,CVE_PAIS_SAT,NUMIDREGFISCAL,FORMADEPAGOSAT);", 
            "title": "Manually debugging S3Stream"
        }, 
        {
            "location": "/troubleshooting/#load-from-s3-privileges", 
            "text": "The Aurora user that executes  LOAD DATA FROM S3  requires the following privilege:  GRANT LOAD FROM S3 ON *.* TO 'your-user-name';  By default this privilege is set to your  Master Username  when you  created your Aurora instance.  If you are using a different user and the privilege is not set, the following error appears:  Access denied; you need (at least one of) the LOAD FROM S3 privilege(s) for this operation  The only way to see this error is executing  LOAD FROM S3   manually.  If your MySQL user already has this privilege and you see the following error, try  these steps.  Access denied for user 'your-user-name'@'xx.xx.xxx.xxx' (using password: YES)", 
            "title": "LOAD FROM S3 privileges"
        }, 
        {
            "location": "/troubleshooting/#firebird-column-name-starts-with-underscore", 
            "text": "Firebird SQL does not naturally support creating a column starting with underscore, so avoid that on Aurora if your source DB is Firebird.  Token unknown - line 1", 
            "title": "Firebird column name starts with underscore"
        }, 
        {
            "location": "/troubleshooting/#need-more-help", 
            "text": "Please send us an email to:  info@factorbi.com", 
            "title": "Need more help?"
        }, 
        {
            "location": "/about/", 
            "text": "History\n\n\nBipost Sync was once designed to simplify the use of \nCloud Business Intelligence\n tools.\n\n\nOver time we started using Bipost Sync to seamless connect on-premises ERP's to AWS cloud.\n\n\n\n\nEnterprise Applications\n\n\nWant to build a tailored made \nserverless\n enterprise web application connected to your on-premises Relational Database System?\n\n\n\n\n\n\nContact Us\n\n\nWrite us about your project: \ninfo@factorbi.com", 
            "title": "About"
        }, 
        {
            "location": "/about/#history", 
            "text": "Bipost Sync was once designed to simplify the use of  Cloud Business Intelligence  tools.  Over time we started using Bipost Sync to seamless connect on-premises ERP's to AWS cloud.", 
            "title": "History"
        }, 
        {
            "location": "/about/#enterprise-applications", 
            "text": "Want to build a tailored made  serverless  enterprise web application connected to your on-premises Relational Database System?", 
            "title": "Enterprise Applications"
        }, 
        {
            "location": "/about/#contact-us", 
            "text": "Write us about your project:  info@factorbi.com", 
            "title": "Contact Us"
        }
    ]
}