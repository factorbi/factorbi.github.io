{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nAnnouncing General Availability!\n \n\n\nBipost is a simple database synchronization tool built for developers in mind, fully deployed on AWS.\n\n\nSynchronize \nMicrosoft SQL Server\u00ae\n and \nFirebird SQL\n (running on Windows) to any given \nAmazon Aurora MySQL\n instance.\n\n\nTwo-way database synchronization also available, from AWS back to your on-prem.\n\n\nIt is created to keep your databases on-premises while providing a way to extract and load specific sets of data to AWS Aurora.\n\n\n\n\n\n\nHow it works\n\n\n\n\nOn every sync our tool reads table schema's and data and uploads it to AWS.\n\n\nFully customize your data sets to upload using \ncustomData.json\n\n\nDatabase and tables are created/altered if they don't exist, and data is loaded to Aurora-MySQL.\n\n\nBefore and after data is loaded to Aurora-MySQL you can transform your data with stored procedures.\n\n\nUpload the same data multiple times and avoid duplicates. Data is replaced using primary keys.\n\n\nSync can run manually or automatically with a scheduled \nWindows Task.\n\n\nUpload big datasets using \nRecursive Sync.\n\n\n\n\nData is also available as CSV files on S3 so you can use other AWS services like \nAthena\n and \nGlue\n and build your data lake.\n\n\nTwo-way synchronization\n\n\n\n\nSynchronize from Aurora-MySQL to on-premises SQL Server or Firebird SQL.\n\n\nFully customize what data you want to download using \noutData.json\n\n\nDecide whether you want to insert/update the returned data to your on-prem DB or just save the CSV files on Windows.\n\n\nTables schemas are created/altered on your on-prem DB if they don't exist.\n\n\nPrimary keys set on Aurora-MySQL are used on your on-prem DB to avoid duplicates.\n\n\nBefore and after data is loaded to your on-prem DB you can transform your data with stored procedures.\n\n\nLearn more \nhere.\n\n\n\n\n\n\nUse Cases\n\n\n\n\nGreat way to consolidate information from separate databases and locations, e.g. merge your sales and inventory information from different branches.\n\n\nIdeal to extend your on-premises ERP to the cloud and build web applications, web services and API's on top of AWS cloud platform, with services such as API Gateway and Lambda.\n\n\nPower your ERP with supper low cost Business Intelligence dashboards using \nGoogle Data Studio\n or \nAWS QuickSight\n, with a direct connection to Aurora-MySQL.\n\n\n\n\n\n\nPrivate Cloud\n\n\nWe care deeply about privacy.\n\n\nOur API calls your RDS instance on your AWS account, so you have full control of your databases.\n\n\nEach RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.\n\n\nArchitecture\n\n\n\n\nAurora\n is a MySQL compatible, fully managed database service, built for the cloud with the performance and scalability of high-end commercial databases. \n\n\n\n\nStart Using\n\n\n\n\n30 days free\n\n\nUnlimited databases and synchronizations. \n\n\nNo need to provide credit card information.\n\n\nNot familiar with AWS or just want to skip creating AWS releated services? \nWrite us.\n\n\n\n\nRegister here.\n\n\nOr email us: \ninfo@factorbi.com\n\n\n\n\nPrices\n\n\nPrincing here: \nwww.factorbi.com\n\n\nFirebird Community, \nMembers to Members Offer available.\n\n\nMembers of \nComunidad de AWS en Espa\u00f1ol\n, ask for special deal.\n\n\n\n\nStaff Stories\n\n\nA journey from on-premises to Cloud Business Intelligence\n\n\n\n\nWhy we dropped Microsoft Power BI and embraced AWS QuickSight\n\n\n\n\n\n\nRelease Notes\n\n\n1.0.0 (General Availability) 2018-03-02\n\n\n\n\nData upload is now done through secure HTTPS.\n\n\nJOIN parameter now supported for Firebird SQL.\n\n\nRecursive sync\n now supported for Firebird SQL.\n\n\nConnection information is now encrypted.\n\n\nBug fixes.\n\n\n\n\n0.5.6 (Beta) 2017-12-02\n\n\n\n\nBidirectional syncing is here!\n\n\nSynchronize to any AWS Region.\n\n\nPerformance improvements to API, now able to load nearly 1.5 million rows (or 280 MB uncompressed files) on a single call. Future releases will support much more since \nAWS Lambda recently doubled maximum memory capacity to 3 GB.\n\n\nFirebird transaction READ UNCOMMITTED to prevent Bipost Sync from being stopped while other transactions are still not committed.\n\n\nInitial and final statements on Aurora MySQL are disabled on recursive sync. This prevents excessive workload on your RDS instance.\n\n\nBug fixes to API and Bipost Sync.\n\n\n\n\n0.4.2 (Beta) 2017-09-16\n\n\n\n\nTable schemas are now synchronized against source definition on every sync, details \nhere.\n\n\nBipost Sync bug fixes.\n\n\nBipost API bug fixes.\n\n\n\n\n0.4.0 (Beta) 2017-08-20\n\n\n\n\nCustom connections added.\n\n\nInitial statement\n added to API.\n\n\nSpecial characters are deleted on string columns of 100 characters length or up.\n\n\n\n\n\n\nContact\n\n\nWe are always happy to hear about you.\n\n\nPlease send us an email to: \ninfo@factorbi.com", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Announcing General Availability!    Bipost is a simple database synchronization tool built for developers in mind, fully deployed on AWS.  Synchronize  Microsoft SQL Server\u00ae  and  Firebird SQL  (running on Windows) to any given  Amazon Aurora MySQL  instance.  Two-way database synchronization also available, from AWS back to your on-prem.  It is created to keep your databases on-premises while providing a way to extract and load specific sets of data to AWS Aurora.", 
            "title": "Overview"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "On every sync our tool reads table schema's and data and uploads it to AWS.  Fully customize your data sets to upload using  customData.json  Database and tables are created/altered if they don't exist, and data is loaded to Aurora-MySQL.  Before and after data is loaded to Aurora-MySQL you can transform your data with stored procedures.  Upload the same data multiple times and avoid duplicates. Data is replaced using primary keys.  Sync can run manually or automatically with a scheduled  Windows Task.  Upload big datasets using  Recursive Sync.   Data is also available as CSV files on S3 so you can use other AWS services like  Athena  and  Glue  and build your data lake.", 
            "title": "How it works"
        }, 
        {
            "location": "/#two-way-synchronization", 
            "text": "Synchronize from Aurora-MySQL to on-premises SQL Server or Firebird SQL.  Fully customize what data you want to download using  outData.json  Decide whether you want to insert/update the returned data to your on-prem DB or just save the CSV files on Windows.  Tables schemas are created/altered on your on-prem DB if they don't exist.  Primary keys set on Aurora-MySQL are used on your on-prem DB to avoid duplicates.  Before and after data is loaded to your on-prem DB you can transform your data with stored procedures.  Learn more  here.", 
            "title": "Two-way synchronization"
        }, 
        {
            "location": "/#use-cases", 
            "text": "Great way to consolidate information from separate databases and locations, e.g. merge your sales and inventory information from different branches.  Ideal to extend your on-premises ERP to the cloud and build web applications, web services and API's on top of AWS cloud platform, with services such as API Gateway and Lambda.  Power your ERP with supper low cost Business Intelligence dashboards using  Google Data Studio  or  AWS QuickSight , with a direct connection to Aurora-MySQL.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/#private-cloud", 
            "text": "We care deeply about privacy.  Our API calls your RDS instance on your AWS account, so you have full control of your databases.  Each RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.", 
            "title": "Private Cloud"
        }, 
        {
            "location": "/#architecture", 
            "text": "Aurora  is a MySQL compatible, fully managed database service, built for the cloud with the performance and scalability of high-end commercial databases.", 
            "title": "Architecture"
        }, 
        {
            "location": "/#start-using", 
            "text": "30 days free  Unlimited databases and synchronizations.   No need to provide credit card information.  Not familiar with AWS or just want to skip creating AWS releated services?  Write us.   Register here.  Or email us:  info@factorbi.com", 
            "title": "Start Using"
        }, 
        {
            "location": "/#prices", 
            "text": "Princing here:  www.factorbi.com  Firebird Community,  Members to Members Offer available.  Members of  Comunidad de AWS en Espa\u00f1ol , ask for special deal.", 
            "title": "Prices"
        }, 
        {
            "location": "/#staff-stories", 
            "text": "A journey from on-premises to Cloud Business Intelligence   Why we dropped Microsoft Power BI and embraced AWS QuickSight", 
            "title": "Staff Stories"
        }, 
        {
            "location": "/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/#100-general-availability-2018-03-02", 
            "text": "Data upload is now done through secure HTTPS.  JOIN parameter now supported for Firebird SQL.  Recursive sync  now supported for Firebird SQL.  Connection information is now encrypted.  Bug fixes.", 
            "title": "1.0.0 (General Availability) 2018-03-02"
        }, 
        {
            "location": "/#056-beta-2017-12-02", 
            "text": "Bidirectional syncing is here!  Synchronize to any AWS Region.  Performance improvements to API, now able to load nearly 1.5 million rows (or 280 MB uncompressed files) on a single call. Future releases will support much more since  AWS Lambda recently doubled maximum memory capacity to 3 GB.  Firebird transaction READ UNCOMMITTED to prevent Bipost Sync from being stopped while other transactions are still not committed.  Initial and final statements on Aurora MySQL are disabled on recursive sync. This prevents excessive workload on your RDS instance.  Bug fixes to API and Bipost Sync.", 
            "title": "0.5.6 (Beta) 2017-12-02"
        }, 
        {
            "location": "/#042-beta-2017-09-16", 
            "text": "Table schemas are now synchronized against source definition on every sync, details  here.  Bipost Sync bug fixes.  Bipost API bug fixes.", 
            "title": "0.4.2 (Beta) 2017-09-16"
        }, 
        {
            "location": "/#040-beta-2017-08-20", 
            "text": "Custom connections added.  Initial statement  added to API.  Special characters are deleted on string columns of 100 characters length or up.", 
            "title": "0.4.0 (Beta) 2017-08-20"
        }, 
        {
            "location": "/#contact", 
            "text": "We are always happy to hear about you.  Please send us an email to:  info@factorbi.com", 
            "title": "Contact"
        }, 
        {
            "location": "/registration/", 
            "text": "Create your Account\n\n\nCreate your \nAccount here.\n \n\n\nClick \nCreate Your Account\n and follow steps.\n\n\n\n\n\n\nConfigure your first service\n\n\n\n\nLog in to the \nconsole.\n\n\nOn the left pane, go to \nService Numbers\n, click \nEdit\n then \nNew\n.\n\n\nEdit database name and click \nEdit\n to save.\n\n\nGo to \nRDS Instances\n and click under \nHostname\n.\n\n\n\n\nDon't have an AWS Account? please \nstart here.\n\n\n\n\nHave an AWS Account?\n\n\nIf you already have an AWS Account, please follow:\n\n\n\n\nTake note of your Account ID and Canonical User ID following --\n \nthese steps.\n\n\nLog in to \nFactor BI Console.\n\n\nGo to RDS Instances and then click under Hostname.\n\n\nGo to the bottom of the form and fill in your AWS Account ID and Canonical. Click Save.\n\n\n\n\nEmail to \ninfo@factorbi.com\n letting us know you completed the above steps so far, so we can create and configure a dedicated bucket for your API calls. \n\n\nBe sure to email us from same email address as your registration.\n\n\n\n\nYour dedicated bucket\n\n\nWhen you receive from us your dedicated bucket, please proceed creating the \nrest of necessary AWS services here.\n\n\nAfter you complete this guide you're done!, now \ndownload and setup\n Bipost Sync on your Windows.", 
            "title": "Register"
        }, 
        {
            "location": "/registration/#create-your-account", 
            "text": "Create your  Account here.    Click  Create Your Account  and follow steps.", 
            "title": "Create your Account"
        }, 
        {
            "location": "/registration/#configure-your-first-service", 
            "text": "Log in to the  console.  On the left pane, go to  Service Numbers , click  Edit  then  New .  Edit database name and click  Edit  to save.  Go to  RDS Instances  and click under  Hostname .   Don't have an AWS Account? please  start here.", 
            "title": "Configure your first service"
        }, 
        {
            "location": "/registration/#have-an-aws-account", 
            "text": "If you already have an AWS Account, please follow:   Take note of your Account ID and Canonical User ID following --   these steps.  Log in to  Factor BI Console.  Go to RDS Instances and then click under Hostname.  Go to the bottom of the form and fill in your AWS Account ID and Canonical. Click Save.   Email to  info@factorbi.com  letting us know you completed the above steps so far, so we can create and configure a dedicated bucket for your API calls.   Be sure to email us from same email address as your registration.", 
            "title": "Have an AWS Account?"
        }, 
        {
            "location": "/registration/#your-dedicated-bucket", 
            "text": "When you receive from us your dedicated bucket, please proceed creating the  rest of necessary AWS services here.  After you complete this guide you're done!, now  download and setup  Bipost Sync on your Windows.", 
            "title": "Your dedicated bucket"
        }, 
        {
            "location": "/setupaws/", 
            "text": "Link your AWS Account\n\n\nFollow these instructions to link your AWS Account and RDS Aurora instance to Bipost API.\n\n\nIMPORTANT NOTICE: Many settings suggested here are for testing purposes. If you are to use the following AWS services for production you may want to follow your company policies and understand how to use AWS security according to your needs.\n\n\n\n\nDon't have an AWS account?\n\n\nNot familiar with AWS or just want to skip creating AWS releated services? \nWrite us.\n\n\n\n\n\n\nCreate an AWS account here \naws.amazon.com\n\n\n\n\n\n\n\n\nAWS usually makes an automated verification phone call, we suggest to provide a land line.\n\n\n\n\nProvide payment information.\n\n\nSelect Basic Support (free plan).\n\n\nCheck if you can open \nRDS Dashboard\n, by searching under \nAWS services\n.\n\n\nCongrats you have an AWS account!\n\n\n\n\n\n\nCheck closest AWS Region to you location\n\n\ncloudping.info\n\n\nClick the above link and hit \nHTTP Ping\n and look for the lowest latency.\n\n\nMaybe you want to try this at different times of the day.\n\n\nTake note of the closest region.\n\n\n\n\n\n\nGet Canonical User ID from your IAM Home\n\n\nTo perform the following steps, you need to sign in with the \nroot\n AWS account.\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nAccount Identifiers.\n\n\n\n\nCopy AWS Account ID (12-digit) and Canonical User ID (64-digit).\n\n\n\n\n\n\n\n\nPaste these numbers on Factor BI Console, RDS Instances section, following --\n \nthese steps.\n\n\n\n\n\n\nQ: Is it secure to provide these numbers?\n\n\nYes, we use your \nCanonical User ID\n to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.\n\n\n\n\nCreate Aurora Instance\n\n\nAurora DB Details\n\n\n\n\nFrom AWS Console Home, upper right corner (next to you name) be sure to select the \nclosest region to your location.\n\n\nFrom AWS Console Home, search RDS.\n\n\nFrom \nRDS Dashboard\n, click Instances.\n\n\nLaunch DB Instance, blue button.\n\n\nSelect Engine: \nAmazon Aurora\n, click select.\n\n\nDB Instance Class: for testing purposes select the smallest available, currently \nt2.small\n\n\nMulti-AZ Deployment: for testing purposes select \nNo\n\n\nDB Instance Identifier: assign a name, lower-case and no special characters.\n\n\nMaster Username: \nroot\n\n\nMaster Password: assign a hard password and store it in a secure place.\n\n\nConfirm Password.\n\n\nOn your left pane it is displayed an estimated monthly cost. For further information check On-Demand Pricing: \nRDS Pricing\n\n\nClick Next Step, blue button.\n\n\n\n\n\n\nAurora Network \n Security\n\n\n\n\nVPC: \nCreate new VPC\n\n\nSubnet Group: \nCreate new DB Subnet Group\n\n\nPublicly Accessible: \nYes\n\n\nAvailability Zone: \nNo Preference\n\n\nVPC Security Group: \nCreate new Security Group\n\n\n\n\n\n\nAurora Database Options\n\n\n\n\nDB Cluster Identifier: \nleave blank\n\n\nDatabase Name: \nleave blank\n\n\nDatabase Port: \n3306\n\n\nDB Parameter Group: \ndefault.aurora5.6\n\n\nDB Cluster Parameter Group: \ndefault.aurora5.6\n\n\nOption Group: \nleave default\n\n\nEnable Encryption: \nNo\n\n\n\n\nAurora Failover\n\n\n\n\nPriority: \ntier-0\n\n\n\n\nAurora Backup\n\n\n\n\nBackup Retention Period: \n1 day\n\n\n\n\nAurora Monitoring\n\n\n\n\nEnable Enhanced Monitoring: \nNo\n\n\n\n\nAurora Maintenance\n\n\n\n\nAuto Minor Version Upgrade: \nYes\n\n\nMaintenance Windows: \nNo Preference\n\n\n\n\n\n\nLaunch DB Instance\n\n\n\n\n\n\nClick Launch DB Instance blue button.\n\n\n\n\n\n\nThis process may take a while, sometimes 30 minutes or more.\n\n\n\n\n\n\nYou can check \nStatus\n of your instance by going to \nInstances\n on left navigation pane.\n\n\n\n\n\n\n\n\nRDS Instance Security Group\n\n\nOnce the instance has \nStatus:\n \navailable\n proceed:\n\n\n\n\nClick check box way left of your DB Instance name.\n\n\nClick \nInstance Actions \\ See Details\n gray button, on top.\n\n\n\n\nLookup for \nSecurity Groups\n and click the blue string to the right, it may appear as\n\n\n\n\ndefault (sg-XXXXXXXX)\n\n\n\n\n\n\n\n\n\n\nYou are now on EC2 Dashboard and Security Group ID is already selected.\n\n\n\n\nClick \nActions \\ Edit inbound rules\n\n\nClick \nAdd Rule\n, under Type select \nMYSQL/Aurora\n\n\nSource \nCustom\n and type value: \n0.0.0.0/0\n\n\n\n\nRepeat steps 6 \n 7, and type value \n::/0\n\n\n\n\n\n\n\n\nClick \nSave\n blue button.\n\n\n\n\nClick \nActions \\ Edit outbound rules\n\n\nVerify if Type: \nAll traffic\n, Destination: \nCustom\n and value: \n0.0.0.0/0\n is already set, if not, add the rule.\n\n\nGo back to \nRDS Dashboard\n, select your instance, click \nInstance Actions \\ Reboot\n, confirm with blue button on the right.\n\n\nWait until \nStatus\n is \navailable\n and check if \nSecurity Groups\n are \n( active )\n\n\n\n\n\n\nCreate IAM Policy to Grant Access to S3\n\n\nFrom this point on you need the newly S3 bucket ARN that we provided over email.\n\n\nIf you haven't received your bucket information, please follow \nthese steps.\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nPolicies.\n\n\nCreate policy\n blue button.\n\n\n\n\nSelect \nPolicy Generator\n\n\n\n\nEffect: \nAllow\n\n\nAWS Service: \nAmazon S3\n\n\nActions: check \nGetObject\n and \nGetObjectVersion\n\n\nAmazon Resource Name (ARN): \narn you received over email\n, example:\n\n\narn:aws:s3:::bipostdata-123456789012\n\n\n\n\n\n\n\n\nClick \nAdd Statement\n\n\n\n\n\n\nRepeat step 4 adding \n/*\n at the end of ARN bucket string, as follows:\n\n\n\n\nEffect: \nAllow\n\n\nAWS Service: \nAmazon S3\n\n\nActions: check \nGetObject\n and \nGetObjectVersion\n\n\nAmazon Resource Name (ARN): example: \narn:aws:s3:::bipostdata-123456789012/*\n\n\n\n\n\n\n\n\n\n\nClick \nNext Step\n blue button.\n\n\n\n\nPolicy Name:\n \nAllowAuroraToS3\n\n\nOptionally add \nDescription\n.\n\n\n\n\nPolicy Document\n: double check that JSON looks like this:\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt9999999999999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-123456789012\"\n            ]\n        },\n        {\n            \"Sid\": \"Stmt9999999777999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-123456789012/*\"\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\n\nClick \nCreate Policy\n\n\n\n\n\n\nFurther information from AWS go to: \nAllowing Amazon Aurora to Access Amazon S3 Resources\n\n\n\n\nCreate IAM Role to Allow RDS Access to S3\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nRoles.\n\n\nCreate New Role\n blue button.\n\n\n\n\nChoose \nAWS Service Role,\n scroll down and select \nAmazon RDS\n\n\n\n\n\n\n\n\nAttach Policy,\n leave blank and click \nNext Step\n blue button.\n\n\n\n\nRole name: \nRDSLoadFromS3\n\n\nClick \nCreate role\n blue button.\n\n\nClick on your newly created role. This will open a Summary.\n\n\n\n\nUnder Permissions, click \nAttach Policy\n blue button.\n\n\n\n\n\n\n\n\nUse Filter and select \nPolicy Type: Customer Managed\n\n\n\n\nClick the check box of your newly created Policy: \nAllowAuroraToS3\n\n\nClick \nAttach Policy\n blue button.\n\n\nCopy \nRole ARN\n string and save it for further use. It may look like this: \narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\nFurther information from AWS go to: \nCreating an IAM Role to Allow Amazon Aurora to Access AWS Services\n\n\n\n\nSet IAM Role to Aurora Cluster\n\n\n\n\nOpen \nRDS console.\n\n\nChoose \nClusters\n on left pane.\n\n\nClick check box of your newly cluster.\n\n\nClick \nManage IAM Roles\n gray button, on top.\n\n\nSelect the role you just created: \nRDSLoadFromS3\n and click \nDone\n, blue button.\n\n\n\n\n\n\n\n\nCreate Cluster Parameter Group\n\n\nIf you are already using a custom DB Cluster Parameter Group, you can select that group instead of creating a new DB Cluster Parameter Group.\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate Parameter Group\n blue button on top.\n\n\n\n\nParameter Group Family: \naurora5.6\n\n\nType: \nDB Cluster Parameter Group\n\n\nGroup Name: \nAuroraClusterAllowAWSAccess\n\n\nDescription: \nAllow cluster access to Amazon S3\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button.\n\n\n\n\nClick check box on your new \nauroraclusterallowawsaccess\n parameter group and click \nEdit Parameters\n gray button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\nExample\n\n\n\n\n\n\n\n\n\n\naurora_load_from_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naurora_select_into_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n blue button.\n\n\n\n\n\n\nFurther information from AWS go to: \nAssociating an IAM Role with a DB Cluster\n\n\n\n\nCreate DB Parameter Group\n\n\nIf you are already using a custom DB Parameter Group, you can select that group instead of creating a new DB Parameter Group.\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate Parameter Group\n blue button on top.\n\n\n\n\nParameter Group Family: \naurora5.6\n\n\nType: \nDB Parameter Group\n\n\nGroup Name: \nAuroraInstanceAllowAWSAccess\n\n\nDescription: \nAllow instance access to Amazon S3\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button.\n\n\n\n\nClick check box on your new \naurorainstanceallowawsaccess\n parameter group and click \nEdit Parameters\n gray button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\n\n\n\n\n\n\n\n\nlog_bin_trust_function_creators\n\n\n1\n\n\n\n\n\n\nmax_allowed_packet\n\n\n1073741824\n\n\n\n\n\n\nmax_connections\n\n\n16000\n\n\n\n\n\n\nmax_user_connections\n\n\n4294967295\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n blue button.\n\n\n\n\n\n\n\n\nSet Cluster Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nClusters.\n\n\nClick check box on your new cluster.\n\n\nClick \nModify Cluster\n gray button on top.\n\n\nUnder Database Options, set \nDB Cluster Parameter Group\n to \nauroraclusterallowawsaccess\n.\n\n\nClick check box \nApply Immediately\n and click \nContinue\n blue button.\n\n\nReview changes and click \nModify Cluster\n blue button.\n\n\n\n\nSet Instance Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick check box on your new instance.\n\n\nClick \nInstance Actions \\ Modify\n gray button on top.\n\n\nUnder Database Options, set \nDB Parameter Group\n to \naurorainstanceallowawsaccess\n\n\nYou may also notice that \nDB Cluster Parameter Group\n is set to \nauroraclusterallowawsaccess\n\n\nClick check box \nApply Immediately\n and click \nContinue\n blue button.\n\n\nReview changes and click \nModify DB Instance\n blue button.\n\n\nClick \nInstance Actions \\ Reboot\n gray button on top.\n\n\nConfirm reboot with blue button.\n\n\n\n\n\n\nVerify Instance Configuration\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick check box on your new instance.\n\n\nClick \nInstance Actions \\ See Details\n gray button on top.\n\n\n\n\nVerify the following:\n\n\n\n\nEnpoint: \n( authorized )\n\n\nParameter Group: \naurorainstanceallowawsaccess ( in-sync )\n\n\nDB Cluster Parameter Group: \nauroraclusterallowawsaccess ( in-sync )\n\n\nSecurity Groups: \ndefault (sg-XXXXXXXX) ( active )\n\n\nPublicly Accessible: \nYes\n\n\nDB Instance Status: \navailable\n\n\n\n\n\n\n\n\n\n\nTest connection to your RDS Aurora\n\n\n\n\n\n\nDownload and install any \nMySQL client\n of your preference:\n\n\nFor Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"\n\n\n\n\n\n\n\nOn your AWS Console go to \nRDS Dashboard\n, select your instance and copy the \nCluster Endpoint\n, which is a blue string with more than 60 characters.\n\n\n\n\n\n\nLaunch your MySQL client and configure a new connection:\n\n\n\n\nName:\n type any name of your preference.\n\n\nHost:\n Paste the Cluster Endpoint and delete the suffix :3306\n\n\nUsername:\n root\n\n\nPassword:\n type the Master Password\n\n\nPort:\n 3306\n\n\nDatabase:\n Leave blank\n\n\nConnect using SSL:\n No\n\n\n\n\n\n\n\n\nClick Connect and verify that you can successfully connect to your RDS instance.\n\n\n\n\n\n\n\n\nConfigure Instance Connection Details on Factor BI Console\n\n\n\n\nLog in to \nFactor BI Console.\n\n\nGo to RDS Instances and then click under Hostname.\n\n\nComplete all fields on the form with instance connection information.\n\n\n\n\n\n\nSecurity of your RDS Instance for Production\n\n\nIf your are ready to use Bipost API for production, we highly recommend the following:\n\n\n\n\nUse \nMySQL client\n to create a new user.\n\n\nSet a strong password.\n\n\nGran the new user with the following: \nGRANT LOAD FROM S3 ON *.* TO 'your-user-name';\n\n\n\n\nSet the following Global Privileges:\n\n\n\n\n\n\n\n\n\n\nSecurity for Downloading Data\n\n\nIf you plan to download data from Aurora to your on-premises databases, there are some settings to make on your AWS account.\n\n\n\n\nOpen \nIAM console.\n\n\nClick \nAdd user\n blue button on top left corner.\n\n\nUser name: \noutFromS3\n\n\nAccess type: \nProgrammatic access\n\n\n\n\nClick \nNext: Permissions\n blue button lower right corner.\n\n\n\n\n\n\n\n\nSelect \nAttach existing policies directly\n\n\n\n\nOn the search box type \nS3\n and select \nAmazonS3FullAccess\n\n\n\n\nClick \nNext: Review\n blue button lower right corner.\n\n\n\n\n\n\n\n\nClick \nDownload .csv\n.\n\n\n\n\n\n\nEmail the CSV to \ninfo@factorbi.com\n so we can setup the downloading process.\n\n\n\n\n\n\n\n\n\n\nConsole Access to Bucket\n\n\nBipost synchronization uses S3 to upload the data that is extracted from the on-premises database. The bucket is located within Factor BI AWS account so we can efficiently handle API calls, patches and new releases.\n\n\nRemember, we create a unique S3 bucket for each one of our customers, so nothing gets mixed up.\n\n\nSometimes you may want to access this bucket and review files and folders.\n\n\nTo accomplish this we provide an \nAWS Console access\n with a user, password and a direct link to your bucket.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#link-your-aws-account", 
            "text": "Follow these instructions to link your AWS Account and RDS Aurora instance to Bipost API.  IMPORTANT NOTICE: Many settings suggested here are for testing purposes. If you are to use the following AWS services for production you may want to follow your company policies and understand how to use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#dont-have-an-aws-account", 
            "text": "Not familiar with AWS or just want to skip creating AWS releated services?  Write us.    Create an AWS account here  aws.amazon.com     AWS usually makes an automated verification phone call, we suggest to provide a land line.   Provide payment information.  Select Basic Support (free plan).  Check if you can open  RDS Dashboard , by searching under  AWS services .  Congrats you have an AWS account!", 
            "title": "Don't have an AWS account?"
        }, 
        {
            "location": "/setupaws/#check-closest-aws-region-to-you-location", 
            "text": "cloudping.info  Click the above link and hit  HTTP Ping  and look for the lowest latency.  Maybe you want to try this at different times of the day.  Take note of the closest region.", 
            "title": "Check closest AWS Region to you location"
        }, 
        {
            "location": "/setupaws/#get-canonical-user-id-from-your-iam-home", 
            "text": "To perform the following steps, you need to sign in with the  root  AWS account.   Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Account Identifiers.   Copy AWS Account ID (12-digit) and Canonical User ID (64-digit).     Paste these numbers on Factor BI Console, RDS Instances section, following --   these steps.    Q: Is it secure to provide these numbers?  Yes, we use your  Canonical User ID  to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.", 
            "title": "Get Canonical User ID from your IAM Home"
        }, 
        {
            "location": "/setupaws/#create-aurora-instance", 
            "text": "", 
            "title": "Create Aurora Instance"
        }, 
        {
            "location": "/setupaws/#aurora-db-details", 
            "text": "From AWS Console Home, upper right corner (next to you name) be sure to select the  closest region to your location.  From AWS Console Home, search RDS.  From  RDS Dashboard , click Instances.  Launch DB Instance, blue button.  Select Engine:  Amazon Aurora , click select.  DB Instance Class: for testing purposes select the smallest available, currently  t2.small  Multi-AZ Deployment: for testing purposes select  No  DB Instance Identifier: assign a name, lower-case and no special characters.  Master Username:  root  Master Password: assign a hard password and store it in a secure place.  Confirm Password.  On your left pane it is displayed an estimated monthly cost. For further information check On-Demand Pricing:  RDS Pricing  Click Next Step, blue button.", 
            "title": "Aurora DB Details"
        }, 
        {
            "location": "/setupaws/#aurora-network-security", 
            "text": "VPC:  Create new VPC  Subnet Group:  Create new DB Subnet Group  Publicly Accessible:  Yes  Availability Zone:  No Preference  VPC Security Group:  Create new Security Group", 
            "title": "Aurora Network &amp; Security"
        }, 
        {
            "location": "/setupaws/#aurora-database-options", 
            "text": "DB Cluster Identifier:  leave blank  Database Name:  leave blank  Database Port:  3306  DB Parameter Group:  default.aurora5.6  DB Cluster Parameter Group:  default.aurora5.6  Option Group:  leave default  Enable Encryption:  No", 
            "title": "Aurora Database Options"
        }, 
        {
            "location": "/setupaws/#aurora-failover", 
            "text": "Priority:  tier-0", 
            "title": "Aurora Failover"
        }, 
        {
            "location": "/setupaws/#aurora-backup", 
            "text": "Backup Retention Period:  1 day", 
            "title": "Aurora Backup"
        }, 
        {
            "location": "/setupaws/#aurora-monitoring", 
            "text": "Enable Enhanced Monitoring:  No", 
            "title": "Aurora Monitoring"
        }, 
        {
            "location": "/setupaws/#aurora-maintenance", 
            "text": "Auto Minor Version Upgrade:  Yes  Maintenance Windows:  No Preference", 
            "title": "Aurora Maintenance"
        }, 
        {
            "location": "/setupaws/#launch-db-instance", 
            "text": "Click Launch DB Instance blue button.    This process may take a while, sometimes 30 minutes or more.    You can check  Status  of your instance by going to  Instances  on left navigation pane.", 
            "title": "Launch DB Instance"
        }, 
        {
            "location": "/setupaws/#rds-instance-security-group", 
            "text": "Once the instance has  Status:   available  proceed:   Click check box way left of your DB Instance name.  Click  Instance Actions \\ See Details  gray button, on top.   Lookup for  Security Groups  and click the blue string to the right, it may appear as   default (sg-XXXXXXXX)      You are now on EC2 Dashboard and Security Group ID is already selected.   Click  Actions \\ Edit inbound rules  Click  Add Rule , under Type select  MYSQL/Aurora  Source  Custom  and type value:  0.0.0.0/0   Repeat steps 6   7, and type value  ::/0     Click  Save  blue button.   Click  Actions \\ Edit outbound rules  Verify if Type:  All traffic , Destination:  Custom  and value:  0.0.0.0/0  is already set, if not, add the rule.  Go back to  RDS Dashboard , select your instance, click  Instance Actions \\ Reboot , confirm with blue button on the right.  Wait until  Status  is  available  and check if  Security Groups  are  ( active )", 
            "title": "RDS Instance Security Group"
        }, 
        {
            "location": "/setupaws/#create-iam-policy-to-grant-access-to-s3", 
            "text": "From this point on you need the newly S3 bucket ARN that we provided over email.  If you haven't received your bucket information, please follow  these steps.   Open  IAM Console.  In the left navigation pane choose  Policies.  Create policy  blue button.   Select  Policy Generator   Effect:  Allow  AWS Service:  Amazon S3  Actions: check  GetObject  and  GetObjectVersion  Amazon Resource Name (ARN):  arn you received over email , example:  arn:aws:s3:::bipostdata-123456789012     Click  Add Statement    Repeat step 4 adding  /*  at the end of ARN bucket string, as follows:   Effect:  Allow  AWS Service:  Amazon S3  Actions: check  GetObject  and  GetObjectVersion  Amazon Resource Name (ARN): example:  arn:aws:s3:::bipostdata-123456789012/*      Click  Next Step  blue button.   Policy Name:   AllowAuroraToS3  Optionally add  Description .   Policy Document : double check that JSON looks like this:  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt9999999999999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-123456789012\"\n            ]\n        },\n        {\n            \"Sid\": \"Stmt9999999777999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-123456789012/*\"\n            ]\n        }\n    ]\n}    Click  Create Policy    Further information from AWS go to:  Allowing Amazon Aurora to Access Amazon S3 Resources", 
            "title": "Create IAM Policy to Grant Access to S3"
        }, 
        {
            "location": "/setupaws/#create-iam-role-to-allow-rds-access-to-s3", 
            "text": "Open  IAM Console.  In the left navigation pane choose  Roles.  Create New Role  blue button.   Choose  AWS Service Role,  scroll down and select  Amazon RDS     Attach Policy,  leave blank and click  Next Step  blue button.   Role name:  RDSLoadFromS3  Click  Create role  blue button.  Click on your newly created role. This will open a Summary.   Under Permissions, click  Attach Policy  blue button.     Use Filter and select  Policy Type: Customer Managed   Click the check box of your newly created Policy:  AllowAuroraToS3  Click  Attach Policy  blue button.  Copy  Role ARN  string and save it for further use. It may look like this:  arn:aws:iam::123456789012:role/RDSLoadFromS3   Further information from AWS go to:  Creating an IAM Role to Allow Amazon Aurora to Access AWS Services", 
            "title": "Create IAM Role to Allow RDS Access to S3"
        }, 
        {
            "location": "/setupaws/#set-iam-role-to-aurora-cluster", 
            "text": "Open  RDS console.  Choose  Clusters  on left pane.  Click check box of your newly cluster.  Click  Manage IAM Roles  gray button, on top.  Select the role you just created:  RDSLoadFromS3  and click  Done , blue button.", 
            "title": "Set IAM Role to Aurora Cluster"
        }, 
        {
            "location": "/setupaws/#create-cluster-parameter-group", 
            "text": "If you are already using a custom DB Cluster Parameter Group, you can select that group instead of creating a new DB Cluster Parameter Group.   Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create Parameter Group  blue button on top.   Parameter Group Family:  aurora5.6  Type:  DB Cluster Parameter Group  Group Name:  AuroraClusterAllowAWSAccess  Description:  Allow cluster access to Amazon S3     Click  Create  blue button.   Click check box on your new  auroraclusterallowawsaccess  parameter group and click  Edit Parameters  gray button on top.   Set the following:     Name  Edit Values  Example      aurora_load_from_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aurora_select_into_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3       Click  Save Changes  blue button.    Further information from AWS go to:  Associating an IAM Role with a DB Cluster", 
            "title": "Create Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#create-db-parameter-group", 
            "text": "If you are already using a custom DB Parameter Group, you can select that group instead of creating a new DB Parameter Group.   Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create Parameter Group  blue button on top.   Parameter Group Family:  aurora5.6  Type:  DB Parameter Group  Group Name:  AuroraInstanceAllowAWSAccess  Description:  Allow instance access to Amazon S3     Click  Create  blue button.   Click check box on your new  aurorainstanceallowawsaccess  parameter group and click  Edit Parameters  gray button on top.   Set the following:     Name  Edit Values      log_bin_trust_function_creators  1    max_allowed_packet  1073741824    max_connections  16000    max_user_connections  4294967295       Click  Save Changes  blue button.", 
            "title": "Create DB Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-cluster-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Clusters.  Click check box on your new cluster.  Click  Modify Cluster  gray button on top.  Under Database Options, set  DB Cluster Parameter Group  to  auroraclusterallowawsaccess .  Click check box  Apply Immediately  and click  Continue  blue button.  Review changes and click  Modify Cluster  blue button.", 
            "title": "Set Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-instance-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click check box on your new instance.  Click  Instance Actions \\ Modify  gray button on top.  Under Database Options, set  DB Parameter Group  to  aurorainstanceallowawsaccess  You may also notice that  DB Cluster Parameter Group  is set to  auroraclusterallowawsaccess  Click check box  Apply Immediately  and click  Continue  blue button.  Review changes and click  Modify DB Instance  blue button.  Click  Instance Actions \\ Reboot  gray button on top.  Confirm reboot with blue button.", 
            "title": "Set Instance Parameter Group"
        }, 
        {
            "location": "/setupaws/#verify-instance-configuration", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click check box on your new instance.  Click  Instance Actions \\ See Details  gray button on top.   Verify the following:   Enpoint:  ( authorized )  Parameter Group:  aurorainstanceallowawsaccess ( in-sync )  DB Cluster Parameter Group:  auroraclusterallowawsaccess ( in-sync )  Security Groups:  default (sg-XXXXXXXX) ( active )  Publicly Accessible:  Yes  DB Instance Status:  available", 
            "title": "Verify Instance Configuration"
        }, 
        {
            "location": "/setupaws/#test-connection-to-your-rds-aurora", 
            "text": "Download and install any  MySQL client  of your preference:  For Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"    On your AWS Console go to  RDS Dashboard , select your instance and copy the  Cluster Endpoint , which is a blue string with more than 60 characters.    Launch your MySQL client and configure a new connection:   Name:  type any name of your preference.  Host:  Paste the Cluster Endpoint and delete the suffix :3306  Username:  root  Password:  type the Master Password  Port:  3306  Database:  Leave blank  Connect using SSL:  No     Click Connect and verify that you can successfully connect to your RDS instance.", 
            "title": "Test connection to your RDS Aurora"
        }, 
        {
            "location": "/setupaws/#configure-instance-connection-details-on-factor-bi-console", 
            "text": "Log in to  Factor BI Console.  Go to RDS Instances and then click under Hostname.  Complete all fields on the form with instance connection information.", 
            "title": "Configure Instance Connection Details on Factor BI Console"
        }, 
        {
            "location": "/setupaws/#security-of-your-rds-instance-for-production", 
            "text": "If your are ready to use Bipost API for production, we highly recommend the following:   Use  MySQL client  to create a new user.  Set a strong password.  Gran the new user with the following:  GRANT LOAD FROM S3 ON *.* TO 'your-user-name';   Set the following Global Privileges:", 
            "title": "Security of your RDS Instance for Production"
        }, 
        {
            "location": "/setupaws/#security-for-downloading-data", 
            "text": "If you plan to download data from Aurora to your on-premises databases, there are some settings to make on your AWS account.   Open  IAM console.  Click  Add user  blue button on top left corner.  User name:  outFromS3  Access type:  Programmatic access   Click  Next: Permissions  blue button lower right corner.     Select  Attach existing policies directly   On the search box type  S3  and select  AmazonS3FullAccess   Click  Next: Review  blue button lower right corner.     Click  Download .csv .    Email the CSV to  info@factorbi.com  so we can setup the downloading process.", 
            "title": "Security for Downloading Data"
        }, 
        {
            "location": "/setupaws/#console-access-to-bucket", 
            "text": "Bipost synchronization uses S3 to upload the data that is extracted from the on-premises database. The bucket is located within Factor BI AWS account so we can efficiently handle API calls, patches and new releases.  Remember, we create a unique S3 bucket for each one of our customers, so nothing gets mixed up.  Sometimes you may want to access this bucket and review files and folders.  To accomplish this we provide an  AWS Console access  with a user, password and a direct link to your bucket.", 
            "title": "Console Access to Bucket"
        }, 
        {
            "location": "/installation/", 
            "text": "Before you install\n\n\nIf you haven't linked your RDS Instance to Bipost API, please follow these steps: \nLink your AWS account.\n\n\nPrerequisites\n\n\nFor most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.\n\n\nWindow XP \n Vista\n\n\nDownload and install: \n\n\n\n\nWindowsInstaller.\n\n\nPrerequisites for Windows 7 and 8.\n\n\n\n\nWindows 7 and 8\n\n\nDownload and install:\n\n\n\n\ndotNetFx40\n\n\nvcredist\n\n\n\n\nDownload \n Install\n\n\n\n\n\n\nGet latest version here.\n\n\n\n\n\n\nUnzip \nbiPost.zip\n to any folder on your Windows.\n\n\n\n\n\n\nCheck if you can successfully launch \nbiPost.exe\n\n\n\n\n\n\nConfigure Bipost Sync", 
            "title": "Download and Install"
        }, 
        {
            "location": "/installation/#before-you-install", 
            "text": "If you haven't linked your RDS Instance to Bipost API, please follow these steps:  Link your AWS account.", 
            "title": "Before you install"
        }, 
        {
            "location": "/installation/#prerequisites", 
            "text": "For most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/installation/#window-xp-vista", 
            "text": "Download and install:    WindowsInstaller.  Prerequisites for Windows 7 and 8.", 
            "title": "Window XP &amp; Vista"
        }, 
        {
            "location": "/installation/#windows-7-and-8", 
            "text": "Download and install:   dotNetFx40  vcredist", 
            "title": "Windows 7 and 8"
        }, 
        {
            "location": "/installation/#download-install", 
            "text": "Get latest version here.    Unzip  biPost.zip  to any folder on your Windows.    Check if you can successfully launch  biPost.exe    Configure Bipost Sync", 
            "title": "Download &amp; Install"
        }, 
        {
            "location": "/bipostexe/", 
            "text": "Bipost Sync Settings\n\n\nIMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat these as sensitive information.\n\n\nFrom this point on you need your \nService No.\n and \nActivation No.\n available on your \nFactor BI Console.\n\n\n\n\nClick \nConfiguration\n and set:\n\n\n\n\nService No.:\n 36 digit hex number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit hex number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n Select \nFirebird\n or \nSQL\n (Microsoft SQL Server).\n\n\n\n\nSystem:\n Select \nCustom...\n\n\nIf you use \nMicrosip ERP\n or \nAspel SAE\n then select the System according.\n\n\n\n\n\n\n\n\nFirebird Connection\n\n\n\n\nDatabase:\n Location of your \n.FDB\n file.\n\n\nPassword:\n Set your Firebird password, sometimes \nmasterkey\n\n\n\n\nUse the following only when Bipost Sync is not located on the same server as the Firebird server.\n\n\n\n\nRemote Connection:\n Enable when biPost.exe is on a remote location on your LAN.\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nSQL Connection\n\n\n\n\n\n\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nUser:\n Login for your SQL server. It only needs read permissions.\n\n\n\n\n\n\nPassword:\n Password for the Login provided.\n\n\n\n\n\n\nDatabase:\n Name of your database.\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\n\n\n\n\nSpecific Bucket:\n Enable to use your own AWS Account.\n\n\n\n\nEnter your \nBucket Name\n that we provided over email.\n\n\n\n\n\n\n\n\nDownload Data:\n Enable to download data from AWS Aurora-MySQL to your on-premises.\n\n\n\n\n\n\nRecursive Sync\n\n\nWhen enabled it optimizes upload by extracting and uploading one day at a time for the given date range.\n\n\nVery useful to upload historic data or big data sets.\n\n\nIt is always used in combination with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\nWhen \nturned off\n it automatically sets \ntoday\n as start and end date, using your system clock.\n\n\n\n\ncustomData.json\n\n\nThis file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data sets from your on-prem DB.\n\n\nExample 1, using recursiveDateField:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente\",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  }\n]\n\n\n\nExample 2, for catalogs:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\nThe above JSON will send all data for the specified tables, as \njoin\n and \nfilter\n are not in use.\n\n\nNote that you can leave \n\"active\": \"\",\n\n\n\n\nExample 3, using special filter when no datetime is available:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym\",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0)))\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\njoin\n and \nfilter\n can use any syntax supported on SQL Server/Firebird SQL.\n\n\n\n\nExample 4 with Microsip ERP:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"DOCTOS_ENTRE_SIS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\nIn the above example Bipost Sync is using \nSystem: Microsip\n and on every sync it sends all the factory embedded tables plus the tables set in customData.json\n\n\n\n\nTables \n Primary Keys\n\n\nTake note of the following:\n\n\n\n\nEvery table specified in \ncustomData.json\n must have a \nPrimary Key\n and these must be listed on \n\"fields\"\n\n\nYou can only setup customData.json with \nbase tables.\n\n\nUse \n*\n on \nfields\n to retrieve all table columns.\n\n\n\"fields\"\n parameter must only include fields that exist on \n\"table\":\n\n\n\n\nrecursiveDateField\n\n\nThis parameter is used when \nRecursive Sync\n check box is enabled.\n\n\nIt only supports date fields without hour.\n\n\nOn SQL Server using \ndatetime\n data type, it only supports dates ending in \n00:00:00.000\n\n\nHandle multiple customData.json settings\n\n\nIt is very common to make changes to customData.json to upload different sets of data.\n\n\nUse Cases:\n\n\n\n\n\n\nSome tables may be uploaded once since that data is rarely changed, e.g. config \n company tables.\n\n\n\n\n\n\nHistoric data may be uploaded once, e.g. transactions from previous years.\n\n\n\n\n\n\nRecently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.\n\n\n\n\n\n\nRecently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.\n\n\n\n\n\n\nFor all these reasons it may be very useful to make copies of Bipost Sync folder and just change \ncustomData.json.\n Moreover you may want to have different sync schedules, which are explained next.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases, create a separate Bipost Sync folder on Windows for each one. Then customize each database with the desired data set as explained \nhere.", 
            "title": "Sync to AWS"
        }, 
        {
            "location": "/bipostexe/#bipost-sync-settings", 
            "text": "IMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat these as sensitive information.  From this point on you need your  Service No.  and  Activation No.  available on your  Factor BI Console.   Click  Configuration  and set:   Service No.:  36 digit hex number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit hex number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:  Select  Firebird  or  SQL  (Microsoft SQL Server).   System:  Select  Custom...  If you use  Microsip ERP  or  Aspel SAE  then select the System according.", 
            "title": "Bipost Sync Settings"
        }, 
        {
            "location": "/bipostexe/#firebird-connection", 
            "text": "Database:  Location of your  .FDB  file.  Password:  Set your Firebird password, sometimes  masterkey   Use the following only when Bipost Sync is not located on the same server as the Firebird server.   Remote Connection:  Enable when biPost.exe is on a remote location on your LAN.  Server:  IP or name of the server on your LAN network.", 
            "title": "Firebird Connection"
        }, 
        {
            "location": "/bipostexe/#sql-connection", 
            "text": "Server:  IP or name of the server on your LAN network.    User:  Login for your SQL server. It only needs read permissions.    Password:  Password for the Login provided.    Database:  Name of your database.", 
            "title": "SQL Connection"
        }, 
        {
            "location": "/bipostexe/#general-settings", 
            "text": "Specific Bucket:  Enable to use your own AWS Account.   Enter your  Bucket Name  that we provided over email.     Download Data:  Enable to download data from AWS Aurora-MySQL to your on-premises.", 
            "title": "General Settings"
        }, 
        {
            "location": "/bipostexe/#recursive-sync", 
            "text": "When enabled it optimizes upload by extracting and uploading one day at a time for the given date range.  Very useful to upload historic data or big data sets.  It is always used in combination with  customData.json  so you can configure the date field to use for each table.  When  turned off  it automatically sets  today  as start and end date, using your system clock.", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/bipostexe/#customdatajson", 
            "text": "This file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data sets from your on-prem DB.  Example 1, using recursiveDateField:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente\",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  }\n]  Example 2, for catalogs:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]   The above JSON will send all data for the specified tables, as  join  and  filter  are not in use.  Note that you can leave  \"active\": \"\",   Example 3, using special filter when no datetime is available:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym\",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0)))\",\n    \"recursiveDateField\": \"\"\n  }\n]   join  and  filter  can use any syntax supported on SQL Server/Firebird SQL.   Example 4 with Microsip ERP:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"DOCTOS_ENTRE_SIS\",\n    \"fields\": \"*\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]   In the above example Bipost Sync is using  System: Microsip  and on every sync it sends all the factory embedded tables plus the tables set in customData.json", 
            "title": "customData.json"
        }, 
        {
            "location": "/bipostexe/#tables-primary-keys", 
            "text": "Take note of the following:   Every table specified in  customData.json  must have a  Primary Key  and these must be listed on  \"fields\"  You can only setup customData.json with  base tables.  Use  *  on  fields  to retrieve all table columns.  \"fields\"  parameter must only include fields that exist on  \"table\":", 
            "title": "Tables &amp; Primary Keys"
        }, 
        {
            "location": "/bipostexe/#recursivedatefield", 
            "text": "This parameter is used when  Recursive Sync  check box is enabled.  It only supports date fields without hour.  On SQL Server using  datetime  data type, it only supports dates ending in  00:00:00.000", 
            "title": "recursiveDateField"
        }, 
        {
            "location": "/bipostexe/#handle-multiple-customdatajson-settings", 
            "text": "It is very common to make changes to customData.json to upload different sets of data.  Use Cases:    Some tables may be uploaded once since that data is rarely changed, e.g. config   company tables.    Historic data may be uploaded once, e.g. transactions from previous years.    Recently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.    Recently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.    For all these reasons it may be very useful to make copies of Bipost Sync folder and just change  customData.json.  Moreover you may want to have different sync schedules, which are explained next.", 
            "title": "Handle multiple customData.json settings"
        }, 
        {
            "location": "/bipostexe/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/bipostexe/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/bipostexe/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases, create a separate Bipost Sync folder on Windows for each one. Then customize each database with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/synctowindows/", 
            "text": "Download data sets to your on-prem\n\n\nIn many cases you may want to use Aurora-MySQL as a cloud database and make transactions with web applications. In this case it may be useful to download data sets from Aurora-MySQL to your on-premises.\n\n\n\n\n\n\n\n\nWith this option you are able to download data from Aurora to your on-premises Windows.\n\n\n\n\n\n\nData to retrieve from Aurora is specified on \noutData.json\n\n\n\n\n\n\nProcess Data is used to insert retuned data to SQL Server or Firebird SQL.\n\n\n\n\n\n\nIMPORTANT: Follow \nthis security steps\n to enable the downloading process.\n\n\n\n\noutData.json\n\n\n\n\n\n\nWith this file you are able to specify the tables, fields and filter criteria to query data to Aurora.\n\n\n\n\n\n\nTables are case-sensitive on Aurora MySQL, so this is important on \n\"table\":\n parameter.\n\n\n\n\n\n\nDownloaded data will be available on \n%localappdata%/biPost/out_\n Windows folder.\n\n\n\n\n\n\nExcept for \nrecursiveDateField\n all parameters are supported.\n\n\n\n\n\n\nWe highly recomend to prepare your data set on separate non-transactional tables, and process them with \nBipost API Final Statement.\n\n\nExample 1, using outData.json\n\n\n{\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}\n\n\n\nExample 2, using outData.json\n\n\n{\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}\n\n\n\n\n\nNote that you can leave \n\"active\": \"\",\n\n\n\n\nExample of downloaded data on \n%localappdata%/bipost\n folder:\n\n\n\n\n\n\nProcess Data\n\n\n\n\n\n\nWhen this option is on, table schema's are created/altered and data uploaded to your SQL Server or Firebird SQL.\n\n\n\n\n\n\nSame names of output tables on Aurora are going to be created on SQL Server/Firebird.\n\n\n\n\n\n\nWe highly recommend to create specific output tables on Aurora and give them a name that is not in use on your SQL Server/Firebird database.\n\n\n\n\n\n\nPrimary keys on Aurora are used on SQL Server/Firebird to avoid duplicates.\n\n\n\n\n\n\nYou are able to query views from Aurora as output, in which case tables on SQL Server/Firebird are always deleted before any new data load.\n\n\n\n\n\n\nSchema changes on Aurora are applied to SQL Server/Firebird, except for deleting and renaming columns.\n\n\n\n\n\n\nYou're able to run queries before and after data is loaded to SQL Server/Firebird, more instructions below.\n\n\n\n\n\n\nFirebird SQL does not naturally support creating a column name starting with underscore,\n so avoid that on Aurora if your on-prem DB is Firebird.\n\n\n\n\n\n\nExample of processed data: tables were created and data loaded.\n\n\n\n\n\n\nInitial and Final Query\n\n\nBefore and after data is loaded to SQL Server/Firebird you're able to run queries, example:\n\n\n{\n      \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n      \"initialQuery\": \"execute procedure spInitial;\",\n      \"finalQuery\": \"execute procedure spFinal;\"\n}\n\n\n\ninitialQuery\n and \nfinalQuery\n are used to transform your data in any way you want.", 
            "title": "Sync back to Windows"
        }, 
        {
            "location": "/synctowindows/#download-data-sets-to-your-on-prem", 
            "text": "In many cases you may want to use Aurora-MySQL as a cloud database and make transactions with web applications. In this case it may be useful to download data sets from Aurora-MySQL to your on-premises.     With this option you are able to download data from Aurora to your on-premises Windows.    Data to retrieve from Aurora is specified on  outData.json    Process Data is used to insert retuned data to SQL Server or Firebird SQL.    IMPORTANT: Follow  this security steps  to enable the downloading process.", 
            "title": "Download data sets to your on-prem"
        }, 
        {
            "location": "/synctowindows/#outdatajson", 
            "text": "With this file you are able to specify the tables, fields and filter criteria to query data to Aurora.    Tables are case-sensitive on Aurora MySQL, so this is important on  \"table\":  parameter.    Downloaded data will be available on  %localappdata%/biPost/out_  Windows folder.    Except for  recursiveDateField  all parameters are supported.    We highly recomend to prepare your data set on separate non-transactional tables, and process them with  Bipost API Final Statement.  Example 1, using outData.json  {\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}  Example 2, using outData.json  {\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}   Note that you can leave  \"active\": \"\",   Example of downloaded data on  %localappdata%/bipost  folder:", 
            "title": "outData.json"
        }, 
        {
            "location": "/synctowindows/#process-data", 
            "text": "When this option is on, table schema's are created/altered and data uploaded to your SQL Server or Firebird SQL.    Same names of output tables on Aurora are going to be created on SQL Server/Firebird.    We highly recommend to create specific output tables on Aurora and give them a name that is not in use on your SQL Server/Firebird database.    Primary keys on Aurora are used on SQL Server/Firebird to avoid duplicates.    You are able to query views from Aurora as output, in which case tables on SQL Server/Firebird are always deleted before any new data load.    Schema changes on Aurora are applied to SQL Server/Firebird, except for deleting and renaming columns.    You're able to run queries before and after data is loaded to SQL Server/Firebird, more instructions below.    Firebird SQL does not naturally support creating a column name starting with underscore,  so avoid that on Aurora if your on-prem DB is Firebird.    Example of processed data: tables were created and data loaded.", 
            "title": "Process Data"
        }, 
        {
            "location": "/synctowindows/#initial-and-final-query", 
            "text": "Before and after data is loaded to SQL Server/Firebird you're able to run queries, example:  {\n      \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n      \"initialQuery\": \"execute procedure spInitial;\",\n      \"finalQuery\": \"execute procedure spFinal;\"\n}  initialQuery  and  finalQuery  are used to transform your data in any way you want.", 
            "title": "Initial and Final Query"
        }, 
        {
            "location": "/bipostapi/", 
            "text": "Bipost API\n\n\nThe API runs on AWS and has 4 main stages which are described next.\n\n\n1. Create-Alter Schemas\n\n\n\n\n\n\nIf it doesn't exist, database is created with:\n\n\n\n\n\n\nName: The one you specified on \nFactor BI Console.\n\n\n\n\n\n\nEncoding: \ncp1252 West European (latin1)\n\n\n\n\n\n\nCollation: \nlatin1_spanish_ci\n\n\n\n\n\n\n\n\n\n\nTables are created with the full set of fields found on source db.\n\n\n\n\n\n\nAlter tables to match source schemas. Columns are never deleted.\n\n\n\n\n\n\nFields will appear on a different position as the source db.\n\n\n\n\n\n\nOnly the fields specified in \ncustomData.json\n will be populated.\n\n\n\n\n\n\n\n\n2. Initial Statement\n\n\nAfter schemas are created/altered and \nbefore\n new data is loaded, you can specify to run a query against your database.\n\n\nThis is very useful if you need to delete, truncate or make any changes before data is loaded.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostInitial\n and must not have parameters.\n\n\nPlease send us an email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  INSERT INTO logPostInitial (message) VALUES ('auto');\n\n  TRUNCATE TABLE `mytesttable`;\n  TRUNCATE TABLE `bipostlog`;\n  TRUNCATE TABLE `movtipo`;\n\nEND$$\n\n\n\n\n\n3. Load Data\n\n\nData loading is performed by Aurora with a \nREPLACE\n parameter. Rows with the same primary key are updated and the rest inserted.\n\n\nYou can verify which tables where loaded by querying \naurora_s3_load_history\n table like this:\n\n\nselect * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';\n\n\n\nOptionally convert \nload_timestamp\n to your local time, e.g.: \nCONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')\n\n\n\n\n4. Final Statement\n\n\nAfter new data is loaded to your Aurora-MySQL database, and \nbefore\n it begins the downloading process to your on-prem, you can specify to run a query.\n\n\nThis is very handy if you need to execute several routines and, for example, populate new tables.\n\n\nIt is also very useful to prepare tables with data sets for the downloading process back to on-premises.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostFinal\n and must not have parameters.\n\n\nPlease send us an email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  REPLACE INTO dateInfo (tag, cDate)\n  SELECT 'yesterday', fnDateInfo('yesterday',fnServiceDate());\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\n  call spPopulateMyOtherTables;\n\n  INSERT INTO logPostFinal (message) VALUES ('auto');\n\nEND$$", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#bipost-api", 
            "text": "The API runs on AWS and has 4 main stages which are described next.", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#1-create-alter-schemas", 
            "text": "If it doesn't exist, database is created with:    Name: The one you specified on  Factor BI Console.    Encoding:  cp1252 West European (latin1)    Collation:  latin1_spanish_ci      Tables are created with the full set of fields found on source db.    Alter tables to match source schemas. Columns are never deleted.    Fields will appear on a different position as the source db.    Only the fields specified in  customData.json  will be populated.", 
            "title": "1. Create-Alter Schemas"
        }, 
        {
            "location": "/bipostapi/#2-initial-statement", 
            "text": "After schemas are created/altered and  before  new data is loaded, you can specify to run a query against your database.  This is very useful if you need to delete, truncate or make any changes before data is loaded.  Currently you need to include all desired statements in a stored procedure with the name  spPostInitial  and must not have parameters.  Please send us an email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  INSERT INTO logPostInitial (message) VALUES ('auto');\n\n  TRUNCATE TABLE `mytesttable`;\n  TRUNCATE TABLE `bipostlog`;\n  TRUNCATE TABLE `movtipo`;\n\nEND$$", 
            "title": "2. Initial Statement"
        }, 
        {
            "location": "/bipostapi/#3-load-data", 
            "text": "Data loading is performed by Aurora with a  REPLACE  parameter. Rows with the same primary key are updated and the rest inserted.  You can verify which tables where loaded by querying  aurora_s3_load_history  table like this:  select * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';  Optionally convert  load_timestamp  to your local time, e.g.:  CONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')", 
            "title": "3. Load Data"
        }, 
        {
            "location": "/bipostapi/#4-final-statement", 
            "text": "After new data is loaded to your Aurora-MySQL database, and  before  it begins the downloading process to your on-prem, you can specify to run a query.  This is very handy if you need to execute several routines and, for example, populate new tables.  It is also very useful to prepare tables with data sets for the downloading process back to on-premises.  Currently you need to include all desired statements in a stored procedure with the name  spPostFinal  and must not have parameters.  Please send us an email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  REPLACE INTO dateInfo (tag, cDate)\n  SELECT 'yesterday', fnDateInfo('yesterday',fnServiceDate());\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\n  call spPopulateMyOtherTables;\n\n  INSERT INTO logPostFinal (message) VALUES ('auto');\n\nEND$$", 
            "title": "4. Final Statement"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting\n\n\nWhen you use manual sync and it is completed successfully, the following dialog appears.\n\n\n\n\nIf you face any errors, try looking here.\n\n\n\n\nSyntax error\n\n\nMisspelled fields or tables on customData.json may appear as:\n\n\n\n\n\n\nNo Internet Connection\n\n\nWhen no internet connection available, the following message appears:\n\n\n\n\n\n\nFirewall Restrictions\n\n\nIf your internet connection has a firewall, it may show different errors like:\n\n\n\n\nThe remote name could not be resolved.\n\n\nA WebException with status SendFailure was thrown.\n\n\nA WebException with status NameResolutionFailure was thrown.\n\n\nError making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.\n\n\n\n\n\n\nGrant Firewall to reach Amazon S3\n\n\nCreate a policy to Allow to:\n\n\n\n\n54.230.0.0/15\n\n\n52.192.0.0/11\n\n\n\n\n\n\nAWS can have multiple IP addresses for S3 service, so in case the above IP's don't work check the \nAWS Public IP Address Ranges documentation\n and look for\n\n\n  \"region\": \"GLOBAL\",\n  \"service\": \"AMAZON\"\n\n\n\nand\n\n\n  \"region\": \"us-east-1\",\n  \"service\": \"AMAZON\"\n\n\n\nhttps://ip-ranges.amazonaws.com/ip-ranges.json\n\n\n\n\nNo information to Sync\n\n\nIf \nNo information to Sync\n message appears, verify that customData.json is set to send at least one table.\n\n\n\n\nWaiting time\n\n\nBipost Sync may take from a few seconds to several minutes to extract from on-prem DB and upload to AWS. While this is happening no messages/icons will show that biPost.exe is working and maybe you'll see \n(Not responding)\n on the top of the window, this is normal.\n\n\nIf you launch Windows Task Manager probably you'll see that \nbiPost.exe *32\n is running and consuming a considerable amount of CPU.\n\n\nOnce the information is uploaded to AWS, it usually is available on Aurora-MySQL very fast. If a big data set was uploaded it may take up to 5 minutes to be available on Aurora.\n\n\nIf you need to check which tables where loaded, check \naurora_s3_load_history.\n\n\n\n\nUpload Limit\n\n\nDepending on the number of rows and columns on each table, it is possible that a large amount of data sent on a single sync may not load to Aurora-MySQL.\n\n\nWe have tested up to 1.5 million rows on a single sync and works fine.\n\n\nWe recommend using \nRecursive Sync\n for big tables that have a datetime field available.\n\n\n\n\nSpecial Characters\n\n\nSome special characters on \nchar\n and \nvarchar\n fields are not supported and thus removed by biPost.exe\nFor example:\n\n\n\n\nEnter\n\n\n()\n\n\n\n\nOn \nSQL Server\n, all special characters on strings of 100 length or more are removed, leaving only letters and numbers.\n\n\n\n\nSchema Limitations\n\n\n\n\nNULL\n values on char and varchar are converted to \n''\n on MySQL.\n\n\nNULL\n values on float, money and int datatypes are converted to value \n0\n (zero) on MySQL.\n\n\nNULL\n values on datetime are converted to \n0000-00-00 00:00:00\n on MySQL.\n\n\nbit\n datatype is converted to \nVARCHAR(1)\n on MySQL.\n\n\nTables without a \nPRIMARY KEY\n will display an error message.\n\n\n\n\n\n\nAppData\\Local folder\n\n\nSometimes it is necessary to manually delete the content of \n\\AppData\\Local\\biPost\n folder.\n\n\nOpen a new Windows Explorer and enter \n%localappdata%\\bipost\n. Select all and delete.\n\n\n************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.\n\n\n\n\n\nMySQL schemas are created but no data is loaded\n\n\nTwo things might be causing this problem:\n\n\n1. RDS instance cannot reach S3 bucket.\n\n\nWhen we look at our CloudWatch logs, we see \nUnable to initialize S3Stream\n, so do the following:\n\n\n\n\n\n\nCheck if your \nIAM Policy to Grant Access to S3\n is set correctly using the S3 bucket ARN we provided. Also double check the policy document (JSON).\n\n\n\n\n\n\nCheck that \nIAM Role\n has attached the former IAM Policy. Copy ARN Role to a notepad for next steps.\n\n\n\n\n\n\nGo to \nRDS Parameter Groups\n, select the cluster group and click \nCompare Parameters\n, it should show the IAM ARN Role (the one you just copied on a notepad) on the parameters shown \nhere.\n\n\n\n\n\n\nGo to \nRDS Clusters\n and check if \nIAM Role\n is listed and active for your cluster.\n\n\n\n\n\n\nDouble check IAM roles attached to your instance querying \nshow global variables like '%role%'\n\n\n\n\n\n\n\n\nAfter this, if you still experience this error, check out \nManually debugging S3Stream.\n\n\n2. Name of your destination database must be all lower case.\n\n\nWhen we look at our CloudWatch logs, we see:\n\n\nSequelizeConnectionError: ER_BAD_DB_ERROR: Unknown database\n\n\nDouble check that your DB name is all lower case.\n\n\nManually debugging S3Stream\n\n\nIn this section we will manually upload data to Aurora-MySQL. The goal here is to see whether an error is shown while directly importing data from S3 to Aurora-MySQL.\n\n\nUsing MySQL Workbench (or any client of your preference), open a connection to your MySQL instance preferably using \nroot\n account.\n\n\nSyntax\n\n\nLOAD DATA FROM S3 's3-us-east-1://{my-bucket-name}/{my-36-digit-service-number}/process_{my-36-digit-service-number}{YYYY_M_D-of-yesterday}_post/{my-example-table}.csv'\nREPLACE INTO TABLE `{my-database-name}`.`{my-example-table}`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n({column_1},{column_2},...,{column_n});\n\n\n\nReplace curly brackets with your data.\n\n\nParameters\n\n\n\n\nmy-bucket-name\n: Name of your dedicated bucket, assigned by Factor BI.\n\n\nmy-36-digit-service-number\n: Number assigned to your service.\n\n\nYYYY_M_D-of-yesterday\n: Date for yesterday on GMT America/Mexico_City.\n\n\nmy-example-table\n: Pick any of the tables that you are synchronizing.\n\n\nmy-database-name\n: Name of your destination DB on MySQL.\n\n\n{column_1},{column_2},...,{column_n}\n: List all the columns in the same order as they appear in the .CSV file created on \nAppData\\Local folder\n.\n\n\n\n\nExample\n\n\nLOAD DATA FROM S3 's3-us-east-1://bipostdata-f0123abc4567/a1bcd23e-4fa5-67b8-cd9e-f0123abc4567/process_a1bcd23e-4fa5-67b8-cd9e-f0123abc45672017_9_13_post/CLIE.csv'\nREPLACE INTO TABLE `mytestdb`.`clie`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CLAVE,STATUS,NOMBRE,RFC,CALLE,NUMINT,NUMEXT,CRUZAMIENTOS,CRUZAMIENTOS2,COLONIA,CODIGO,LOCALIDAD,MUNICIPIO,ESTADO,NACIONALIDAD,REFERDIR,TELEFONO,CLASIFIC,FAX,PAG_WEB,CURP,CVE_ZONA,IMPRIR,MAIL,NIVELSEC,ENVIOSILEN,EMAILPRED,DIAREV,DIAPAGO,CON_CREDITO,DIASCRED,LIMCRED,SALDO,LISTA_PREC,CVE_BITA,ULT_PAGOD,ULT_PAGOM,ULT_PAGOF,DESCUENTO,ULT_VENTAD,ULT_COMPM,FCH_ULTCOM,VENTAS,CVE_VEND,CVE_OBS,TIPO_EMPRESA,MATRIZ,PROSPECTO,CALLE_ENVIO,NUMINT_ENVIO,NUMEXT_ENVIO,CRUZAMIENTOS_ENVIO,CRUZAMIENTOS_ENVIO2,COLONIA_ENVIO,LOCALIDAD_ENVIO,MUNICIPIO_ENVIO,ESTADO_ENVIO,PAIS_ENVIO,CODIGO_ENVIO,CVE_ZONA_ENVIO,REFERENCIA_ENVIO,CUENTA_CONTABLE,ADDENDAF,ADDENDAD,NAMESPACE,METODODEPAGO,NUMCTAPAGO,MODELO,DES_IMPU1,DES_IMPU2,DES_IMPU3,DES_IMPU4,DES_PER,LAT_GENERAL,LON_GENERAL,LAT_ENVIO,LON_ENVIO,UUID,VERSION_SINC,USO_CFDI,CVE_PAIS_SAT,NUMIDREGFISCAL,FORMADEPAGOSAT);\n\n\n\n\n\nLOAD FROM S3 privileges\n\n\nThe Aurora user that executes \nLOAD DATA FROM S3\n requires the following privilege:\n\n\nGRANT LOAD FROM S3 ON *.* TO 'your-user-name';\n\n\nBy default this privilege is set to your \nMaster Username\n when you \ncreated your Aurora instance.\n\n\nIf you are using a different user and the privilege is not set, the following error appears:\n\n\nAccess denied; you need (at least one of) the LOAD FROM S3 privilege(s) for this operation\n\n\nThe only way to see this error is executing \nLOAD FROM S3\n \nmanually.\n\n\nIf your MySQL user already has this privilege and you see the following error, try \nthese steps.\n\n\nAccess denied for user 'your-user-name'@'xx.xx.xxx.xxx' (using password: YES)\n\n\n\n\nFirebird column name starts with underscore\n\n\nFirebird SQL does not naturally support creating a column starting with underscore, so avoid that on Aurora if your source DB is Firebird.\n\n\nToken unknown - line 1\n\n\n\n\n\n\nNeed more help?\n\n\nPlease send us an email to: \ninfo@factorbi.com", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "When you use manual sync and it is completed successfully, the following dialog appears.   If you face any errors, try looking here.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#syntax-error", 
            "text": "Misspelled fields or tables on customData.json may appear as:", 
            "title": "Syntax error"
        }, 
        {
            "location": "/troubleshooting/#no-internet-connection", 
            "text": "When no internet connection available, the following message appears:", 
            "title": "No Internet Connection"
        }, 
        {
            "location": "/troubleshooting/#firewall-restrictions", 
            "text": "If your internet connection has a firewall, it may show different errors like:   The remote name could not be resolved.  A WebException with status SendFailure was thrown.  A WebException with status NameResolutionFailure was thrown.  Error making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.", 
            "title": "Firewall Restrictions"
        }, 
        {
            "location": "/troubleshooting/#grant-firewall-to-reach-amazon-s3", 
            "text": "Create a policy to Allow to:   54.230.0.0/15  52.192.0.0/11    AWS can have multiple IP addresses for S3 service, so in case the above IP's don't work check the  AWS Public IP Address Ranges documentation  and look for    \"region\": \"GLOBAL\",\n  \"service\": \"AMAZON\"  and    \"region\": \"us-east-1\",\n  \"service\": \"AMAZON\"  https://ip-ranges.amazonaws.com/ip-ranges.json", 
            "title": "Grant Firewall to reach Amazon S3"
        }, 
        {
            "location": "/troubleshooting/#no-information-to-sync", 
            "text": "If  No information to Sync  message appears, verify that customData.json is set to send at least one table.", 
            "title": "No information to Sync"
        }, 
        {
            "location": "/troubleshooting/#waiting-time", 
            "text": "Bipost Sync may take from a few seconds to several minutes to extract from on-prem DB and upload to AWS. While this is happening no messages/icons will show that biPost.exe is working and maybe you'll see  (Not responding)  on the top of the window, this is normal.  If you launch Windows Task Manager probably you'll see that  biPost.exe *32  is running and consuming a considerable amount of CPU.  Once the information is uploaded to AWS, it usually is available on Aurora-MySQL very fast. If a big data set was uploaded it may take up to 5 minutes to be available on Aurora.  If you need to check which tables where loaded, check  aurora_s3_load_history.", 
            "title": "Waiting time"
        }, 
        {
            "location": "/troubleshooting/#upload-limit", 
            "text": "Depending on the number of rows and columns on each table, it is possible that a large amount of data sent on a single sync may not load to Aurora-MySQL.  We have tested up to 1.5 million rows on a single sync and works fine.  We recommend using  Recursive Sync  for big tables that have a datetime field available.", 
            "title": "Upload Limit"
        }, 
        {
            "location": "/troubleshooting/#special-characters", 
            "text": "Some special characters on  char  and  varchar  fields are not supported and thus removed by biPost.exe\nFor example:   Enter  ()   On  SQL Server , all special characters on strings of 100 length or more are removed, leaving only letters and numbers.", 
            "title": "Special Characters"
        }, 
        {
            "location": "/troubleshooting/#schema-limitations", 
            "text": "NULL  values on char and varchar are converted to  ''  on MySQL.  NULL  values on float, money and int datatypes are converted to value  0  (zero) on MySQL.  NULL  values on datetime are converted to  0000-00-00 00:00:00  on MySQL.  bit  datatype is converted to  VARCHAR(1)  on MySQL.  Tables without a  PRIMARY KEY  will display an error message.", 
            "title": "Schema Limitations"
        }, 
        {
            "location": "/troubleshooting/#appdatalocal-folder", 
            "text": "Sometimes it is necessary to manually delete the content of  \\AppData\\Local\\biPost  folder.  Open a new Windows Explorer and enter  %localappdata%\\bipost . Select all and delete.  ************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.", 
            "title": "AppData\\Local folder"
        }, 
        {
            "location": "/troubleshooting/#mysql-schemas-are-created-but-no-data-is-loaded", 
            "text": "Two things might be causing this problem:", 
            "title": "MySQL schemas are created but no data is loaded"
        }, 
        {
            "location": "/troubleshooting/#1-rds-instance-cannot-reach-s3-bucket", 
            "text": "When we look at our CloudWatch logs, we see  Unable to initialize S3Stream , so do the following:    Check if your  IAM Policy to Grant Access to S3  is set correctly using the S3 bucket ARN we provided. Also double check the policy document (JSON).    Check that  IAM Role  has attached the former IAM Policy. Copy ARN Role to a notepad for next steps.    Go to  RDS Parameter Groups , select the cluster group and click  Compare Parameters , it should show the IAM ARN Role (the one you just copied on a notepad) on the parameters shown  here.    Go to  RDS Clusters  and check if  IAM Role  is listed and active for your cluster.    Double check IAM roles attached to your instance querying  show global variables like '%role%'     After this, if you still experience this error, check out  Manually debugging S3Stream.", 
            "title": "1. RDS instance cannot reach S3 bucket."
        }, 
        {
            "location": "/troubleshooting/#2-name-of-your-destination-database-must-be-all-lower-case", 
            "text": "When we look at our CloudWatch logs, we see:  SequelizeConnectionError: ER_BAD_DB_ERROR: Unknown database  Double check that your DB name is all lower case.", 
            "title": "2. Name of your destination database must be all lower case."
        }, 
        {
            "location": "/troubleshooting/#manually-debugging-s3stream", 
            "text": "In this section we will manually upload data to Aurora-MySQL. The goal here is to see whether an error is shown while directly importing data from S3 to Aurora-MySQL.  Using MySQL Workbench (or any client of your preference), open a connection to your MySQL instance preferably using  root  account.  Syntax  LOAD DATA FROM S3 's3-us-east-1://{my-bucket-name}/{my-36-digit-service-number}/process_{my-36-digit-service-number}{YYYY_M_D-of-yesterday}_post/{my-example-table}.csv'\nREPLACE INTO TABLE `{my-database-name}`.`{my-example-table}`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n({column_1},{column_2},...,{column_n});  Replace curly brackets with your data.  Parameters   my-bucket-name : Name of your dedicated bucket, assigned by Factor BI.  my-36-digit-service-number : Number assigned to your service.  YYYY_M_D-of-yesterday : Date for yesterday on GMT America/Mexico_City.  my-example-table : Pick any of the tables that you are synchronizing.  my-database-name : Name of your destination DB on MySQL.  {column_1},{column_2},...,{column_n} : List all the columns in the same order as they appear in the .CSV file created on  AppData\\Local folder .   Example  LOAD DATA FROM S3 's3-us-east-1://bipostdata-f0123abc4567/a1bcd23e-4fa5-67b8-cd9e-f0123abc4567/process_a1bcd23e-4fa5-67b8-cd9e-f0123abc45672017_9_13_post/CLIE.csv'\nREPLACE INTO TABLE `mytestdb`.`clie`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CLAVE,STATUS,NOMBRE,RFC,CALLE,NUMINT,NUMEXT,CRUZAMIENTOS,CRUZAMIENTOS2,COLONIA,CODIGO,LOCALIDAD,MUNICIPIO,ESTADO,NACIONALIDAD,REFERDIR,TELEFONO,CLASIFIC,FAX,PAG_WEB,CURP,CVE_ZONA,IMPRIR,MAIL,NIVELSEC,ENVIOSILEN,EMAILPRED,DIAREV,DIAPAGO,CON_CREDITO,DIASCRED,LIMCRED,SALDO,LISTA_PREC,CVE_BITA,ULT_PAGOD,ULT_PAGOM,ULT_PAGOF,DESCUENTO,ULT_VENTAD,ULT_COMPM,FCH_ULTCOM,VENTAS,CVE_VEND,CVE_OBS,TIPO_EMPRESA,MATRIZ,PROSPECTO,CALLE_ENVIO,NUMINT_ENVIO,NUMEXT_ENVIO,CRUZAMIENTOS_ENVIO,CRUZAMIENTOS_ENVIO2,COLONIA_ENVIO,LOCALIDAD_ENVIO,MUNICIPIO_ENVIO,ESTADO_ENVIO,PAIS_ENVIO,CODIGO_ENVIO,CVE_ZONA_ENVIO,REFERENCIA_ENVIO,CUENTA_CONTABLE,ADDENDAF,ADDENDAD,NAMESPACE,METODODEPAGO,NUMCTAPAGO,MODELO,DES_IMPU1,DES_IMPU2,DES_IMPU3,DES_IMPU4,DES_PER,LAT_GENERAL,LON_GENERAL,LAT_ENVIO,LON_ENVIO,UUID,VERSION_SINC,USO_CFDI,CVE_PAIS_SAT,NUMIDREGFISCAL,FORMADEPAGOSAT);", 
            "title": "Manually debugging S3Stream"
        }, 
        {
            "location": "/troubleshooting/#load-from-s3-privileges", 
            "text": "The Aurora user that executes  LOAD DATA FROM S3  requires the following privilege:  GRANT LOAD FROM S3 ON *.* TO 'your-user-name';  By default this privilege is set to your  Master Username  when you  created your Aurora instance.  If you are using a different user and the privilege is not set, the following error appears:  Access denied; you need (at least one of) the LOAD FROM S3 privilege(s) for this operation  The only way to see this error is executing  LOAD FROM S3   manually.  If your MySQL user already has this privilege and you see the following error, try  these steps.  Access denied for user 'your-user-name'@'xx.xx.xxx.xxx' (using password: YES)", 
            "title": "LOAD FROM S3 privileges"
        }, 
        {
            "location": "/troubleshooting/#firebird-column-name-starts-with-underscore", 
            "text": "Firebird SQL does not naturally support creating a column starting with underscore, so avoid that on Aurora if your source DB is Firebird.  Token unknown - line 1", 
            "title": "Firebird column name starts with underscore"
        }, 
        {
            "location": "/troubleshooting/#need-more-help", 
            "text": "Please send us an email to:  info@factorbi.com", 
            "title": "Need more help?"
        }, 
        {
            "location": "/businessintelligence/", 
            "text": "Use Case\n\n\nAt \nFactor BI\n we use Bipost Sync to power small and medium companies with Dashboards using \nGoogle Data Studio\n and \nAWS QuickSight\n\n\nWant to see a demo? Go to --\n \nGoogle Data Studio Demo.\n\n\nHere is an example of the architected solution.\n\n\n\n\nIn this example some dashboards are built with AWS QuickSight because it has an iPhone App so it is used by high level executives on the go.\n\n\n\n\nGoogle Data Studio is used for desktop users and detailed analytical situations, because it has great date \n string filters, excellent UX, and dashboards are shared using \nGoogle accounts,\n which is a great way to avoid managing user accounts.", 
            "title": "Business Intelligence"
        }, 
        {
            "location": "/businessintelligence/#use-case", 
            "text": "At  Factor BI  we use Bipost Sync to power small and medium companies with Dashboards using  Google Data Studio  and  AWS QuickSight  Want to see a demo? Go to --   Google Data Studio Demo.  Here is an example of the architected solution.   In this example some dashboards are built with AWS QuickSight because it has an iPhone App so it is used by high level executives on the go.   Google Data Studio is used for desktop users and detailed analytical situations, because it has great date   string filters, excellent UX, and dashboards are shared using  Google accounts,  which is a great way to avoid managing user accounts.", 
            "title": "Use Case"
        }, 
        {
            "location": "/intelisis/", 
            "text": "Intelisis ERP\n\n\nIntelisis es un ERP que fabrica la empresa \nIntelisis Software, S.A. de C.V.\n\n\nEs un sistema grande y las soluciones de Business Intelligence se desarrollan a la medida de cada necesidad.\n\n\nEn nuestro \nrepositorio de GitHub\n podr\u00e1s encontrar varios objetos de ejemplo para realizar la sincronizaci\u00f3n de la base de SQL hacia MySQL-Aurora. Estos objetos se publican bajo la licencia \nGNU GPLv3\n GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.\n\n\n\n\nCasos de Uso\n\n\nFolleto Casos de Uso: \nTableros de Mando \n Presupuestos\n\n\n\n\n\n\nEjemplos de configuraci\u00f3n\n\n\nEjemplos de informaci\u00f3n a sincronizar usando \ncustomData.json\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"Venta\",\n        \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"VentaD\",\n        \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n        \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\"\n    }\n]\n\n\n\nM\u00e1s ejemplos en nuestro \nrepositorio de GitHub.\n\n\n\n\nRecomendaciones\n\n\nCrear dos carpetas\n\n\nCrea al menos dos carpetas para Bipost Sync, esto te permitir\u00e1 sincronizar cat\u00e1logos y movimientos en forma independiente.\n\n\nCarpeta Cat\u00e1logos\n\n\nTu primer carpeta donde est\u00e1 Bipost Sync la puedes nombrar \nBipost_catalogos\n y utilizar\u00e1s \ncustomData.json\n para enviar \u00fanicamente cat\u00e1logos. Ejemplo:\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"Sucursal\",\n        \"fields\": \"Sucursal, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Agente\",\n        \"fields\": \"Agente, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n      \"active\": \"true\",\n      \"table\": \"MovTipo\",\n      \"fields\": \"Modulo, Mov, Clave\",\n      \"join\": \"\",\n      \"filter\": \"\",\n      \"recursiveDateField\": \"\"\n    },\n    {\n      \"active\": \"true\",\n      \"table\": \"Cta\",\n      \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n      \"join\": \"\",\n      \"filter\": \"\",\n      \"recursiveDateField\": \"\"\n    },\n    {\n      \"active\": \"true\",\n      \"table\": \"CentroCostos\",\n      \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n      \"join\": \"\",\n      \"filter\": \"\",\n      \"recursiveDateField\": \"\"\n    }\n]\n\n\n\nAbre biPost.exe y aseg\u00farate de quitar el check \nRecursive Sync\n.\n\n\nUsando el JSON del ejemplo anterior, se enviar\u00e1 en cada sincronizaci\u00f3n \ntodos los renglones\n de las tablas \nSucursal\n, \nAgente\n, \nMovTipo\n, \nCta\n y \nCentroCostos\n. Debido a que Bipost API utiliza una \nsentencia \nREPLACE\n en la base destino de MySQL, los registros se reemplazan utilizando la llave primaria de la tabla origen.\n\n\nNOTA:\n Si la tabla que vas a enviar tiene m\u00e1s de un mill\u00f3n de registros, es recomendable que filtres el query utilizando la opci\u00f3n \n\"filter\":\n.\n\n\nCarpeta Recursivo\n\n\nLa otra carpeta de Bipost Sync puedes nombrarla \nBipost_diario\n. Abre el archivo \ncustomData.json\n y especifica las tablas de movimientos que vas a enviar, por ejemplo:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"Cont\",\n    \"fields\": \"Cont.ID, Cont.Empresa, Cont.Mov, Cont.MovID, Cont.FechaEmision, Cont.FechaContable, Cont.Proyecto, Cont.Moneda, Cont.TipoCambio, Cont.Estatus, Cont.Ejercicio, Cont.Periodo, Cont.Moneda2, Cont.TipoCambio2\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus \n 'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"ContD\",\n    \"fields\": \"ContD.ID, ContD.Renglon, ContD.RenglonSub, ContD.Cuenta, ContD.SubCuenta, ContD.SubCuenta2, ContD.SubCuenta3, ContD.Debe, ContD.Debe2, ContD.Haber, ContD.Haber2, ContD.Sucursal, ContD.SucursalContable \",\n    \"join\": \"Cont WITH (NOLOCK) ON Cont.ID = ContD.ID JOIN MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus \n 'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  }\n]\n\n\n\nEs importante que incluyas el par\u00e1metro \n\"recursiveDateField\":\n ya que lo utiliza Bipost Sync para optimizar la extracci\u00f3n y subida de informaci\u00f3n, por ejemplo: Supongamos que la tabla ContD tiene millones de registros, por tanto al momento de configurar \n\"recursiveDateField\":\n lo utilizar\u00e1s en conjunto con el check \nRecursive Sync\n, ubicado en la pesta\u00f1a \nGeneral Settings\n de biPost.exe:\n\n\n\n\nEl rango de fechas que especifiques en \nGeneral Settings\n se utilizar\u00e1 para generar una extracci\u00f3n y subida individual hacia AWS, reduciendo el tama\u00f1o de los datos y el tiempo de carga hacia Aurora-MySQL.\n\n\nMediante esta opci\u00f3n se pueden cargar hist\u00f3ricos de varios a\u00f1os.\n\n\nNOTA: Al apagar el check \nRecursive Sync\n autom\u00e1ticamente la fecha de inicio y fin se fija en el d\u00eda de hoy utilizando la fecha de la m\u00e1quina Windows.\n\n\nUso del Filter\n\n\nEn el ejemplo customData.json anterior, se utiliz\u00f3 el siguiente filtro para la tabla \nContD\n:\n\n\n\"filter\": \"Cont.Estatus \n 'SINAFECTAR'\n\n\nSi un registro del m\u00f3dulo Contabilidad se cancela, cambia su estatus a \nCANCELADO\n por tanto el query a la base de datos incluye los estatus \nCANCELADO\n para que estos cambios se vean reflejados en la base de Aurora-MySQL.\n\n\nSi por el contrario se utiliza el filtro \n\"Cont.Estatus = 'CONCLUIDO'\n entonces el query omite los estatus \nCANCELADO\n y al cambiar un ID a estatus \nCANCELADO\n no se reflejar\u00e1 el cambio en la base de MySQL, causando inconsistencias.\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?\n\n\nEscr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Intelisis"
        }, 
        {
            "location": "/intelisis/#intelisis-erp", 
            "text": "Intelisis es un ERP que fabrica la empresa  Intelisis Software, S.A. de C.V.  Es un sistema grande y las soluciones de Business Intelligence se desarrollan a la medida de cada necesidad.  En nuestro  repositorio de GitHub  podr\u00e1s encontrar varios objetos de ejemplo para realizar la sincronizaci\u00f3n de la base de SQL hacia MySQL-Aurora. Estos objetos se publican bajo la licencia  GNU GPLv3  GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.", 
            "title": "Intelisis ERP"
        }, 
        {
            "location": "/intelisis/#casos-de-uso", 
            "text": "Folleto Casos de Uso:  Tableros de Mando   Presupuestos", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/intelisis/#ejemplos-de-configuracion", 
            "text": "Ejemplos de informaci\u00f3n a sincronizar usando  customData.json  [\n    {\n        \"active\": \"true\",\n        \"table\": \"Venta\",\n        \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"VentaD\",\n        \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n        \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n        \"filter\": \"Venta.Estatus IN ('PENDIENTE','CONCLUIDO','PROCESAR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n        \"recursiveDateField\": \"Venta.FechaEmision\"\n    }\n]  M\u00e1s ejemplos en nuestro  repositorio de GitHub.", 
            "title": "Ejemplos de configuraci\u00f3n"
        }, 
        {
            "location": "/intelisis/#recomendaciones", 
            "text": "", 
            "title": "Recomendaciones"
        }, 
        {
            "location": "/intelisis/#crear-dos-carpetas", 
            "text": "Crea al menos dos carpetas para Bipost Sync, esto te permitir\u00e1 sincronizar cat\u00e1logos y movimientos en forma independiente.", 
            "title": "Crear dos carpetas"
        }, 
        {
            "location": "/intelisis/#carpeta-catalogos", 
            "text": "Tu primer carpeta donde est\u00e1 Bipost Sync la puedes nombrar  Bipost_catalogos  y utilizar\u00e1s  customData.json  para enviar \u00fanicamente cat\u00e1logos. Ejemplo:  [\n    {\n        \"active\": \"true\",\n        \"table\": \"Sucursal\",\n        \"fields\": \"Sucursal, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Agente\",\n        \"fields\": \"Agente, Nombre, Categoria, Estatus\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n      \"active\": \"true\",\n      \"table\": \"MovTipo\",\n      \"fields\": \"Modulo, Mov, Clave\",\n      \"join\": \"\",\n      \"filter\": \"\",\n      \"recursiveDateField\": \"\"\n    },\n    {\n      \"active\": \"true\",\n      \"table\": \"Cta\",\n      \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n      \"join\": \"\",\n      \"filter\": \"\",\n      \"recursiveDateField\": \"\"\n    },\n    {\n      \"active\": \"true\",\n      \"table\": \"CentroCostos\",\n      \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus\",\n      \"join\": \"\",\n      \"filter\": \"\",\n      \"recursiveDateField\": \"\"\n    }\n]  Abre biPost.exe y aseg\u00farate de quitar el check  Recursive Sync .  Usando el JSON del ejemplo anterior, se enviar\u00e1 en cada sincronizaci\u00f3n  todos los renglones  de las tablas  Sucursal ,  Agente ,  MovTipo ,  Cta  y  CentroCostos . Debido a que Bipost API utiliza una  sentencia  REPLACE  en la base destino de MySQL, los registros se reemplazan utilizando la llave primaria de la tabla origen.  NOTA:  Si la tabla que vas a enviar tiene m\u00e1s de un mill\u00f3n de registros, es recomendable que filtres el query utilizando la opci\u00f3n  \"filter\": .", 
            "title": "Carpeta Cat\u00e1logos"
        }, 
        {
            "location": "/intelisis/#carpeta-recursivo", 
            "text": "La otra carpeta de Bipost Sync puedes nombrarla  Bipost_diario . Abre el archivo  customData.json  y especifica las tablas de movimientos que vas a enviar, por ejemplo:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"Cont\",\n    \"fields\": \"Cont.ID, Cont.Empresa, Cont.Mov, Cont.MovID, Cont.FechaEmision, Cont.FechaContable, Cont.Proyecto, Cont.Moneda, Cont.TipoCambio, Cont.Estatus, Cont.Ejercicio, Cont.Periodo, Cont.Moneda2, Cont.TipoCambio2\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus   'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"ContD\",\n    \"fields\": \"ContD.ID, ContD.Renglon, ContD.RenglonSub, ContD.Cuenta, ContD.SubCuenta, ContD.SubCuenta2, ContD.SubCuenta3, ContD.Debe, ContD.Debe2, ContD.Haber, ContD.Haber2, ContD.Sucursal, ContD.SucursalContable \",\n    \"join\": \"Cont WITH (NOLOCK) ON Cont.ID = ContD.ID JOIN MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus   'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  }\n]  Es importante que incluyas el par\u00e1metro  \"recursiveDateField\":  ya que lo utiliza Bipost Sync para optimizar la extracci\u00f3n y subida de informaci\u00f3n, por ejemplo: Supongamos que la tabla ContD tiene millones de registros, por tanto al momento de configurar  \"recursiveDateField\":  lo utilizar\u00e1s en conjunto con el check  Recursive Sync , ubicado en la pesta\u00f1a  General Settings  de biPost.exe:   El rango de fechas que especifiques en  General Settings  se utilizar\u00e1 para generar una extracci\u00f3n y subida individual hacia AWS, reduciendo el tama\u00f1o de los datos y el tiempo de carga hacia Aurora-MySQL.  Mediante esta opci\u00f3n se pueden cargar hist\u00f3ricos de varios a\u00f1os.  NOTA: Al apagar el check  Recursive Sync  autom\u00e1ticamente la fecha de inicio y fin se fija en el d\u00eda de hoy utilizando la fecha de la m\u00e1quina Windows.", 
            "title": "Carpeta Recursivo"
        }, 
        {
            "location": "/intelisis/#uso-del-filter", 
            "text": "En el ejemplo customData.json anterior, se utiliz\u00f3 el siguiente filtro para la tabla  ContD :  \"filter\": \"Cont.Estatus   'SINAFECTAR'  Si un registro del m\u00f3dulo Contabilidad se cancela, cambia su estatus a  CANCELADO  por tanto el query a la base de datos incluye los estatus  CANCELADO  para que estos cambios se vean reflejados en la base de Aurora-MySQL.  Si por el contrario se utiliza el filtro  \"Cont.Estatus = 'CONCLUIDO'  entonces el query omite los estatus  CANCELADO  y al cambiar un ID a estatus  CANCELADO  no se reflejar\u00e1 el cambio en la base de MySQL, causando inconsistencias.", 
            "title": "Uso del Filter"
        }, 
        {
            "location": "/intelisis/#contacto", 
            "text": "\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?  Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/microsip/", 
            "text": "Microsip ERP\n\n\nMicrosip es un sistema administrativo ERP que fabrica la empresa \nAplicaciones y Proyectos Computacionales S.A. de C.V.\n\n\nFactor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.\n\n\nEn nuestro \nrepositorio de GitHub\n podr\u00e1s encontrar estos objetos bajo la licencia \nGNU GPLv3\n GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.\n\n\n\n\nCasos de Uso\n\n\nFolleto Casos de Uso: \nTableros de Mando \n Presupuestos\n\n\n\n\n\n\nTablas Microsip\n\n\n\n\nAl configurar Bipost Sync con \nSystem: Microsip\n, de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:\n\n\nagentes\nalmacenes\nanticipos_cc\narticulos\nbancos\nbeneficiarios\ncajas\ncajeros\ncentros_costo\nciudades\nclaves_articulos\nclientes\ncobradores\ncomprom_articulos\nconceptos_ba\nconceptos_cc\nconceptos_cp\nconceptos_in\ncondiciones_pago\ncondiciones_pago_cp\ncuentas_bancarias\ncuentas_co\ndepositos_cc\ndepositos_cc_det\ndeptos_co\ndirs_clientes\ndoctos_ba\ndoctos_cc\ndoctos_cm\ndoctos_cm_det\ndoctos_cp\ndoctos_in\ndoctos_in_det\ndoctos_ve\ndoctos_ve_det\nestados\nexis_discretos\nformas_cobro_cc\nformas_cobro_doctos\ngrupos_lineas\nhistoria_cambiaria\nimportes_doctos_cc\nimportes_doctos_cp\nimpuestos\nimpuestos_doctos_ve\nlibres_articulos\nlibres_cargos_cc\nlibres_cargos_cp\nlibres_clientes\nlibres_com_cm\nlibres_cot_ve\nlibres_creditos_cc\nlibres_creditos_cp\nlibres_ctas_ban\nlibres_cuentas_co\nlibres_devcom_cm\nlibres_devfac_ve\nlibres_fac_ve\nlibres_ped_ve\nlibres_pol_co\nlibres_proveedor\nlineas_articulos\nmonedas\npaises\nplazos_cond_pag\nplazos_cond_pag_cp\npoliticas_comisiones_vendedores\nproveedores\nroles_claves_articulos\nsaldos_ba\nsaldos_cc\nsaldos_co\nsaldos_cp\nsaldos_in\nsucursales\ntipos_clientes\ntipos_impuestos\ntipos_polizas\ntipos_prov\ntraspasos_ba\nusos_anticipos_cc\nvencimientos_cargos_cc\nvencimientos_cargos_cm\nvencimientos_cargos_cp\nvencimientos_cargos_ve\nvendedores\nvias_embarque\nzonas_clientes\n\n\n\nEn el siguiente link est\u00e1 el listado de tablas de Microsip.\n\n\nTablas Microsip, archivo Google Sheets.\n\n\nNOTA:\n La lista del link anterior puede no estar completa.\n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nA\u00f1adir tablas a la Sincronizaci\u00f3n\n\n\nPara incluir tablas adicionales en la sincronizaci\u00f3n se utiliza el archivo \ncustomData.json\n, ejemplo:\n\n\n    [\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_CFDI\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },          \n      {\n        \"active\": \"true\",\n        \"table\": \"USOS_FOLIOS_FISCALES\",\n        \"fields\": \"USO_FOLIO_ID, FOLIOS_FISCALES_ID, FOLIO, FECHA, SISTEMA, DOCTO_ID, PROV_CERT, FECHA_HORA_TIMBRADO, UUID, CFDI_ID \",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPUESTOS_ARTICULOS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPUESTOS_DOCTOS_CM\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_PV\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"FECHA\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_PV_DET\",\n        \"fields\": \"DOCTOS_PV_DET.DOCTO_PV_DET_ID, DOCTOS_PV_DET.DOCTO_PV_ID, DOCTOS_PV_DET.CLAVE_ARTICULO, DOCTOS_PV_DET.ARTICULO_ID, DOCTOS_PV_DET.UNIDADES, DOCTOS_PV_DET.UNIDADES_DEV, DOCTOS_PV_DET.TIPO_CONTAB_UNID, DOCTOS_PV_DET.PRECIO_UNITARIO, DOCTOS_PV_DET.PRECIO_UNITARIO_IMPTO, DOCTOS_PV_DET.IMPUESTO_POR_UNIDAD, DOCTOS_PV_DET.PCTJE_DSCTO, DOCTOS_PV_DET.PRECIO_TOTAL_NETO, DOCTOS_PV_DET.PRECIO_MODIFICADO, DOCTOS_PV_DET.VENDEDOR_ID, DOCTOS_PV_DET.PCTJE_COMIS, DOCTOS_PV_DET.ROL, DOCTOS_PV_DET.ES_TRAN_ELECT, DOCTOS_PV_DET.ESTATUS_TRAN_ELECT, DOCTOS_PV_DET.POSICION, DOCTOS_PV_DET.DSCTO_ART, DOCTOS_PV_DET.DSCTO_EXTRA\",\n        \"join\": \"DOCTOS_PV ON DOCTOS_PV_DET.DOCTO_PV_ID = DOCTOS_PV.DOCTO_PV_ID\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"DOCTOS_PV.FECHA\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPUESTOS_DOCTOS_PV\",\n        \"fields\": \"IMPUESTOS_DOCTOS_PV.DOCTO_PV_ID, IMPUESTOS_DOCTOS_PV.IMPUESTO_ID, IMPUESTOS_DOCTOS_PV.VENTA_NETA, IMPUESTOS_DOCTOS_PV.VENTA_BRUTA, IMPUESTOS_DOCTOS_PV.OTROS_IMPUESTOS, IMPUESTOS_DOCTOS_PV.PCTJE_IMPUESTO, IMPUESTOS_DOCTOS_PV.IMPORTE_IMPUESTO, IMPUESTOS_DOCTOS_PV.UNIDADES_IMPUESTO, IMPUESTOS_DOCTOS_PV.IMPORTE_UNITARIO_IMPUESTO \",\n        \"join\": \"DOCTOS_PV ON IMPUESTOS_DOCTOS_PV.DOCTO_PV_ID = DOCTOS_PV.DOCTO_PV_ID\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"DOCTOS_PV.FECHA\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_PV_LIGAS\",\n        \"fields\": \"DOCTOS_PV_LIGAS.DOCTO_PV_LIGA_ID, DOCTOS_PV_LIGAS.DOCTO_PV_FTE_ID, DOCTOS_PV_LIGAS.DOCTO_PV_DEST_ID\",\n        \"join\": \"DOCTOS_PV ON DOCTOS_PV_LIGAS.DOCTO_PV_DEST_ID = DOCTOS_PV.DOCTO_PV_ID\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"DOCTOS_PV.FECHA\"\n      }\n    ]\n\n\n\nPara m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver \naqu\u00ed.\n\n\n\n\nSincroniza desde AWS hacia Microsip\n\n\n\n\n\n\nAl usar la sincronizaci\u00f3n con \nDownload Data\n es posible traer datos desde AWS Aurora-MySQL hacia tu base de Microsip. \n\n\n\n\n\n\nCuando activas la opci\u00f3n \nProcess Data\n se insertan tus datos en Firebird.\n\n\n\n\n\n\nUsando la opci\u00f3n \nFinal Query\n puedes correr cualquier stored procedure para que termines de calcular y procesar hacia otras tablas. \n\n\n\n\n\n\nPor ejemplo, despu\u00e9s que se han recibido e insertado los datos en Firebird, queremos correr el store \nspFinal\n as\u00ed que usamos el par\u00e1metro \n\"finalQuery\": \"execute procedure spFinal;\"\n dentro de \noutData.json\n. Aqu\u00ed ejemplo del contenido de \nspFinal\n:\n\n\n    SET TERM ^ ;\n\n    RECREATE PROCEDURE SPFINAL\n    AS\n    DECLARE VARIABLE id            VARCHAR(50);\n    DECLARE VARIABLE idd           VARCHAR(50);\n    DECLARE VARIABLE tipod         VARCHAR(8);\n    DECLARE VARIABLE docto_id      BIGINT;\n    DECLARE VARIABLE new_docto_id  BIGINT;\n    DECLARE VARIABLE new_doctod_id BIGINT;\n    DECLARE VARIABLE estatus       VARCHAR(1);\n    DECLARE VARIABLE new_posicion  INT;\n    DECLARE VARIABLE posicion      INT;\n    BEGIN\n      -- DOCTOS_VE\n      FOR SELECT id FROM EXPORT_DOCTOS_VE INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_VE WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_VE_ID FROM DOCTOS_VE WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_VE(\n                   DOCTO_VE_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CONSIG_ID, ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CLI_ID,    ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, ID\n              FROM EXPORT_DOCTOS_VE\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_VE_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_VE_DET(\n                     DOCTO_VE_DET_ID, DOCTO_VE_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_VE_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_VE SET ESTATUS = :estatus WHERE DOCTO_VE_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_CM\n      FOR SELECT id FROM EXPORT_DOCTOS_CM INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_CM WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_CM_ID FROM DOCTOS_CM WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_CM(\n                   DOCTO_CM_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , ID\n              FROM EXPORT_DOCTOS_CM\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_CM_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_CM_DET(\n                     DOCTO_CM_DET_ID, DOCTO_CM_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_CM_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_CM SET ESTATUS = :estatus WHERE DOCTO_CM_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_IN\n      FOR SELECT id FROM EXPORT_DOCTOS_IN INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT CANCELADO   FROM EXPORT_DOCTOS_IN WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_IN_ID FROM DOCTOS_IN WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_IN(\n                   DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, WEBID)\n            SELECT :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, ID\n              FROM EXPORT_DOCTOS_IN\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_IN_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_IN_DET(\n                     DOCTO_IN_DET_ID, DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID)\n              SELECT :new_doctod_id,  :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID\n                FROM EXPORT_DOCTOS_IN_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_IN SET CANCELADO = :estatus WHERE DOCTO_IN_ID = :docto_id;\n        END\n      END    \n      SUSPEND;\n    END^\n\n    SET TERM ; ^\n\n\n\n\n\nTablas B.I.\n\n\nAl correr los scripts del \nrepositorio de GitHub,\n se crean las siguientes tablas.\n\n\n\n\n\n\n\n\nTabla\n\n\nDescripci\u00f3n\n\n\n\n\n\n\n\n\n\n\nT\n\n\nTiraje de numeraci\u00f3n consecutiva.\n\n\n\n\n\n\ntime\n\n\nUtil para usarse con dimensiones de tiempo en el B.I.\n\n\n\n\n\n\nym\n\n\nUtil para usarse con dimensiones a\u00f1o y mes en el B.I.\n\n\n\n\n\n\nbiabc\n\n\nClasificaci\u00f3n ABC de los inventarios.\n\n\n\n\n\n\nbiblia\n\n\nMatriz de estad\u00edsticas de inventario por mes.\n\n\n\n\n\n\nmodulosgrupo1\n\n\nUtil para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.\n\n\n\n\n\n\nbusinessDay\n\n\nD\u00edas laborales de la empresa.\n\n\n\n\n\n\ncustomDates\n\n\nPasa una fecha espec\u00edfica a las funciones \nfnSyncDate()\n, \nfnAgingDate\n y \nfnServiceDate\n\n\n\n\n\n\ndateInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nymInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nsyncInfo\n\n\nContiene informaci\u00f3n de la sincronizaci\u00f3n.\n\n\n\n\n\n\nlogPostInitial\n\n\nLog de sincronizaciones, se ejecuta al inicio (antes de cargar la data).\n\n\n\n\n\n\nlogPostFinal\n\n\nLog de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).\n\n\n\n\n\n\nmatrixrep1\n\n\nEstructura matricial \u00fatil para reportes en Google Data Studio.\n\n\n\n\n\n\ncontagrupo1\n\n\nResumen contable por a\u00f1o y mes.\n\n\n\n\n\n\ncontagrupo2\n\n\nResumen contable por fecha.\n\n\n\n\n\n\neqtabla\n\n\nTabla para agregar equivalencias.\n\n\n\n\n\n\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?\n\n\nEscr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Microsip"
        }, 
        {
            "location": "/microsip/#microsip-erp", 
            "text": "Microsip es un sistema administrativo ERP que fabrica la empresa  Aplicaciones y Proyectos Computacionales S.A. de C.V.  Factor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.  En nuestro  repositorio de GitHub  podr\u00e1s encontrar estos objetos bajo la licencia  GNU GPLv3  GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.", 
            "title": "Microsip ERP"
        }, 
        {
            "location": "/microsip/#casos-de-uso", 
            "text": "Folleto Casos de Uso:  Tableros de Mando   Presupuestos", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/microsip/#tablas-microsip", 
            "text": "Al configurar Bipost Sync con  System: Microsip , de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:  agentes\nalmacenes\nanticipos_cc\narticulos\nbancos\nbeneficiarios\ncajas\ncajeros\ncentros_costo\nciudades\nclaves_articulos\nclientes\ncobradores\ncomprom_articulos\nconceptos_ba\nconceptos_cc\nconceptos_cp\nconceptos_in\ncondiciones_pago\ncondiciones_pago_cp\ncuentas_bancarias\ncuentas_co\ndepositos_cc\ndepositos_cc_det\ndeptos_co\ndirs_clientes\ndoctos_ba\ndoctos_cc\ndoctos_cm\ndoctos_cm_det\ndoctos_cp\ndoctos_in\ndoctos_in_det\ndoctos_ve\ndoctos_ve_det\nestados\nexis_discretos\nformas_cobro_cc\nformas_cobro_doctos\ngrupos_lineas\nhistoria_cambiaria\nimportes_doctos_cc\nimportes_doctos_cp\nimpuestos\nimpuestos_doctos_ve\nlibres_articulos\nlibres_cargos_cc\nlibres_cargos_cp\nlibres_clientes\nlibres_com_cm\nlibres_cot_ve\nlibres_creditos_cc\nlibres_creditos_cp\nlibres_ctas_ban\nlibres_cuentas_co\nlibres_devcom_cm\nlibres_devfac_ve\nlibres_fac_ve\nlibres_ped_ve\nlibres_pol_co\nlibres_proveedor\nlineas_articulos\nmonedas\npaises\nplazos_cond_pag\nplazos_cond_pag_cp\npoliticas_comisiones_vendedores\nproveedores\nroles_claves_articulos\nsaldos_ba\nsaldos_cc\nsaldos_co\nsaldos_cp\nsaldos_in\nsucursales\ntipos_clientes\ntipos_impuestos\ntipos_polizas\ntipos_prov\ntraspasos_ba\nusos_anticipos_cc\nvencimientos_cargos_cc\nvencimientos_cargos_cm\nvencimientos_cargos_cp\nvencimientos_cargos_ve\nvendedores\nvias_embarque\nzonas_clientes  En el siguiente link est\u00e1 el listado de tablas de Microsip.  Tablas Microsip, archivo Google Sheets.  NOTA:  La lista del link anterior puede no estar completa.  Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Tablas Microsip"
        }, 
        {
            "location": "/microsip/#anadir-tablas-a-la-sincronizacion", 
            "text": "Para incluir tablas adicionales en la sincronizaci\u00f3n se utiliza el archivo  customData.json , ejemplo:      [\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_DET\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_CO_CFDI\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },          \n      {\n        \"active\": \"true\",\n        \"table\": \"USOS_FOLIOS_FISCALES\",\n        \"fields\": \"USO_FOLIO_ID, FOLIOS_FISCALES_ID, FOLIO, FECHA, SISTEMA, DOCTO_ID, PROV_CERT, FECHA_HORA_TIMBRADO, UUID, CFDI_ID \",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPUESTOS_ARTICULOS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPUESTOS_DOCTOS_CM\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_PV\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"FECHA\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_PV_DET\",\n        \"fields\": \"DOCTOS_PV_DET.DOCTO_PV_DET_ID, DOCTOS_PV_DET.DOCTO_PV_ID, DOCTOS_PV_DET.CLAVE_ARTICULO, DOCTOS_PV_DET.ARTICULO_ID, DOCTOS_PV_DET.UNIDADES, DOCTOS_PV_DET.UNIDADES_DEV, DOCTOS_PV_DET.TIPO_CONTAB_UNID, DOCTOS_PV_DET.PRECIO_UNITARIO, DOCTOS_PV_DET.PRECIO_UNITARIO_IMPTO, DOCTOS_PV_DET.IMPUESTO_POR_UNIDAD, DOCTOS_PV_DET.PCTJE_DSCTO, DOCTOS_PV_DET.PRECIO_TOTAL_NETO, DOCTOS_PV_DET.PRECIO_MODIFICADO, DOCTOS_PV_DET.VENDEDOR_ID, DOCTOS_PV_DET.PCTJE_COMIS, DOCTOS_PV_DET.ROL, DOCTOS_PV_DET.ES_TRAN_ELECT, DOCTOS_PV_DET.ESTATUS_TRAN_ELECT, DOCTOS_PV_DET.POSICION, DOCTOS_PV_DET.DSCTO_ART, DOCTOS_PV_DET.DSCTO_EXTRA\",\n        \"join\": \"DOCTOS_PV ON DOCTOS_PV_DET.DOCTO_PV_ID = DOCTOS_PV.DOCTO_PV_ID\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"DOCTOS_PV.FECHA\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"IMPUESTOS_DOCTOS_PV\",\n        \"fields\": \"IMPUESTOS_DOCTOS_PV.DOCTO_PV_ID, IMPUESTOS_DOCTOS_PV.IMPUESTO_ID, IMPUESTOS_DOCTOS_PV.VENTA_NETA, IMPUESTOS_DOCTOS_PV.VENTA_BRUTA, IMPUESTOS_DOCTOS_PV.OTROS_IMPUESTOS, IMPUESTOS_DOCTOS_PV.PCTJE_IMPUESTO, IMPUESTOS_DOCTOS_PV.IMPORTE_IMPUESTO, IMPUESTOS_DOCTOS_PV.UNIDADES_IMPUESTO, IMPUESTOS_DOCTOS_PV.IMPORTE_UNITARIO_IMPUESTO \",\n        \"join\": \"DOCTOS_PV ON IMPUESTOS_DOCTOS_PV.DOCTO_PV_ID = DOCTOS_PV.DOCTO_PV_ID\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"DOCTOS_PV.FECHA\"\n      },\n      {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_PV_LIGAS\",\n        \"fields\": \"DOCTOS_PV_LIGAS.DOCTO_PV_LIGA_ID, DOCTOS_PV_LIGAS.DOCTO_PV_FTE_ID, DOCTOS_PV_LIGAS.DOCTO_PV_DEST_ID\",\n        \"join\": \"DOCTOS_PV ON DOCTOS_PV_LIGAS.DOCTO_PV_DEST_ID = DOCTOS_PV.DOCTO_PV_ID\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"DOCTOS_PV.FECHA\"\n      }\n    ]  Para m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver  aqu\u00ed.", 
            "title": "A\u00f1adir tablas a la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#sincroniza-desde-aws-hacia-microsip", 
            "text": "Al usar la sincronizaci\u00f3n con  Download Data  es posible traer datos desde AWS Aurora-MySQL hacia tu base de Microsip.     Cuando activas la opci\u00f3n  Process Data  se insertan tus datos en Firebird.    Usando la opci\u00f3n  Final Query  puedes correr cualquier stored procedure para que termines de calcular y procesar hacia otras tablas.     Por ejemplo, despu\u00e9s que se han recibido e insertado los datos en Firebird, queremos correr el store  spFinal  as\u00ed que usamos el par\u00e1metro  \"finalQuery\": \"execute procedure spFinal;\"  dentro de  outData.json . Aqu\u00ed ejemplo del contenido de  spFinal :      SET TERM ^ ;\n\n    RECREATE PROCEDURE SPFINAL\n    AS\n    DECLARE VARIABLE id            VARCHAR(50);\n    DECLARE VARIABLE idd           VARCHAR(50);\n    DECLARE VARIABLE tipod         VARCHAR(8);\n    DECLARE VARIABLE docto_id      BIGINT;\n    DECLARE VARIABLE new_docto_id  BIGINT;\n    DECLARE VARIABLE new_doctod_id BIGINT;\n    DECLARE VARIABLE estatus       VARCHAR(1);\n    DECLARE VARIABLE new_posicion  INT;\n    DECLARE VARIABLE posicion      INT;\n    BEGIN\n      -- DOCTOS_VE\n      FOR SELECT id FROM EXPORT_DOCTOS_VE INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_VE WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_VE_ID FROM DOCTOS_VE WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_VE(\n                   DOCTO_VE_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CONSIG_ID, ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_CLIENTE, CLIENTE_ID, DIR_CLI_ID, DIR_CLI_ID,    ALMACEN_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_VIGENCIA_ENTREGA, ORDEN_COMPRA, FECHA_ORDEN_COMPRA, FOLIO_RECIBO_MERCANCIA, FECHA_RECIBO_MERCANCIA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, TOTAL_ANTICIPOS, PESO_EMBARQUE, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXC, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VENDEDOR_ID, PCTJE_COMIS, VIA_EMBARQUE_ID, IMPORTE_COBRO, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, USUARIO_CREADOR, ES_CFD, ENVIADO, FECHA_HORA_ENVIO, CFD_ENVIO_ESPECIAL, CFDI_CERTIFICADO, FECHA_HORA_CREACION, FECHA_HORA_ULT_MODIF, CARGAR_SUN, FECHA_HORA_CANCELACION, ID\n              FROM EXPORT_DOCTOS_VE\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_VE_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_VE_DET(\n                     DOCTO_VE_DET_ID, DOCTO_VE_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_COMPROM, UNIDADES_SURT_DEV, UNIDADES_A_SURTIR, PRECIO_UNITARIO, PCTJE_DSCTO, DSCTO_ART, PCTJE_DSCTO_CLI, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_COMIS, ROL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_VE_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_VE SET ESTATUS = :estatus WHERE DOCTO_VE_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_CM\n      FOR SELECT id FROM EXPORT_DOCTOS_CM INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT ESTATUS     FROM EXPORT_DOCTOS_CM WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_CM_ID FROM DOCTOS_CM WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_CM(\n                   DOCTO_CM_ID,   TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , WEBID)\n            SELECT :new_docto_id, TIPO_DOCTO, SUBTIPO_DOCTO, FOLIO, FECHA, CLAVE_PROV, PROVEEDOR_ID, FOLIO_PROV, FACTURA_DEV, CONSIG_CM_ID, ALMACEN_ID, PEDIMENTO_ID, MONEDA_ID, TIPO_CAMBIO, TIPO_DSCTO, DSCTO_PCTJE, DSCTO_IMPORTE, ESTATUS, APLICADO, FECHA_ENTREGA, DESCRIPCION, IMPORTE_NETO, FLETES, OTROS_CARGOS, TOTAL_IMPUESTOS, TOTAL_RETENCIONES, GASTOS_ADUANALES, OTROS_GASTOS, FORMA_EMITIDA, CONTABILIZADO, ACREDITAR_CXP, SISTEMA_ORIGEN, COND_PAGO_ID, FECHA_DSCTO_PPAG, PCTJE_DSCTO_PPAG, VIA_EMBARQUE_ID, IMPUESTO_SUSTITUIDO_ID, IMPUESTO_SUSTITUTO_ID, CARGAR_SUN, ENVIADO, FECHA_HORA_ENVIO, EMAIL_ENVIO, TIENE_CFD, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION , ID\n              FROM EXPORT_DOCTOS_CM\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_CM_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_CM_DET(\n                     DOCTO_CM_DET_ID, DOCTO_CM_ID,   CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, POSICION)\n              SELECT :new_doctod_id,  :new_docto_id, CLAVE_ARTICULO, ARTICULO_ID, UNIDADES, UNIDADES_REC_DEV, UNIDADES_A_REC, UMED, CONTENIDO_UMED, PRECIO_UNITARIO, PCTJE_DSCTO, PCTJE_DSCTO_PRO, PCTJE_DSCTO_VOL, PCTJE_DSCTO_PROMO, PRECIO_TOTAL_NETO, PCTJE_ARANCEL, NOTAS, :new_posicion\n                FROM EXPORT_DOCTOS_CM_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_CM SET ESTATUS = :estatus WHERE DOCTO_CM_ID = :docto_id;\n        END\n      END\n\n\n      -- DOCTOS_IN\n      FOR SELECT id FROM EXPORT_DOCTOS_IN INTO :id \n      DO\n      BEGIN\n        docto_id = NULL;\n\n        SELECT CANCELADO   FROM EXPORT_DOCTOS_IN WHERE ID = :id INTO :estatus;\n        SELECT DOCTO_IN_ID FROM DOCTOS_IN WHERE WEBID = :id INTO :docto_id;\n\n        IF (docto_id IS NULL) THEN\n        BEGIN\n          new_docto_id = GEN_ID(ID_DOCTOS,1);\n\n          INSERT INTO DOCTOS_IN(\n                   DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, WEBID)\n            SELECT :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, FOLIO, NATURALEZA_CONCEPTO, FECHA, ALMACEN_DESTINO_ID, CENTRO_COSTO_ID, CANCELADO, APLICADO, DESCRIPCION, CUENTA_CONCEPTO, FORMA_EMITIDA, CONTABILIZADO, SISTEMA_ORIGEN, USUARIO_CREADOR, FECHA_HORA_CREACION, USUARIO_AUT_CREACION, USUARIO_ULT_MODIF, FECHA_HORA_ULT_MODIF, USUARIO_AUT_MODIF, USUARIO_CANCELACION, FECHA_HORA_CANCELACION, USUARIO_AUT_CANCELACION, ID\n              FROM EXPORT_DOCTOS_IN\n             WHERE ID = :id;\n\n          new_posicion = 1;\n          FOR SELECT id, tipo, posicion FROM EXPORT_DOCTOS_IN_DET WHERE ID = :id INTO :idd, :tipod, :posicion\n          DO\n          BEGIN\n            new_doctod_id = NULL;\n            new_doctod_id = GEN_ID(ID_DOCTOS,1);\n\n            INSERT INTO DOCTOS_IN_DET(\n                     DOCTO_IN_DET_ID, DOCTO_IN_ID,   ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID)\n              SELECT :new_doctod_id,  :new_docto_id, ALMACEN_ID, CONCEPTO_IN_ID, CLAVE_ARTICULO, ARTICULO_ID, TIPO_MOVTO, UNIDADES, COSTO_UNITARIO, COSTO_TOTAL, METODO_COSTEO, CANCELADO, APLICADO, COSTEO_PEND, PEDIMENTO_PEND, ROL, FECHA, CENTRO_COSTO_ID\n                FROM EXPORT_DOCTOS_IN_DET\n               WHERE ID = :idd\n                 AND tipo = :tipod\n                 AND posicion = :posicion;\n\n            new_posicion = new_posicion + 1;\n          END\n        END\n        ELSE\n        BEGIN\n          UPDATE DOCTOS_IN SET CANCELADO = :estatus WHERE DOCTO_IN_ID = :docto_id;\n        END\n      END    \n      SUSPEND;\n    END^\n\n    SET TERM ; ^", 
            "title": "Sincroniza desde AWS hacia Microsip"
        }, 
        {
            "location": "/microsip/#tablas-bi", 
            "text": "Al correr los scripts del  repositorio de GitHub,  se crean las siguientes tablas.     Tabla  Descripci\u00f3n      T  Tiraje de numeraci\u00f3n consecutiva.    time  Util para usarse con dimensiones de tiempo en el B.I.    ym  Util para usarse con dimensiones a\u00f1o y mes en el B.I.    biabc  Clasificaci\u00f3n ABC de los inventarios.    biblia  Matriz de estad\u00edsticas de inventario por mes.    modulosgrupo1  Util para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.    businessDay  D\u00edas laborales de la empresa.    customDates  Pasa una fecha espec\u00edfica a las funciones  fnSyncDate() ,  fnAgingDate  y  fnServiceDate    dateInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    ymInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    syncInfo  Contiene informaci\u00f3n de la sincronizaci\u00f3n.    logPostInitial  Log de sincronizaciones, se ejecuta al inicio (antes de cargar la data).    logPostFinal  Log de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).    matrixrep1  Estructura matricial \u00fatil para reportes en Google Data Studio.    contagrupo1  Resumen contable por a\u00f1o y mes.    contagrupo2  Resumen contable por fecha.    eqtabla  Tabla para agregar equivalencias.", 
            "title": "Tablas B.I."
        }, 
        {
            "location": "/microsip/#contacto", 
            "text": "\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?  Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/aspel/", 
            "text": "Aspel SAE\n\n\nAspel SAE es un sistema administrativo que fabrica la empresa \nAspel de M\u00e9xico, S.A. de C.V.\n\n\nFactor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.\n\n\nEn nuestro \nrepositorio de GitHub\n podr\u00e1s encontrar estos objetos bajo la licencia \nGNU GPLv3\n GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.\n\n\n\n\nCasos de Uso\n\n\nFolleto Casos de Uso: \nTableros de Mando \n Presupuestos\n\n\n\n\n\n\nTablas Aspel\n\n\n\n\nAl configurar Bipost Sync con \nSystem: SAE\n, de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:\n\n\nalmacenes\nclie\nclin\ncolor\ncompc\ncompd\ncompo\nconc\nconm\nconp\ncuen_det\ncuen_m\nfactd\nfactf\nfactp\ninve\nminve\nmoned\npaga_det\npaga_m\npar_compc\npar_compd\npar_compo\npar_factd\npar_factf\npar_factp\nprov\nprvprod\ntalla\nvend\n\n\n\nEn la base de datos del sistema Aspel SAE cada tabla tiene un sufijo \n01\n, \n02\n, etc., de acuerdo a la empresa que se ha creado. Por ejemplo la tabla de las facturas podr\u00edas encontrarla como \nfactf01\n y para otra raz\u00f3n social como \nfactf02\n.\n\n\nBipost Sync autom\u00e1ticamente elimina estos sufijos por lo que las tablas en Aurora-MySQL ser\u00e1n creadas sin el sufijo. Esta funcionalidad aplica incluso cuando se a\u00f1aden tablas con \ncustomData.json\n.\n\n\nEn el siguiente link est\u00e1 el listado de tablas de Aspel SAE.\n\n\nTablas Aspel SAE, archivo Google Sheets.\n\n\nNOTA:\n La lista del link anterior puede no estar completa.\n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nA\u00f1adir tablas a la Sincronizaci\u00f3n\n\n\nPara incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo \ncustomData.json\n, por ejemplo:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"CLIE_CLIB\",\n    \"fields\": \"CVE_CLIE, CAMPLIB1, CAMPLIB2, CAMPLIB3, CAMPLIB4, CAMPLIB5, CAMPLIB6\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"AFACT\",\n    \"fields\": \"* \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\nNotar que no se incluye el sufijo \n01\n, \n02\n, etc. de las tablas.\n\n\nPara m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver \naqu\u00ed.\n\n\n\n\nTablas B.I.\n\n\nAl correr los scripts del \nrepositorio de GitHub,\n se crean las siguientes tablas.\n\n\n\n\n\n\n\n\nTabla\n\n\nDescripci\u00f3n\n\n\n\n\n\n\n\n\n\n\nT\n\n\nTiraje de numeraci\u00f3n consecutiva.\n\n\n\n\n\n\ntime\n\n\nUtil para usarse con dimensiones de tiempo en el B.I.\n\n\n\n\n\n\nym\n\n\nUtil para usarse con dimensiones a\u00f1o y mes en el B.I.\n\n\n\n\n\n\nbiabc\n\n\nClasificaci\u00f3n ABC de los inventarios.\n\n\n\n\n\n\nbiblia\n\n\nMatriz de estad\u00edsticas de inventario por mes.\n\n\n\n\n\n\nmodulosgrupo1\n\n\nUtil para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.\n\n\n\n\n\n\nbusinessDay\n\n\nD\u00edas laborales de la empresa.\n\n\n\n\n\n\ncustomDates\n\n\nPasa una fecha espec\u00edfica a las funciones \nfnSyncDate()\n, \nfnAgingDate\n y \nfnServiceDate\n\n\n\n\n\n\ndateInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nymInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nsyncInfo\n\n\nContiene informaci\u00f3n de la sincronizaci\u00f3n.\n\n\n\n\n\n\nlogPostInitial\n\n\nLog de sincronizaciones, se ejecuta al inicio (antes de cargar la data).\n\n\n\n\n\n\nlogPostFinal\n\n\nLog de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).\n\n\n\n\n\n\nmatrixrep1\n\n\nEstructura matricial \u00fatil para reportes en Google Data Studio.\n\n\n\n\n\n\ncontagrupo1\n\n\nResumen contable por a\u00f1o y mes.\n\n\n\n\n\n\ncontagrupo2\n\n\nResumen contable por fecha.\n\n\n\n\n\n\neqtabla\n\n\nTabla para agregar equivalencias.\n\n\n\n\n\n\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?\n\n\nEscr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Aspel"
        }, 
        {
            "location": "/aspel/#aspel-sae", 
            "text": "Aspel SAE es un sistema administrativo que fabrica la empresa  Aspel de M\u00e9xico, S.A. de C.V.  Factor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.  En nuestro  repositorio de GitHub  podr\u00e1s encontrar estos objetos bajo la licencia  GNU GPLv3  GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.", 
            "title": "Aspel SAE"
        }, 
        {
            "location": "/aspel/#casos-de-uso", 
            "text": "Folleto Casos de Uso:  Tableros de Mando   Presupuestos", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/aspel/#tablas-aspel", 
            "text": "Al configurar Bipost Sync con  System: SAE , de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:  almacenes\nclie\nclin\ncolor\ncompc\ncompd\ncompo\nconc\nconm\nconp\ncuen_det\ncuen_m\nfactd\nfactf\nfactp\ninve\nminve\nmoned\npaga_det\npaga_m\npar_compc\npar_compd\npar_compo\npar_factd\npar_factf\npar_factp\nprov\nprvprod\ntalla\nvend  En la base de datos del sistema Aspel SAE cada tabla tiene un sufijo  01 ,  02 , etc., de acuerdo a la empresa que se ha creado. Por ejemplo la tabla de las facturas podr\u00edas encontrarla como  factf01  y para otra raz\u00f3n social como  factf02 .  Bipost Sync autom\u00e1ticamente elimina estos sufijos por lo que las tablas en Aurora-MySQL ser\u00e1n creadas sin el sufijo. Esta funcionalidad aplica incluso cuando se a\u00f1aden tablas con  customData.json .  En el siguiente link est\u00e1 el listado de tablas de Aspel SAE.  Tablas Aspel SAE, archivo Google Sheets.  NOTA:  La lista del link anterior puede no estar completa.  Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Tablas Aspel"
        }, 
        {
            "location": "/aspel/#anadir-tablas-a-la-sincronizacion", 
            "text": "Para incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo  customData.json , por ejemplo:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"CLIE_CLIB\",\n    \"fields\": \"CVE_CLIE, CAMPLIB1, CAMPLIB2, CAMPLIB3, CAMPLIB4, CAMPLIB5, CAMPLIB6\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"AFACT\",\n    \"fields\": \"* \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]  Notar que no se incluye el sufijo  01 ,  02 , etc. de las tablas.  Para m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver  aqu\u00ed.", 
            "title": "A\u00f1adir tablas a la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/aspel/#tablas-bi", 
            "text": "Al correr los scripts del  repositorio de GitHub,  se crean las siguientes tablas.     Tabla  Descripci\u00f3n      T  Tiraje de numeraci\u00f3n consecutiva.    time  Util para usarse con dimensiones de tiempo en el B.I.    ym  Util para usarse con dimensiones a\u00f1o y mes en el B.I.    biabc  Clasificaci\u00f3n ABC de los inventarios.    biblia  Matriz de estad\u00edsticas de inventario por mes.    modulosgrupo1  Util para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.    businessDay  D\u00edas laborales de la empresa.    customDates  Pasa una fecha espec\u00edfica a las funciones  fnSyncDate() ,  fnAgingDate  y  fnServiceDate    dateInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    ymInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    syncInfo  Contiene informaci\u00f3n de la sincronizaci\u00f3n.    logPostInitial  Log de sincronizaciones, se ejecuta al inicio (antes de cargar la data).    logPostFinal  Log de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).    matrixrep1  Estructura matricial \u00fatil para reportes en Google Data Studio.    contagrupo1  Resumen contable por a\u00f1o y mes.    contagrupo2  Resumen contable por fecha.    eqtabla  Tabla para agregar equivalencias.", 
            "title": "Tablas B.I."
        }, 
        {
            "location": "/aspel/#contacto", 
            "text": "\u00bfNecesitas ayuda? \u00bfBuscas una soluci\u00f3n a la medida?  Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/about/", 
            "text": "History\n\n\nBipost Sync was once designed to ease the use of \nCloud Business Intelligence\n tools.\n\n\nOver time we started using Bipost Sync to seamless connect on-premises ERP's to AWS cloud.\n\n\n\n\nEnterprise Applications\n\n\nWe build tailored made \nserverless\n enterprise web applications connected to on-premises Relational Database Systems (RDBMS) using Bipost Sync.\n\n\n\n\n\n\nContact Us\n\n\nWe are always happy to hear about you.\n\n\nPlease send us an email to: \ninfo@factorbi.com", 
            "title": "About"
        }, 
        {
            "location": "/about/#history", 
            "text": "Bipost Sync was once designed to ease the use of  Cloud Business Intelligence  tools.  Over time we started using Bipost Sync to seamless connect on-premises ERP's to AWS cloud.", 
            "title": "History"
        }, 
        {
            "location": "/about/#enterprise-applications", 
            "text": "We build tailored made  serverless  enterprise web applications connected to on-premises Relational Database Systems (RDBMS) using Bipost Sync.", 
            "title": "Enterprise Applications"
        }, 
        {
            "location": "/about/#contact-us", 
            "text": "We are always happy to hear about you.  Please send us an email to:  info@factorbi.com", 
            "title": "Contact Us"
        }
    ]
}