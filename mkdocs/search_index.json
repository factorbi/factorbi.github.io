{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nBipost is a simple database synchronization tool built for developers in mind. \n\n\nCurrently it can synchronize \nMicrosoft SQL Server\u00ae\n and \nFirebird SQL\n (running on Windows) to any given \nAmazon Aurora MySQL\n database.\n\n\nIt is created to keep your databases on-premises while providing a way to asynchronous extract and load specific sets of data to AWS Aurora.\n\n\nIt uses a similar \nExtract, Transform \n Load (ETL)\n technique with recurring syncing.\n\n\n\n\nHow it works\n\n\n\n\nOn every sync biPost.exe reads table schema's and data, creates a compressed file and uploads it to S3.\n\n\nDestination bucket triggers an API which reads schema's, creates database and tables (if they don't exist) and loads the data.\n\n\nOn every upload MySQL uses a \nREPLACE\n command so if you set the appropriate primary keys on source database, data would be updated in destination (and avoid duplicates).\n\n\nYou can customize query criteria by using \ncustomData.json\n file.\n\n\nbiPost.exe can run manually or automatically with a scheduled \nWindows Task.\n\n\n\n\nPrivate Cloud\n\n\nBipost API allows to load data to private AWS accounts.\n\n\nEach RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.\n\n\nRoad Map\n\n\nWe are currently developing a secure web page to automate all settings that are exchanged over email. Thank you for your patience and we look forward to provide a world-class service.\n\n\nRelease Notes\n\n\n0.4.0\n\n\n\n\nCustom connections added.\n\n\nInitial statement\n added to API.\n\n\nSpecial characters are deleted on string columns of 100 characters length or up.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Bipost is a simple database synchronization tool built for developers in mind.   Currently it can synchronize  Microsoft SQL Server\u00ae  and  Firebird SQL  (running on Windows) to any given  Amazon Aurora MySQL  database.  It is created to keep your databases on-premises while providing a way to asynchronous extract and load specific sets of data to AWS Aurora.  It uses a similar  Extract, Transform   Load (ETL)  technique with recurring syncing.", 
            "title": "Overview"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "On every sync biPost.exe reads table schema's and data, creates a compressed file and uploads it to S3.  Destination bucket triggers an API which reads schema's, creates database and tables (if they don't exist) and loads the data.  On every upload MySQL uses a  REPLACE  command so if you set the appropriate primary keys on source database, data would be updated in destination (and avoid duplicates).  You can customize query criteria by using  customData.json  file.  biPost.exe can run manually or automatically with a scheduled  Windows Task.", 
            "title": "How it works"
        }, 
        {
            "location": "/#private-cloud", 
            "text": "Bipost API allows to load data to private AWS accounts.  Each RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.", 
            "title": "Private Cloud"
        }, 
        {
            "location": "/#road-map", 
            "text": "We are currently developing a secure web page to automate all settings that are exchanged over email. Thank you for your patience and we look forward to provide a world-class service.", 
            "title": "Road Map"
        }, 
        {
            "location": "/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/#040", 
            "text": "Custom connections added.  Initial statement  added to API.  Special characters are deleted on string columns of 100 characters length or up.", 
            "title": "0.4.0"
        }, 
        {
            "location": "/setupaws/", 
            "text": "Link your AWS Account\n\n\nFollow these instructions to link your AWS Account and RDS Aurora instance to Bipost API.\n\n\nIMPORTANT NOTICE: Many settings suggested here are for testing purposes. If you are to use the following AWS services for production you may want to follow your company policies and understand how to use AWS security according to your needs.\n\n\n\n\nDon't have an AWS account?\n\n\n\n\n\n\nCreate an AWS account here \naws.amazon.com\n\n\n\n\n\n\n\n\nAWS usually makes an automated verification phone call, we suggest to provide a land line.\n\n\n\n\nProvide payment information.\n\n\nSelect Basic Support (free plan).\n\n\nCheck if you can open \nRDS Dashboard\n, by searching under \nAWS services\n.\n\n\nCongrats you have an AWS account!\n\n\n\n\n\n\nCheck closest AWS Region to you location\n\n\ncloudping.info\n\n\nClick the link above and hit \nHTTP Ping\n and look for the lowest latency.\n\n\nMaybe you want to try this at different times of the day.\n\n\nTake note of the closest region.\n\n\n\n\n\n\nGet Canonical User ID from your IAM Home\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nAccount Identifiers.\n\n\n\n\nCopy AWS Account ID and Canonical User ID.\n\n\n\n\nCopy the entire string that starts with \nAWS Account ID\n up to the very end of a long hex string of 64 characters.\n\n\n\n\n\n\n\n\n\n\nEmail the above to \njaime@factorbi.com\n so we can configure a dedicated bucket for your API calls.\n\n\n\n\n\n\nQ: Is it secure to provide these numbers?\n\n\nYes, we use your \nCanonical User ID\n to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.\n\n\n\n\nCreate Aurora Instance\n\n\nAurora DB Details\n\n\n\n\nFrom AWS Console Home, upper right corner (next to you name) be sure to select the \nclosest region to your location.\n\n\nFrom AWS Console Home, search RDS.\n\n\nFrom \nRDS Dashboard\n, click Instances.\n\n\nLaunch DB Instance, blue button.\n\n\nSelect Engine: \nAmazon Aurora\n, click select.\n\n\nDB Instance Class: for testing purposes select the smallest available, currently \nt2.small\n\n\nMulti-AZ Deployment: for testing purposes select \nNo\n\n\nDB Instance Identifier: assign a name, lower-case and no special characters.\n\n\nMaster Username: \nroot\n\n\nMaster Password: assign a hard password and store it in a secure place.\n\n\nConfirm Password.\n\n\nOn your left pane it is displayed an estimated monthly cost. For further information check On-Demand Pricing: \nRDS Pricing\n\n\nClick Next Step, blue button.\n\n\n\n\n\n\nAurora Network \n Security\n\n\n\n\nVPC: \nCreate new VPC\n\n\nSubnet Group: \nCreate new DB Subnet Group\n\n\nPublicly Accessible: \nYes\n\n\nAvailability Zone: \nNo Preference\n\n\nVPC Security Group: \nCreate new Security Group\n\n\n\n\n\n\nAurora Database Options\n\n\n\n\nDB Cluster Identifier: \nleave blank\n\n\nDatabase Name: \nleave blank\n\n\nDatabase Port: \n3306\n\n\nDB Parameter Group: \ndefault.aurora5.6\n\n\nDB Cluster Parameter Group: \ndefault.aurora5.6\n\n\nOption Group: \nleave default\n\n\nEnable Encryption: \nNo\n\n\n\n\nAurora Failover\n\n\n\n\nPriority: \ntier-0\n\n\n\n\nAurora Backup\n\n\n\n\nBackup Retention Period: \n1 day\n\n\n\n\nAurora Monitoring\n\n\n\n\nEnable Enhanced Monitoring: \nNo\n\n\n\n\nAurora Maintenance\n\n\n\n\nAuto Minor Version Upgrade: \nYes\n\n\nMaintenance Windows: \nNo Preference\n\n\n\n\n\n\nLaunch DB Instance\n\n\n\n\n\n\nClick Launch DB Instance blue button.\n\n\n\n\n\n\nThis process may take a while, sometimes 30 minutes or more.\n\n\n\n\n\n\nYou can check \nStatus\n of your instance by going to \nInstances\n on left navigation pane.\n\n\n\n\n\n\n\n\nRDS Instance Security Group\n\n\nOnce the instance has \nStatus:\n \navailable\n proceed:\n\n\n\n\nClick check box way left of your DB Instance name.\n\n\nClick \nInstance Actions \\ See Details\n gray button, on top.\n\n\n\n\nLookup for \nSecurity Groups\n and click the blue string to the right, it may appear as \n\n\n\n\ndefault (sg-XXXXXXXX)\n\n\n\n\n\n\n\n\n\n\nYou are now on EC2 Dashboard and Security Group ID is already selected.\n\n\n\n\nClick \nActions \\ Edit inbound rules\n\n\nClick \nAdd Rule\n, under Type select \nMYSQL/Aurora\n\n\nSource \nCustom\n and type value: \n0.0.0.0/0\n\n\n\n\nRepeat steps 6 \n 7, and type value \n::/0\n\n\n\n\n\n\n\n\nClick \nSave\n blue button.\n\n\n\n\nClick \nActions \\ Edit outbound rules\n\n\nVerify if Type: \nAll traffic\n, Destination: \nCustom\n and value: \n0.0.0.0/0\n is already set, if not, add the rule. \n\n\nGo back to \nRDS Dashboard\n, select your instance, click \nInstance Actions \\ Reboot\n, confirm with blue button on the right.\n\n\nWait until \nStatus\n is \navailable\n and check if \nSecurity Groups\n are \n( active )\n\n\n\n\n\n\nCreate IAM Policy to Grant Access to S3\n\n\nFrom this point on you need the newly S3 bucket ARN that we provided over email. \n\n\nIf you haven't emailed us with your Canonical User ID, please follow \nthese steps.\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nPolicies.\n\n\nCreate policy\n blue button.\n\n\n\n\nSelect \nPolicy Generator\n\n\n\n\nEffect: \nAllow\n\n\nAWS Service: \nAmazon S3\n\n\nActions: check \nGetObject\n and \nGetObjectVersion\n\n\nAmazon Resource Name (ARN): \narn you received over email\n, example: \n\n\narn:aws:s3:::bipost-000111222\n\n\n\n\n\n\n\n\nClick \nAdd Statement\n\n\n\n\n\n\nRepeat step 4 adding \n/*\n at the end of ARN bucket string, as follows:\n\n\n\n\nEffect: \nAllow\n\n\nAWS Service: \nAmazon S3\n\n\nActions: check \nGetObject\n and \nGetObjectVersion\n\n\nAmazon Resource Name (ARN): example: \narn:aws:s3:::bipost-000111222/*\n\n\n\n\n\n\n\n\n\n\nClick \nNext Step\n blue button.\n\n\n\n\nPolicy Name:\n \nAllowAuroraToS3\n\n\nOptionally add \nDescription\n.\n\n\n\n\nPolicy Document\n: double check that JSON looks like this:\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt9999999999999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222\"\n            ]\n        },\n        {\n            \"Sid\": \"Stmt9999999777999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222/*\"\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\n\nClick \nCreate Policy\n\n\n\n\n\n\nFurther information from AWS go to: \nAllowing Amazon Aurora to Access Amazon S3 Resources\n\n\n\n\nCreate IAM Role to Allow RDS Access to S3\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nRoles.\n\n\nCreate New Role\n blue button.\n\n\n\n\nChoose \nAWS Service Role,\n scroll down and select \nAmazon RDS\n\n\n\n\n\n\n\n\nAttach Policy,\n leave blank and click \nNext Step\n blue button.\n\n\n\n\nRole name: \nRDSLoadFromS3\n\n\nClick \nCreate role\n blue button.\n\n\nClick on your newly created role. This will open a Summary.\n\n\n\n\nUnder Permissions, click \nAttach Policy\n blue button.\n\n\n\n\n\n\n\n\nUse Filter and select \nPolicy Type: Customer Managed\n\n\n\n\nClick the check box of your newly created Policy: \nAllowAuroraToS3\n\n\nClick \nAttach Policy\n blue button.\n\n\nCopy \nRole ARN\n string and save it for further use. It may look like this: \narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\nFurther information from AWS go to: \nCreating an IAM Role to Allow Amazon Aurora to Access AWS Services\n\n\n\n\nSet IAM Role to Aurora Cluster\n\n\n\n\nOpen \nRDS console.\n\n\nChoose \nClusters\n on left pane.\n\n\nClick check box of your newly cluster.\n\n\nClick \nManage IAM Roles\n gray button, on top.\n\n\nSelect the role you just created: \nRDSLoadFromS3\n and click \nDone\n, blue button.\n\n\n\n\n\n\nCreate Cluster Parameter Group\n\n\nIf you are already using a custom DB Cluster Parameter Group, you can select that group instead of creating a new DB Cluster Parameter Group.\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate Parameter Group\n blue button on top.\n\n\n\n\nParameter Group Family: \naurora5.6\n\n\nType: \nDB Cluster Parameter Group\n\n\nGroup Name: \nAuroraClusterAllowAWSAccess\n\n\nDescription: \nAllow cluster access to Amazon S3\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button.\n\n\n\n\nClick check box on your new \nauroraclusterallowawsaccess\n parameter group and click \nEdit Parameters\n gray button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\nExample\n\n\n\n\n\n\n\n\n\n\naurora_load_from_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naurora_select_into_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n blue button.\n\n\n\n\n\n\nFurther information from AWS go to: \nAssociating an IAM Role with a DB Cluster\n\n\n\n\nCreate DB Parameter Group\n\n\nIf you are already using a custom DB Parameter Group, you can select that group instead of creating a new DB Parameter Group.\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate Parameter Group\n blue button on top.\n\n\n\n\nParameter Group Family: \naurora5.6\n\n\nType: \nDB Parameter Group\n\n\nGroup Name: \nAuroraInstanceAllowAWSAccess\n\n\nDescription: \nAllow instance access to Amazon S3\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button.\n\n\n\n\nClick check box on your new \naurorainstanceallowawsaccess\n parameter group and click \nEdit Parameters\n gray button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\n\n\n\n\n\n\n\n\nlog_bin_trust_function_creators\n\n\n1\n\n\n\n\n\n\nmax_allowed_packet\n\n\n1073741824\n\n\n\n\n\n\nmax_connections\n\n\n16000\n\n\n\n\n\n\nmax_user_connections\n\n\n4294967295\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n blue button.\n\n\n\n\n\n\n\n\nSet Cluster Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nClusters.\n\n\nClick check box on your new cluster.\n\n\nClick \nModify Cluster\n gray button on top.\n\n\nUnder Database Options, set \nDB Cluster Parameter Group\n to \nauroraclusterallowawsaccess\n.\n\n\nClick check box \nApply Immediately\n and click \nContinue\n blue button.\n\n\nReview changes and click \nModify Cluster\n blue button.\n\n\n\n\nSet Instance Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick check box on your new instance.\n\n\nClick \nInstance Actions \\ Modify\n gray button on top.\n\n\nUnder Database Options, set \nDB Parameter Group\n to \naurorainstanceallowawsaccess\n\n\nYou may also notice that \nDB Cluster Parameter Group\n is set to \nauroraclusterallowawsaccess\n\n\nClick check box \nApply Immediately\n and click \nContinue\n blue button.\n\n\nReview changes and click \nModify DB Instance\n blue button.\n\n\nClick \nInstance Actions \\ Reboot\n gray button on top.\n\n\nConfirm reboot with blue button.\n\n\n\n\n\n\nVerify Instance Configuration\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick check box on your new instance.\n\n\nClick \nInstance Actions \\ See Details\n gray button on top.\n\n\n\n\nVerify the following:\n\n\n\n\nEnpoint: \n( authorized )\n\n\nParameter Group: \naurorainstanceallowawsaccess ( in-sync )\n\n\nDB Cluster Parameter Group: \nauroraclusterallowawsaccess ( in-sync )\n\n\nSecurity Groups: \ndefault (sg-XXXXXXXX) ( active )\n\n\nPublicly Accessible: \nYes\n\n\nDB Instance Status: \navailable\n\n\n\n\n\n\n\n\n\n\nTest connection to your RDS Aurora\n\n\n\n\n\n\nDownload and install any \nMySQL client\n of your preference: \n\n\nFor Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"\n\n\n\n\n\n\n\nOn your AWS Console go to \nRDS Dashboard\n, select your instance and copy the \nCluster Endpoint\n, which is a blue string with more than 60 characters. \n\n\n\n\n\n\nLaunch your MySQL client and configure a new connection:\n\n\n\n\nName:\n type any name of your preference.\n\n\nHost:\n Paste the Cluster Endpoint and delete the suffix :3306\n\n\nUsername:\n root\n\n\nPassword:\n type the Master Password\n\n\nPort:\n 3306\n\n\nDatabase:\n Leave blank\n\n\nConnect using SSL:\n No\n\n\n\n\n\n\n\n\nClick Connect and verify that you can successfully connect to your RDS instance.\n\n\n\n\n\n\n\n\nSend Instance Connection Details to Factor BI\n\n\nEmail all the information you used to \nTest connection to your RDS Aurora\n to \njaime@factorbi.com\n so we can add your instance to our Bipost API.\n\n\n\n\nSecurity of your RDS Instance for Production\n\n\nIf your are ready to use Bipost API for production, we highly recommend the following:\n\n\n\n\nUse \nMySQL client\n to create a new user.\n\n\nSet a strong password.\n\n\n\n\nSet the following Global Privileges:", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#link-your-aws-account", 
            "text": "Follow these instructions to link your AWS Account and RDS Aurora instance to Bipost API.  IMPORTANT NOTICE: Many settings suggested here are for testing purposes. If you are to use the following AWS services for production you may want to follow your company policies and understand how to use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#dont-have-an-aws-account", 
            "text": "Create an AWS account here  aws.amazon.com     AWS usually makes an automated verification phone call, we suggest to provide a land line.   Provide payment information.  Select Basic Support (free plan).  Check if you can open  RDS Dashboard , by searching under  AWS services .  Congrats you have an AWS account!", 
            "title": "Don't have an AWS account?"
        }, 
        {
            "location": "/setupaws/#check-closest-aws-region-to-you-location", 
            "text": "cloudping.info  Click the link above and hit  HTTP Ping  and look for the lowest latency.  Maybe you want to try this at different times of the day.  Take note of the closest region.", 
            "title": "Check closest AWS Region to you location"
        }, 
        {
            "location": "/setupaws/#get-canonical-user-id-from-your-iam-home", 
            "text": "Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Account Identifiers.   Copy AWS Account ID and Canonical User ID.   Copy the entire string that starts with  AWS Account ID  up to the very end of a long hex string of 64 characters.      Email the above to  jaime@factorbi.com  so we can configure a dedicated bucket for your API calls.    Q: Is it secure to provide these numbers?  Yes, we use your  Canonical User ID  to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.", 
            "title": "Get Canonical User ID from your IAM Home"
        }, 
        {
            "location": "/setupaws/#create-aurora-instance", 
            "text": "", 
            "title": "Create Aurora Instance"
        }, 
        {
            "location": "/setupaws/#aurora-db-details", 
            "text": "From AWS Console Home, upper right corner (next to you name) be sure to select the  closest region to your location.  From AWS Console Home, search RDS.  From  RDS Dashboard , click Instances.  Launch DB Instance, blue button.  Select Engine:  Amazon Aurora , click select.  DB Instance Class: for testing purposes select the smallest available, currently  t2.small  Multi-AZ Deployment: for testing purposes select  No  DB Instance Identifier: assign a name, lower-case and no special characters.  Master Username:  root  Master Password: assign a hard password and store it in a secure place.  Confirm Password.  On your left pane it is displayed an estimated monthly cost. For further information check On-Demand Pricing:  RDS Pricing  Click Next Step, blue button.", 
            "title": "Aurora DB Details"
        }, 
        {
            "location": "/setupaws/#aurora-network-security", 
            "text": "VPC:  Create new VPC  Subnet Group:  Create new DB Subnet Group  Publicly Accessible:  Yes  Availability Zone:  No Preference  VPC Security Group:  Create new Security Group", 
            "title": "Aurora Network &amp; Security"
        }, 
        {
            "location": "/setupaws/#aurora-database-options", 
            "text": "DB Cluster Identifier:  leave blank  Database Name:  leave blank  Database Port:  3306  DB Parameter Group:  default.aurora5.6  DB Cluster Parameter Group:  default.aurora5.6  Option Group:  leave default  Enable Encryption:  No", 
            "title": "Aurora Database Options"
        }, 
        {
            "location": "/setupaws/#aurora-failover", 
            "text": "Priority:  tier-0", 
            "title": "Aurora Failover"
        }, 
        {
            "location": "/setupaws/#aurora-backup", 
            "text": "Backup Retention Period:  1 day", 
            "title": "Aurora Backup"
        }, 
        {
            "location": "/setupaws/#aurora-monitoring", 
            "text": "Enable Enhanced Monitoring:  No", 
            "title": "Aurora Monitoring"
        }, 
        {
            "location": "/setupaws/#aurora-maintenance", 
            "text": "Auto Minor Version Upgrade:  Yes  Maintenance Windows:  No Preference", 
            "title": "Aurora Maintenance"
        }, 
        {
            "location": "/setupaws/#launch-db-instance", 
            "text": "Click Launch DB Instance blue button.    This process may take a while, sometimes 30 minutes or more.    You can check  Status  of your instance by going to  Instances  on left navigation pane.", 
            "title": "Launch DB Instance"
        }, 
        {
            "location": "/setupaws/#rds-instance-security-group", 
            "text": "Once the instance has  Status:   available  proceed:   Click check box way left of your DB Instance name.  Click  Instance Actions \\ See Details  gray button, on top.   Lookup for  Security Groups  and click the blue string to the right, it may appear as    default (sg-XXXXXXXX)      You are now on EC2 Dashboard and Security Group ID is already selected.   Click  Actions \\ Edit inbound rules  Click  Add Rule , under Type select  MYSQL/Aurora  Source  Custom  and type value:  0.0.0.0/0   Repeat steps 6   7, and type value  ::/0     Click  Save  blue button.   Click  Actions \\ Edit outbound rules  Verify if Type:  All traffic , Destination:  Custom  and value:  0.0.0.0/0  is already set, if not, add the rule.   Go back to  RDS Dashboard , select your instance, click  Instance Actions \\ Reboot , confirm with blue button on the right.  Wait until  Status  is  available  and check if  Security Groups  are  ( active )", 
            "title": "RDS Instance Security Group"
        }, 
        {
            "location": "/setupaws/#create-iam-policy-to-grant-access-to-s3", 
            "text": "From this point on you need the newly S3 bucket ARN that we provided over email.   If you haven't emailed us with your Canonical User ID, please follow  these steps.   Open  IAM Console.  In the left navigation pane choose  Policies.  Create policy  blue button.   Select  Policy Generator   Effect:  Allow  AWS Service:  Amazon S3  Actions: check  GetObject  and  GetObjectVersion  Amazon Resource Name (ARN):  arn you received over email , example:   arn:aws:s3:::bipost-000111222     Click  Add Statement    Repeat step 4 adding  /*  at the end of ARN bucket string, as follows:   Effect:  Allow  AWS Service:  Amazon S3  Actions: check  GetObject  and  GetObjectVersion  Amazon Resource Name (ARN): example:  arn:aws:s3:::bipost-000111222/*      Click  Next Step  blue button.   Policy Name:   AllowAuroraToS3  Optionally add  Description .   Policy Document : double check that JSON looks like this:  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt9999999999999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222\"\n            ]\n        },\n        {\n            \"Sid\": \"Stmt9999999777999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222/*\"\n            ]\n        }\n    ]\n}    Click  Create Policy    Further information from AWS go to:  Allowing Amazon Aurora to Access Amazon S3 Resources", 
            "title": "Create IAM Policy to Grant Access to S3"
        }, 
        {
            "location": "/setupaws/#create-iam-role-to-allow-rds-access-to-s3", 
            "text": "Open  IAM Console.  In the left navigation pane choose  Roles.  Create New Role  blue button.   Choose  AWS Service Role,  scroll down and select  Amazon RDS     Attach Policy,  leave blank and click  Next Step  blue button.   Role name:  RDSLoadFromS3  Click  Create role  blue button.  Click on your newly created role. This will open a Summary.   Under Permissions, click  Attach Policy  blue button.     Use Filter and select  Policy Type: Customer Managed   Click the check box of your newly created Policy:  AllowAuroraToS3  Click  Attach Policy  blue button.  Copy  Role ARN  string and save it for further use. It may look like this:  arn:aws:iam::123456789012:role/RDSLoadFromS3   Further information from AWS go to:  Creating an IAM Role to Allow Amazon Aurora to Access AWS Services", 
            "title": "Create IAM Role to Allow RDS Access to S3"
        }, 
        {
            "location": "/setupaws/#set-iam-role-to-aurora-cluster", 
            "text": "Open  RDS console.  Choose  Clusters  on left pane.  Click check box of your newly cluster.  Click  Manage IAM Roles  gray button, on top.  Select the role you just created:  RDSLoadFromS3  and click  Done , blue button.", 
            "title": "Set IAM Role to Aurora Cluster"
        }, 
        {
            "location": "/setupaws/#create-cluster-parameter-group", 
            "text": "If you are already using a custom DB Cluster Parameter Group, you can select that group instead of creating a new DB Cluster Parameter Group.   Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create Parameter Group  blue button on top.   Parameter Group Family:  aurora5.6  Type:  DB Cluster Parameter Group  Group Name:  AuroraClusterAllowAWSAccess  Description:  Allow cluster access to Amazon S3     Click  Create  blue button.   Click check box on your new  auroraclusterallowawsaccess  parameter group and click  Edit Parameters  gray button on top.   Set the following:     Name  Edit Values  Example      aurora_load_from_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aurora_select_into_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3       Click  Save Changes  blue button.    Further information from AWS go to:  Associating an IAM Role with a DB Cluster", 
            "title": "Create Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#create-db-parameter-group", 
            "text": "If you are already using a custom DB Parameter Group, you can select that group instead of creating a new DB Parameter Group.   Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create Parameter Group  blue button on top.   Parameter Group Family:  aurora5.6  Type:  DB Parameter Group  Group Name:  AuroraInstanceAllowAWSAccess  Description:  Allow instance access to Amazon S3     Click  Create  blue button.   Click check box on your new  aurorainstanceallowawsaccess  parameter group and click  Edit Parameters  gray button on top.   Set the following:     Name  Edit Values      log_bin_trust_function_creators  1    max_allowed_packet  1073741824    max_connections  16000    max_user_connections  4294967295       Click  Save Changes  blue button.", 
            "title": "Create DB Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-cluster-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Clusters.  Click check box on your new cluster.  Click  Modify Cluster  gray button on top.  Under Database Options, set  DB Cluster Parameter Group  to  auroraclusterallowawsaccess .  Click check box  Apply Immediately  and click  Continue  blue button.  Review changes and click  Modify Cluster  blue button.", 
            "title": "Set Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-instance-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click check box on your new instance.  Click  Instance Actions \\ Modify  gray button on top.  Under Database Options, set  DB Parameter Group  to  aurorainstanceallowawsaccess  You may also notice that  DB Cluster Parameter Group  is set to  auroraclusterallowawsaccess  Click check box  Apply Immediately  and click  Continue  blue button.  Review changes and click  Modify DB Instance  blue button.  Click  Instance Actions \\ Reboot  gray button on top.  Confirm reboot with blue button.", 
            "title": "Set Instance Parameter Group"
        }, 
        {
            "location": "/setupaws/#verify-instance-configuration", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click check box on your new instance.  Click  Instance Actions \\ See Details  gray button on top.   Verify the following:   Enpoint:  ( authorized )  Parameter Group:  aurorainstanceallowawsaccess ( in-sync )  DB Cluster Parameter Group:  auroraclusterallowawsaccess ( in-sync )  Security Groups:  default (sg-XXXXXXXX) ( active )  Publicly Accessible:  Yes  DB Instance Status:  available", 
            "title": "Verify Instance Configuration"
        }, 
        {
            "location": "/setupaws/#test-connection-to-your-rds-aurora", 
            "text": "Download and install any  MySQL client  of your preference:   For Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"    On your AWS Console go to  RDS Dashboard , select your instance and copy the  Cluster Endpoint , which is a blue string with more than 60 characters.     Launch your MySQL client and configure a new connection:   Name:  type any name of your preference.  Host:  Paste the Cluster Endpoint and delete the suffix :3306  Username:  root  Password:  type the Master Password  Port:  3306  Database:  Leave blank  Connect using SSL:  No     Click Connect and verify that you can successfully connect to your RDS instance.", 
            "title": "Test connection to your RDS Aurora"
        }, 
        {
            "location": "/setupaws/#send-instance-connection-details-to-factor-bi", 
            "text": "Email all the information you used to  Test connection to your RDS Aurora  to  jaime@factorbi.com  so we can add your instance to our Bipost API.", 
            "title": "Send Instance Connection Details to Factor BI"
        }, 
        {
            "location": "/setupaws/#security-of-your-rds-instance-for-production", 
            "text": "If your are ready to use Bipost API for production, we highly recommend the following:   Use  MySQL client  to create a new user.  Set a strong password.   Set the following Global Privileges:", 
            "title": "Security of your RDS Instance for Production"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nIf you haven't linked your RDS Instance to Bipost API, please follow these steps: \nLink your AWS account.\n\n\nPrerequisites\n\n\nFor most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.\n\n\nWindow XP \n Vista\n\n\nDownload and install: \n\n\n\n\nWindowsInstaller.\n\n\nPrerequisites for Windows 7 and 8.\n\n\n\n\nWindows 7 and 8\n\n\nDownload and install:\n\n\n\n\ndotNetFx40\n\n\nvcredist\n\n\n\n\nDownload \n Install\n\n\n\n\n\n\nGet latest version here.\n\n\n\n\n\n\nUnzip \nbiPost.zip\n to any folder on your Windows.\n\n\n\n\n\n\nCheck if you can successfully launch \nbiPost.exe\n\n\n\n\n\n\nConfigure biPost.exe", 
            "title": "Download & Install"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "If you haven't linked your RDS Instance to Bipost API, please follow these steps:  Link your AWS account.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#prerequisites", 
            "text": "For most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/installation/#window-xp-vista", 
            "text": "Download and install:    WindowsInstaller.  Prerequisites for Windows 7 and 8.", 
            "title": "Window XP &amp; Vista"
        }, 
        {
            "location": "/installation/#windows-7-and-8", 
            "text": "Download and install:   dotNetFx40  vcredist", 
            "title": "Windows 7 and 8"
        }, 
        {
            "location": "/installation/#download-install", 
            "text": "Get latest version here.    Unzip  biPost.zip  to any folder on your Windows.    Check if you can successfully launch  biPost.exe    Configure biPost.exe", 
            "title": "Download &amp; Install"
        }, 
        {
            "location": "/bipostexe/", 
            "text": "Configuration\n\n\nIMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat configuration settings as sensitive information.\n\n\nFrom this point on you need your \nService No.\n and \nActivation No.\n that we provided over email. \n\n\nIf you haven't emailed us with your Instance Connection Details, please follow \nthese steps.\n\n\n\n\nClick \nConfiguration\n and set:\n\n\n\n\nService No.:\n 36 digit hex number, it may look like this: \na1cfe23e-4de5-67c8-bc9a-c0123cfd4567\n\n\nActivation No.:\n 24 digit hex number, it may look like this: \n5990ce12c3ac45c6d78be90d\n\n\nEngine:\n Select \nFirebird\n or \nSQL\n (Microsoft SQL Server).\n\n\n\n\nSystem:\n Select \nCustom...\n \n\n\nIf you use \nIntelisis ERP\n, \nMicrosip ERP\n or \nAspel SAE\n then select the System according. \n\n\n\n\n\n\n\n\nFirebird Connection\n\n\n\n\n\n\nRemote Connection:\n Enable when you are using biPost.exe over a LAN network, this is, biPost.exe is not running on the same server as the Firebird server.\n\n\n\n\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nPassword:\n Set your Firebird password, sometimes \nmasterkey\n\n\n\n\n\n\nDatabase:\n Location of your \n.FDB\n file.\n\n\n\n\n\n\n\n\nSQL Connection\n\n\n\n\n\n\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nUser:\n Login for your SQL server. It only needs read permissions.\n\n\n\n\n\n\nPassword:\n Password for the Login provided.\n\n\n\n\n\n\nDatabase:\n Name of your database.\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\n\n\n\n\nSpecific Bucket:\n Enable to use your own AWS Account.\n\n\n\n\nEnter your \nBucket Name\n that we provided over email.\n\n\n\n\n\n\n\n\nInclude Catalogs:\n For use only with \nIntelisis ERP\n system.\n\n\nWhen enabled, the following catalogs are included on every post:\n\n\nAgente\nAlm\nArt\nCondicion\nCta\nCte\nEmpresa\nEmpresaCfg\nEmpresaCfg2\nEmpresaGral\nMon\nMonHist\nProv\nSucursal\nUEN\nVersion\n\n\n\n\n\n\n\nRecursive Sync\n\n\nCurrently supported only for SQL Server, when enabled it optimizes upload by extracting and uploading one day at a time for the given date range. \n\n\nIt is very useful to upload historic data.\n\n\nIt is used in combination with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\nWhen \nturned off\n it automatically sets start and end date to yesterday.\n\n\n\n\ncustomData.json\n\n\nThis file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data.\n\n\nAll parameters are supported for SQL Server. For Firebird SQL please review supported parameters \nhere.\n\n\nExample 1, using recursiveDateField:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  }\n]\n\n\n\nExample 2, for catalogs:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\nThe above statements will send all data for the specified tables, as \njoin\n and \nfilter\n are not in use.\n\n\nNote that you can use \n\"active\": \"\",\n\n\n\n\nExample 3, using special filter when no datetime is available:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym \",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) \",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\njoin\n and \nfilter\n can use any syntax supported on SQL Server.\n\n\n\n\nExample 4 with Microsip ERP:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"ATRIBUTO_ID, NOMBRE, NOMBRE_COLUMNA, CLAVE_OBJETO, POSICION, TIPO, LONGITUD, ESCALA, VALOR_MINIMO, VALOR_MAXIMO, VALOR_DEFAULT_NUMERICO, VALOR_DEFAULT_CARACTER, DESCRIPCION \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"IMPUESTO_ART_ID, ARTICULO_ID, IMPUESTO_ID, UNIDADES_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"DOCTO_CM_ID, IMPUESTO_ID, COMPRA_NETA, OTROS_IMPUESTOS, PCTJE_IMPUESTO, IMPORTE_IMPUESTO, UNIDADES_IMPUESTO, IMPORTE_UNITARIO_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CC_IMPTO_ID, IMPTE_DOCTO_CC_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CP_IMPTO_ID, IMPTE_DOCTO_CP_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\nIn the above example biPost.exe is using \nSystem: Microsip\n and on every sync it sends all the factory embedded tables plus the tables set in customData.json \n\n\n\n\nTables \n Primary Keys\n\n\nTake note of the following:\n\n\n\n\nEvery table specified in \ncustomData.json\n must have a \nPrimary Key\n and these must be listed on \n\"fields\"\n\n\nYou can only setup customData.json with \nbase tables.\n If you use views it may work but it is not supported.\n\n\n\"fields\"\n parameter must only include fields that exist on \n\"table\":\n.\n\n\n\n\nrecursiveDateField\n\n\nThis parameter is used when \nRecursive Sync\n check box is enabled.\n\n\nIt only supports date fields without hour.\n\n\nOn SQL Server using \ndatetime\n data type, it only supports dates ending in \n00:00:00.000\n\n\ncustomData.json supported parameters for Firebird SQL.\n\n\n\n\nactive\n\n\ntable\n\n\nfields\n\n\n\n\nRest of the parameters do not work for Firebird.\n\n\nHandle several customData.json settings\n\n\nIt is very common to make changes to customData.json to upload different sets of data.\n\n\nUse Cases:\n\n\n\n\n\n\nSome tables may be uploaded once since its data is rarely changed, e.g. config \n company tables.\n\n\n\n\n\n\nHistoric data may be uploaded once, e.g. transactions from previous years.\n\n\n\n\n\n\nRecently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.\n\n\n\n\n\n\nRecently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.\n\n\n\n\n\n\nFor all the reasons above, it may be very useful to create two or more folders and copy entire biPost files and just change \ncustomData.json.\n Moreover you may want to have different sync schedules, which are explained next.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of biPost.exe, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of biPost can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nHandling Different Settings with Folders\n\n\nWe already explained why creating two or more folders is a good workaround for \nhandling different customData.json settings.\n\n\nIt is also possible that you may want to sync more than one database. For this matter proceed creating several folders so each one has it's own settings.", 
            "title": "Configure & Run"
        }, 
        {
            "location": "/bipostexe/#configuration", 
            "text": "IMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat configuration settings as sensitive information.  From this point on you need your  Service No.  and  Activation No.  that we provided over email.   If you haven't emailed us with your Instance Connection Details, please follow  these steps.   Click  Configuration  and set:   Service No.:  36 digit hex number, it may look like this:  a1cfe23e-4de5-67c8-bc9a-c0123cfd4567  Activation No.:  24 digit hex number, it may look like this:  5990ce12c3ac45c6d78be90d  Engine:  Select  Firebird  or  SQL  (Microsoft SQL Server).   System:  Select  Custom...    If you use  Intelisis ERP ,  Microsip ERP  or  Aspel SAE  then select the System according.", 
            "title": "Configuration"
        }, 
        {
            "location": "/bipostexe/#firebird-connection", 
            "text": "Remote Connection:  Enable when you are using biPost.exe over a LAN network, this is, biPost.exe is not running on the same server as the Firebird server.    Server:  IP or name of the server on your LAN network.    Password:  Set your Firebird password, sometimes  masterkey    Database:  Location of your  .FDB  file.", 
            "title": "Firebird Connection"
        }, 
        {
            "location": "/bipostexe/#sql-connection", 
            "text": "Server:  IP or name of the server on your LAN network.    User:  Login for your SQL server. It only needs read permissions.    Password:  Password for the Login provided.    Database:  Name of your database.", 
            "title": "SQL Connection"
        }, 
        {
            "location": "/bipostexe/#general-settings", 
            "text": "Specific Bucket:  Enable to use your own AWS Account.   Enter your  Bucket Name  that we provided over email.     Include Catalogs:  For use only with  Intelisis ERP  system.  When enabled, the following catalogs are included on every post:  Agente\nAlm\nArt\nCondicion\nCta\nCte\nEmpresa\nEmpresaCfg\nEmpresaCfg2\nEmpresaGral\nMon\nMonHist\nProv\nSucursal\nUEN\nVersion", 
            "title": "General Settings"
        }, 
        {
            "location": "/bipostexe/#recursive-sync", 
            "text": "Currently supported only for SQL Server, when enabled it optimizes upload by extracting and uploading one day at a time for the given date range.   It is very useful to upload historic data.  It is used in combination with  customData.json  so you can configure the date field to use for each table.  When  turned off  it automatically sets start and end date to yesterday.", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/bipostexe/#customdatajson", 
            "text": "This file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data.  All parameters are supported for SQL Server. For Firebird SQL please review supported parameters  here.  Example 1, using recursiveDateField:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  }\n]  Example 2, for catalogs:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]   The above statements will send all data for the specified tables, as  join  and  filter  are not in use.  Note that you can use  \"active\": \"\",   Example 3, using special filter when no datetime is available:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym \",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) \",\n    \"recursiveDateField\": \"\"\n  }\n]   join  and  filter  can use any syntax supported on SQL Server.   Example 4 with Microsip ERP:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"ATRIBUTO_ID, NOMBRE, NOMBRE_COLUMNA, CLAVE_OBJETO, POSICION, TIPO, LONGITUD, ESCALA, VALOR_MINIMO, VALOR_MAXIMO, VALOR_DEFAULT_NUMERICO, VALOR_DEFAULT_CARACTER, DESCRIPCION \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"IMPUESTO_ART_ID, ARTICULO_ID, IMPUESTO_ID, UNIDADES_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"DOCTO_CM_ID, IMPUESTO_ID, COMPRA_NETA, OTROS_IMPUESTOS, PCTJE_IMPUESTO, IMPORTE_IMPUESTO, UNIDADES_IMPUESTO, IMPORTE_UNITARIO_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CC_IMPTO_ID, IMPTE_DOCTO_CC_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CP_IMPTO_ID, IMPTE_DOCTO_CP_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]   In the above example biPost.exe is using  System: Microsip  and on every sync it sends all the factory embedded tables plus the tables set in customData.json", 
            "title": "customData.json"
        }, 
        {
            "location": "/bipostexe/#tables-primary-keys", 
            "text": "Take note of the following:   Every table specified in  customData.json  must have a  Primary Key  and these must be listed on  \"fields\"  You can only setup customData.json with  base tables.  If you use views it may work but it is not supported.  \"fields\"  parameter must only include fields that exist on  \"table\": .", 
            "title": "Tables &amp; Primary Keys"
        }, 
        {
            "location": "/bipostexe/#recursivedatefield", 
            "text": "This parameter is used when  Recursive Sync  check box is enabled.  It only supports date fields without hour.  On SQL Server using  datetime  data type, it only supports dates ending in  00:00:00.000", 
            "title": "recursiveDateField"
        }, 
        {
            "location": "/bipostexe/#customdatajson-supported-parameters-for-firebird-sql", 
            "text": "active  table  fields   Rest of the parameters do not work for Firebird.", 
            "title": "customData.json supported parameters for Firebird SQL."
        }, 
        {
            "location": "/bipostexe/#handle-several-customdatajson-settings", 
            "text": "It is very common to make changes to customData.json to upload different sets of data.  Use Cases:    Some tables may be uploaded once since its data is rarely changed, e.g. config   company tables.    Historic data may be uploaded once, e.g. transactions from previous years.    Recently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.    Recently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.    For all the reasons above, it may be very useful to create two or more folders and copy entire biPost files and just change  customData.json.  Moreover you may want to have different sync schedules, which are explained next.", 
            "title": "Handle several customData.json settings"
        }, 
        {
            "location": "/bipostexe/#schedule", 
            "text": "If you want automated execution of biPost.exe, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/bipostexe/#check-for-updates", 
            "text": "New versions of biPost can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/bipostexe/#handling-different-settings-with-folders", 
            "text": "We already explained why creating two or more folders is a good workaround for  handling different customData.json settings.  It is also possible that you may want to sync more than one database. For this matter proceed creating several folders so each one has it's own settings.", 
            "title": "Handling Different Settings with Folders"
        }, 
        {
            "location": "/bipostapi/", 
            "text": "Bipost API\n\n\nBipost API runs on AWS with a Lambda function.\nIt has 4 main stages which are described next.\n\n\nCreate Schemas\n\n\n\n\n\n\nIf it doesn't exist, database is created with:\n\n\n\n\n\n\nName: Over email we ask for this name for every \nService No.\n provided.\n\n\n\n\n\n\nEncoding: \ncp1252 West European (latin1)\n\n\n\n\n\n\nCollation: \nlatin1_spanish_ci\n\n\n\n\n\n\n\n\n\n\nTables are created with the full set of fields found on source db. \n\n\n\n\n\n\nFields will appear on a different position as the source db.\n\n\n\n\n\n\nOnly the fields specified in \ncustomData.json\n will be populated.\n\n\n\n\n\n\n\n\nInitial Statement\n\n\nAfter the schemas are created (or checked if they exist) and \nbefore\n new data is loaded, you can specify to run a query on your database.\n\n\nThis is very useful if you need to delete, truncate or make any changes before data is loaded.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostInitial\n and must not have parameters.\n\n\nPlease let us know over email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  TRUNCATE TABLE `mytesttable`;\n\nEND$$\n\n\n\n\n\nLoad Data\n\n\nData loading is performed by Aurora using \nLOAD DATA FROM S3\n statement with \nREPLACE\n parameter. This technique ensures that all data is loaded with great performance.\n\n\nYou can verify which tables where loaded by querying the \naurora_s3_load_history\n table like this:\n\n\nselect * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';\n\n\n\nOptionally convert \nload_timestamp\n to your local time: \nCONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')\n\n\nAfter data is loaded, it always checks to create and populate these objects:\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\n\n\n\n\n\n\n\n\nT\n\n\nTable\n\n\n\n\n\n\ntime\n\n\nTable\n\n\n\n\n\n\nym\n\n\nTable\n\n\n\n\n\n\ndowhile\n\n\nStored Procedure\n\n\n\n\n\n\nspCreateTime\n\n\nStored Procedure\n\n\n\n\n\n\nspCreateYM\n\n\nStored Procedure\n\n\n\n\n\n\nsp_createIndex\n\n\nStored Procedure\n\n\n\n\n\n\n\n\n\n\nFinal Statement\n\n\nAfter new data is loaded to your database, you can specify to run a query.\n\n\nThis is very handy if you need to execute several routines and, for example, populate new tables.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostFinal\n and must not have parameters. \n\n\nPlease let us know over email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  INSERT INTO mytable (message) VALUES ('my_message');\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\nEND$$", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#bipost-api", 
            "text": "Bipost API runs on AWS with a Lambda function.\nIt has 4 main stages which are described next.", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#create-schemas", 
            "text": "If it doesn't exist, database is created with:    Name: Over email we ask for this name for every  Service No.  provided.    Encoding:  cp1252 West European (latin1)    Collation:  latin1_spanish_ci      Tables are created with the full set of fields found on source db.     Fields will appear on a different position as the source db.    Only the fields specified in  customData.json  will be populated.", 
            "title": "Create Schemas"
        }, 
        {
            "location": "/bipostapi/#initial-statement", 
            "text": "After the schemas are created (or checked if they exist) and  before  new data is loaded, you can specify to run a query on your database.  This is very useful if you need to delete, truncate or make any changes before data is loaded.  Currently you need to include all desired statements in a stored procedure with the name  spPostInitial  and must not have parameters.  Please let us know over email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  TRUNCATE TABLE `mytesttable`;\n\nEND$$", 
            "title": "Initial Statement"
        }, 
        {
            "location": "/bipostapi/#load-data", 
            "text": "Data loading is performed by Aurora using  LOAD DATA FROM S3  statement with  REPLACE  parameter. This technique ensures that all data is loaded with great performance.  You can verify which tables where loaded by querying the  aurora_s3_load_history  table like this:  select * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';  Optionally convert  load_timestamp  to your local time:  CONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')  After data is loaded, it always checks to create and populate these objects:     Name  Type      T  Table    time  Table    ym  Table    dowhile  Stored Procedure    spCreateTime  Stored Procedure    spCreateYM  Stored Procedure    sp_createIndex  Stored Procedure", 
            "title": "Load Data"
        }, 
        {
            "location": "/bipostapi/#final-statement", 
            "text": "After new data is loaded to your database, you can specify to run a query.  This is very handy if you need to execute several routines and, for example, populate new tables.  Currently you need to include all desired statements in a stored procedure with the name  spPostFinal  and must not have parameters.   Please let us know over email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  INSERT INTO mytable (message) VALUES ('my_message');\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\nEND$$", 
            "title": "Final Statement"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting\n\n\nWhen you use manual sync and it is completed successfully, the following dialog appears.\n\n\n\n\nIf you face any errors, try looking here.\n\n\n\n\nCheck Log\n\n\nAfter you make a sync, you are able to check on you PC the compressed file uploaded.\n\n\n\n\n\n\nOpen a new Windows Explorer window and enter \n%LocalAppData%\\biPost\n\n\n\n\n\n\nOpen corresponding ZIP file and review contents.\n\n\n\n\n\n\nIt might me also useful to check after a \nSync Completed\n process, and \nbefore\n you press \nOK\n, open \n%LocalAppData%\\biPost\n and check the files that were just created.\n\n\nIf you need to check which files where loaded to Aurora, check \naurora_s3_load_history.\n\n\n\n\ncustomData.json syntax error\n\n\nMisspelled fields or tables on customData.json may appear as:\n\n\n\n\n\n\nDate format inside post.json\n\n\nIf immediately after launching biPost.exe this error appears \nString was not recognized as valid DateTime\n, try the following:\n\n\nOpen \npost.json\n file and delete all datetime strings found inside \n\" \"\n. Do not delete the double quotation marks.\n\n\n\n\n\n\nNo information to Sync\n\n\nIf \nNo information to Sync\n message appears, verify that customData.json is set to send at least one table.\n\n\n\n\nTime to sync and load to Aurora\n\n\nWhen a new Sync is initiated on biPost.exe, it may take a few minutes to extract, compress and upload to information to S3. While this is happening, no messages/icons will show that biPost.exe is working and maybe you will see \n(Not responding)\n on the top of the window, this is normal.\n\n\nIf you launch Windows Task Manager probably you'll see that \nbiPost.exe *32\n is running and consuming a considerable amount of CPU.\n\n\nOnce the compressed package it uploaded to S3, it may take 1 to 3 minutes to process and load your data to MySQL.\n\n\nIf you need to check which tables where loaded, check \naurora_s3_load_history.\n\n\n\n\nUpload Limit\n\n\nDepending on the number of columns on each table and the amount of data of each row, it is possible that a large amount of data sent on a single sync may not load on your Aurora DB.\n\n\nWe have tested different scenarios and a 100,000 row limit for a single sync works fine.\n\n\nWe recommend using \nRecursive Sync\n for large tables that have a datetime field available.\n\n\n\n\nSpecial Characters\n\n\nSome special characters on \nchar\n and \nvarchar\n fields are not supported and thus removed by biPost.exe\nFor example:\n\n\n\n\nEnter\n\n\n()\n\n\n\n\nOn \nSQL Server\n, all special characters on strings of 100 length or more are removed, leaving only letters and numbers.\n\n\n\n\nSchema Limitations\n\n\n\n\nNULL\n values on char and varchar are converted to \n''\n on MySQL.\n\n\nNULL\n values on float, money and int datatypes are converted to value \n0\n (zero) on MySQL.\n\n\nNULL\n values on datetime are converted to \n0000-00-00 00:00:00\n on MySQL.\n\n\nbit\n datatype is converted to \nVARCHAR(1)\n on MySQL.\n\n\nTables without a \nPRIMARY KEY\n will not load data on Aurora.\n\n\n\n\n\n\nNo Internet Connection\n\n\nIf there is no internet connection available, biPost will show the following message:\n\n\n\n\n\n\nFirewall Restrictions\n\n\nIf your internet connection has a firewall, it may show different errors like: \n\n\n\n\nThe remote name could not be resolved.\n\n\nA WebException with status SendFailure was thrown.\n\n\nA WebException with status NameResolutionFailure was thrown.\n\n\nError making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.\n\n\n\n\n\n\nGrant Firewall to reach Amazon S3:\n\n\nCreate a policy to Allow to:\n\n\n\n\n54.230.0.0/15\n\n\n52.192.0.0/11\n\n\n\n\n\n\n\n\nAppData\\Local folder\n\n\nSometimes it is necessary to manually delete the content of \n\\AppData\\Local\\biPost\n folder.\n\n\nOpen a new Windows Explorer window and enter \n%LocalAppData%\\biPost\n. Select all and delete.\n\n\n************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "When you use manual sync and it is completed successfully, the following dialog appears.   If you face any errors, try looking here.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#check-log", 
            "text": "After you make a sync, you are able to check on you PC the compressed file uploaded.    Open a new Windows Explorer window and enter  %LocalAppData%\\biPost    Open corresponding ZIP file and review contents.    It might me also useful to check after a  Sync Completed  process, and  before  you press  OK , open  %LocalAppData%\\biPost  and check the files that were just created.  If you need to check which files where loaded to Aurora, check  aurora_s3_load_history.", 
            "title": "Check Log"
        }, 
        {
            "location": "/troubleshooting/#customdatajson-syntax-error", 
            "text": "Misspelled fields or tables on customData.json may appear as:", 
            "title": "customData.json syntax error"
        }, 
        {
            "location": "/troubleshooting/#date-format-inside-postjson", 
            "text": "If immediately after launching biPost.exe this error appears  String was not recognized as valid DateTime , try the following:  Open  post.json  file and delete all datetime strings found inside  \" \" . Do not delete the double quotation marks.", 
            "title": "Date format inside post.json"
        }, 
        {
            "location": "/troubleshooting/#no-information-to-sync", 
            "text": "If  No information to Sync  message appears, verify that customData.json is set to send at least one table.", 
            "title": "No information to Sync"
        }, 
        {
            "location": "/troubleshooting/#time-to-sync-and-load-to-aurora", 
            "text": "When a new Sync is initiated on biPost.exe, it may take a few minutes to extract, compress and upload to information to S3. While this is happening, no messages/icons will show that biPost.exe is working and maybe you will see  (Not responding)  on the top of the window, this is normal.  If you launch Windows Task Manager probably you'll see that  biPost.exe *32  is running and consuming a considerable amount of CPU.  Once the compressed package it uploaded to S3, it may take 1 to 3 minutes to process and load your data to MySQL.  If you need to check which tables where loaded, check  aurora_s3_load_history.", 
            "title": "Time to sync and load to Aurora"
        }, 
        {
            "location": "/troubleshooting/#upload-limit", 
            "text": "Depending on the number of columns on each table and the amount of data of each row, it is possible that a large amount of data sent on a single sync may not load on your Aurora DB.  We have tested different scenarios and a 100,000 row limit for a single sync works fine.  We recommend using  Recursive Sync  for large tables that have a datetime field available.", 
            "title": "Upload Limit"
        }, 
        {
            "location": "/troubleshooting/#special-characters", 
            "text": "Some special characters on  char  and  varchar  fields are not supported and thus removed by biPost.exe\nFor example:   Enter  ()   On  SQL Server , all special characters on strings of 100 length or more are removed, leaving only letters and numbers.", 
            "title": "Special Characters"
        }, 
        {
            "location": "/troubleshooting/#schema-limitations", 
            "text": "NULL  values on char and varchar are converted to  ''  on MySQL.  NULL  values on float, money and int datatypes are converted to value  0  (zero) on MySQL.  NULL  values on datetime are converted to  0000-00-00 00:00:00  on MySQL.  bit  datatype is converted to  VARCHAR(1)  on MySQL.  Tables without a  PRIMARY KEY  will not load data on Aurora.", 
            "title": "Schema Limitations"
        }, 
        {
            "location": "/troubleshooting/#no-internet-connection", 
            "text": "If there is no internet connection available, biPost will show the following message:", 
            "title": "No Internet Connection"
        }, 
        {
            "location": "/troubleshooting/#firewall-restrictions", 
            "text": "If your internet connection has a firewall, it may show different errors like:    The remote name could not be resolved.  A WebException with status SendFailure was thrown.  A WebException with status NameResolutionFailure was thrown.  Error making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.", 
            "title": "Firewall Restrictions"
        }, 
        {
            "location": "/troubleshooting/#grant-firewall-to-reach-amazon-s3", 
            "text": "Create a policy to Allow to:   54.230.0.0/15  52.192.0.0/11", 
            "title": "Grant Firewall to reach Amazon S3:"
        }, 
        {
            "location": "/troubleshooting/#appdatalocal-folder", 
            "text": "Sometimes it is necessary to manually delete the content of  \\AppData\\Local\\biPost  folder.  Open a new Windows Explorer window and enter  %LocalAppData%\\biPost . Select all and delete.  ************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.", 
            "title": "AppData\\Local folder"
        }, 
        {
            "location": "/about/", 
            "text": "History\n\n\nBipost was once designed to ease the use of Cloud Business Intelligence tools such as \nGoogle Data Studio\n and \nMicrosoft Power BI.\n\n\nOver time, we started using Bipost as an extension of on-premises systems (like ERP's) to AWS cloud. Using MySQL Aurora and other AWS services, we build tailored made enterprise-grade web applications.\n\n\n\n\nEnterprise Grade Applications\n\n\nWe build tailored made enterprise web applications with integration to on-premises Relational Database Systems (RDBMS) using Bipost.\n\n\nWe use \nform.io\n with backend on MySQL-Aurora.\n\n\nWe also build \nServerless Applications\n using \nS3\n, \nAPI Gateway\n, \nLambda\n, \nAurora\n \n \nCognito\n.\n\n\n\n\n\n\nFeedback\n\n\nWe are always happy to hear about our users.\n\n\nPlease send us an email to: \njaime@factorbi.com", 
            "title": "About"
        }, 
        {
            "location": "/about/#history", 
            "text": "Bipost was once designed to ease the use of Cloud Business Intelligence tools such as  Google Data Studio  and  Microsoft Power BI.  Over time, we started using Bipost as an extension of on-premises systems (like ERP's) to AWS cloud. Using MySQL Aurora and other AWS services, we build tailored made enterprise-grade web applications.", 
            "title": "History"
        }, 
        {
            "location": "/about/#enterprise-grade-applications", 
            "text": "We build tailored made enterprise web applications with integration to on-premises Relational Database Systems (RDBMS) using Bipost.  We use  form.io  with backend on MySQL-Aurora.  We also build  Serverless Applications  using  S3 ,  API Gateway ,  Lambda ,  Aurora     Cognito .", 
            "title": "Enterprise Grade Applications"
        }, 
        {
            "location": "/about/#feedback", 
            "text": "We are always happy to hear about our users.  Please send us an email to:  jaime@factorbi.com", 
            "title": "Feedback"
        }
    ]
}