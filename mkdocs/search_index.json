{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nBipost is database synchronization \n ETL tool for continually moving data from on-premises to AWS Aurora MySQL and back forth.\n\n\nCreated to keep your Windows databases on-premises while providing a way to extract, load \n transform specific sets of data to AWS Aurora. Two-way synchronization lets you sync from AWS back to on-prem database. It is not intended to work as a replication engine or mirroring an entire database.\n\n\nSources:\n\n\n\n\nFirebird SQL\n\n\nMicrosoft SQL Server\n\n\nMySQL\n\n\ndBase\n\n\nVisual FoxPro (VFP)\n\n\nSybase SQL Anywhere\n\n\n\n\nDestination:\n\n\n\n\n\n\nAmazon Aurora MySQL\n\n\n\n\n\n\n\n\n\n\n\n\nDeployed on AWS to scale at demand. Software as a Service (SaaS).\n\n\n\n\n\n\nUse Cases\n\n\n\n\nConsolidate/merge information on AWS from across multiple locations and database engines, e.g. consolidate sales, inventory and financial information from several branches.\n\n\nBuild a Data Warehouse and make Analytics and Business Intelligence dashboards using \nGoogle Data Studio\n or \nAWS QuickSight\n, with native connection to Aurora-MySQL.\n\n\nMake forecasts and statistical analysis with AWS services.\n\n\nExtend your on-prem ERP \n POS to AWS cloud and build web applications \n back-end services.\n\n\nPush data from AWS to on-prem systems.\n\n\n\n\n\n\nHow it works\n\n\n\n\nBipost Sync reads table schema's and data and uploads it to AWS.\n\n\nDatabase and tables are created/altered if they don't exist, and data is loaded to Aurora-MySQL.\n\n\nBefore and after data is loaded to Aurora-MySQL you can transform data with stored procedures.\n\n\nDon't worry about sending the same data multiple times. Data is replaced using primary keys, avoiding duplicates.\n\n\nRun manually or automatically with a Windows Task schedule.\n\n\nUpload big datasets using Recursive Sync.\n\n\n\n\n\n\nData is also available as CSV files on S3 so you can use other AWS services like \nAmazon Athena\n and \nAWS Glue\n to build your data lake.\n\n\n\n\nTwo-way synchronization\n\n\n\n\nSynchronize from Aurora-MySQL to on-premises SQL Server or Firebird SQL.\n\n\nCustomize data to download using \noutData.json\n\n\nInsert/update the returned data to your on-prem DB.\n\n\nTables schemas are created/altered on your on-prem DB if they don't exist.\n\n\nPrimary keys set on Aurora-MySQL are used against on-prem DB to avoid duplicates.\n\n\nBefore and after data is loaded to on-prem DB you can transform data with stored procedures.\n\n\nOnly available for SQL Server and Firebird.\n\n\nLearn more \nhere.\n\n\n\n\n\n\nPrivate Cloud\n\n\nWe care deeply about privacy.\n\n\nOur API links to your RDS instance on your AWS account, so you have full control and ownership of your databases.\n\n\nEach RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.\n\n\nArchitecture\n\n\n\n\nAurora\n is a MySQL compatible, fully managed database service built for the cloud with the performance and scalability of high-end commercial databases at 1/10th the cost.\n\n\n\n\nStart Using\n\n\n\n\n30 days free.\n\n\nUnlimited synchronizations.\n\n\nNo need to provide credit card information.\n\n\n\n\n--\n Start here\n\n\nOr email us: \ninfo@factorbi.com\n\n\n\n\nPrices\n\n\nPricing here: \nwww.factorbi.com\n\n\nFirebird Community, \nMembers to Members Offer available.\n\n\nMembers of \nComunidad AWS en Espa\u00f1ol\n, ask for special deal.\n\n\n\n\nOn The Media\n\n\nA journey from on-premises to Cloud Business Intelligence\n\n\n\n\n\n\n\n\nWhy we dropped Microsoft Power BI and embraced AWS QuickSight\n\n\n\n\n\n\n\n\nAt AWS re:Invent 2017\n\n\n\n\n\n\n\n\n\n\nRelease Notes\n\n\n1.4.3 (GA) 2019-03-12\n\n\n\n\nSybase SQL Anywhere connection (release candidate).\n\n\nFirebird 3.0 compatibility.\n\n\nBug fixes.\n\n\n\n\n1.3.0 (GA) 2019-01-29\n\n\n\n\nTenant: Sync several on-premise databases to a single MySQL database and differentiate them with tenant_id. Great for consolidation. Learn more \nhere.\n\n\nSynchronize views and tables without a primary key using \ncustomSchema.json\n\n\nGreater support for SQL Server data types.\n\n\nBug fixes.\n\n\n\n\n1.2.2 (GA) 2018-10-31\n\n\n\n\nMySQL on-prem connection added.\n\n\nBug fixes.\n\n\n\n\n1.1.5 (GA) 2018-10-07\n\n\n\n\nVisual FoxPro on-prem connection added.\n\n\nDefine primary keys for DBF files using \ncustomSchema.json\n\n\nAdded \nbipost_system\n database on Aurora for detailed sync information.\n\n\nRead list of tables and comments from \nbipost_system\n before data is loaded to Aurora. Useful for consolidation scenarios where different on-prem databases all point to the same DB on Aurora.\n\n\nBipost Sync now display some error log messages after data is uploaded to AWS.\n\n\n\n\n1.1.0 (GA) 2018-08-12\n\n\n\n\nDBF dBase III on-prem connection added.\n\n\nRun unlimited time stored procedures after data is uploaded to Aurora.\n\n\nImproved security for two-way synchronization.\n\n\nImproved support for special characters, e.g. now able to sync XML documents inside BLOB (Firebird) and TEXT (SQL Server) fields.\n\n\nFirst time customers: Automated creation of AWS services via \nCloudFormation template.\n\n\nBug fixes.\n\n\n\n\n1.0.0 (General Availability) 2018-03-02\n\n\n\n\nData upload is now done through secure HTTPS.\n\n\nJOIN parameter now supported for Firebird SQL.\n\n\nRecursive sync now supported for Firebird SQL.\n\n\nConnection information now encrypted.\n\n\nBug fixes.\n\n\n\n\n0.5.6 (Beta) 2017-12-02\n\n\n\n\nBidirectional syncing is here!\n\n\nSynchronize to any AWS Region.\n\n\nPerformance improvements to API, now able to load nearly 1.5 million rows (or 280 MB uncompressed files) on a single call. \n\n\nFirebird transaction READ UNCOMMITTED to prevent Bipost Sync from being stopped while other transactions are still not committed.\n\n\nInitial and final statements on Aurora are disabled on recursive sync.\n\n\nBug fixes.\n\n\n\n\n0.4.2 (Beta) 2017-09-16\n\n\n\n\nTable schemas are now synchronized against source definition on every sync, details \nhere.\n\n\nBug fixes.\n\n\n\n\n0.4.0 (Beta) 2017-08-20\n\n\n\n\nCustom connections added.\n\n\nInitial statement added to API.\n\n\n\n\n\n\nContact Us\n\n\nWe are always happy to hear about you.\n\n\n\n\n\n\nEmail: \ninfo@factorbi.com\n\n\n\n\n\n\nCompany page:\n\n\n\n\n\ud83c\uddfa\ud83c\uddf8 English: \nwww.factorbi.com\n\n\n\ud83c\uddf2\ud83c\uddfd Espa\u00f1ol: \nwww.factorbi.com", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Bipost is database synchronization   ETL tool for continually moving data from on-premises to AWS Aurora MySQL and back forth.  Created to keep your Windows databases on-premises while providing a way to extract, load   transform specific sets of data to AWS Aurora. Two-way synchronization lets you sync from AWS back to on-prem database. It is not intended to work as a replication engine or mirroring an entire database.  Sources:   Firebird SQL  Microsoft SQL Server  MySQL  dBase  Visual FoxPro (VFP)  Sybase SQL Anywhere   Destination:    Amazon Aurora MySQL       Deployed on AWS to scale at demand. Software as a Service (SaaS).", 
            "title": "Overview"
        }, 
        {
            "location": "/#use-cases", 
            "text": "Consolidate/merge information on AWS from across multiple locations and database engines, e.g. consolidate sales, inventory and financial information from several branches.  Build a Data Warehouse and make Analytics and Business Intelligence dashboards using  Google Data Studio  or  AWS QuickSight , with native connection to Aurora-MySQL.  Make forecasts and statistical analysis with AWS services.  Extend your on-prem ERP   POS to AWS cloud and build web applications   back-end services.  Push data from AWS to on-prem systems.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "Bipost Sync reads table schema's and data and uploads it to AWS.  Database and tables are created/altered if they don't exist, and data is loaded to Aurora-MySQL.  Before and after data is loaded to Aurora-MySQL you can transform data with stored procedures.  Don't worry about sending the same data multiple times. Data is replaced using primary keys, avoiding duplicates.  Run manually or automatically with a Windows Task schedule.  Upload big datasets using Recursive Sync.    Data is also available as CSV files on S3 so you can use other AWS services like  Amazon Athena  and  AWS Glue  to build your data lake.", 
            "title": "How it works"
        }, 
        {
            "location": "/#two-way-synchronization", 
            "text": "Synchronize from Aurora-MySQL to on-premises SQL Server or Firebird SQL.  Customize data to download using  outData.json  Insert/update the returned data to your on-prem DB.  Tables schemas are created/altered on your on-prem DB if they don't exist.  Primary keys set on Aurora-MySQL are used against on-prem DB to avoid duplicates.  Before and after data is loaded to on-prem DB you can transform data with stored procedures.  Only available for SQL Server and Firebird.  Learn more  here.", 
            "title": "Two-way synchronization"
        }, 
        {
            "location": "/#private-cloud", 
            "text": "We care deeply about privacy.  Our API links to your RDS instance on your AWS account, so you have full control and ownership of your databases.  Each RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.", 
            "title": "Private Cloud"
        }, 
        {
            "location": "/#architecture", 
            "text": "Aurora  is a MySQL compatible, fully managed database service built for the cloud with the performance and scalability of high-end commercial databases at 1/10th the cost.", 
            "title": "Architecture"
        }, 
        {
            "location": "/#start-using", 
            "text": "30 days free.  Unlimited synchronizations.  No need to provide credit card information.   --  Start here  Or email us:  info@factorbi.com", 
            "title": "Start Using"
        }, 
        {
            "location": "/#prices", 
            "text": "Pricing here:  www.factorbi.com  Firebird Community,  Members to Members Offer available.  Members of  Comunidad AWS en Espa\u00f1ol , ask for special deal.", 
            "title": "Prices"
        }, 
        {
            "location": "/#on-the-media", 
            "text": "A journey from on-premises to Cloud Business Intelligence     Why we dropped Microsoft Power BI and embraced AWS QuickSight     At AWS re:Invent 2017", 
            "title": "On The Media"
        }, 
        {
            "location": "/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/#143-ga-2019-03-12", 
            "text": "Sybase SQL Anywhere connection (release candidate).  Firebird 3.0 compatibility.  Bug fixes.", 
            "title": "1.4.3 (GA) 2019-03-12"
        }, 
        {
            "location": "/#130-ga-2019-01-29", 
            "text": "Tenant: Sync several on-premise databases to a single MySQL database and differentiate them with tenant_id. Great for consolidation. Learn more  here.  Synchronize views and tables without a primary key using  customSchema.json  Greater support for SQL Server data types.  Bug fixes.", 
            "title": "1.3.0 (GA) 2019-01-29"
        }, 
        {
            "location": "/#122-ga-2018-10-31", 
            "text": "MySQL on-prem connection added.  Bug fixes.", 
            "title": "1.2.2 (GA) 2018-10-31"
        }, 
        {
            "location": "/#115-ga-2018-10-07", 
            "text": "Visual FoxPro on-prem connection added.  Define primary keys for DBF files using  customSchema.json  Added  bipost_system  database on Aurora for detailed sync information.  Read list of tables and comments from  bipost_system  before data is loaded to Aurora. Useful for consolidation scenarios where different on-prem databases all point to the same DB on Aurora.  Bipost Sync now display some error log messages after data is uploaded to AWS.", 
            "title": "1.1.5 (GA) 2018-10-07"
        }, 
        {
            "location": "/#110-ga-2018-08-12", 
            "text": "DBF dBase III on-prem connection added.  Run unlimited time stored procedures after data is uploaded to Aurora.  Improved security for two-way synchronization.  Improved support for special characters, e.g. now able to sync XML documents inside BLOB (Firebird) and TEXT (SQL Server) fields.  First time customers: Automated creation of AWS services via  CloudFormation template.  Bug fixes.", 
            "title": "1.1.0 (GA) 2018-08-12"
        }, 
        {
            "location": "/#100-general-availability-2018-03-02", 
            "text": "Data upload is now done through secure HTTPS.  JOIN parameter now supported for Firebird SQL.  Recursive sync now supported for Firebird SQL.  Connection information now encrypted.  Bug fixes.", 
            "title": "1.0.0 (General Availability) 2018-03-02"
        }, 
        {
            "location": "/#056-beta-2017-12-02", 
            "text": "Bidirectional syncing is here!  Synchronize to any AWS Region.  Performance improvements to API, now able to load nearly 1.5 million rows (or 280 MB uncompressed files) on a single call.   Firebird transaction READ UNCOMMITTED to prevent Bipost Sync from being stopped while other transactions are still not committed.  Initial and final statements on Aurora are disabled on recursive sync.  Bug fixes.", 
            "title": "0.5.6 (Beta) 2017-12-02"
        }, 
        {
            "location": "/#042-beta-2017-09-16", 
            "text": "Table schemas are now synchronized against source definition on every sync, details  here.  Bug fixes.", 
            "title": "0.4.2 (Beta) 2017-09-16"
        }, 
        {
            "location": "/#040-beta-2017-08-20", 
            "text": "Custom connections added.  Initial statement added to API.", 
            "title": "0.4.0 (Beta) 2017-08-20"
        }, 
        {
            "location": "/#contact-us", 
            "text": "We are always happy to hear about you.    Email:  info@factorbi.com    Company page:   \ud83c\uddfa\ud83c\uddf8 English:  www.factorbi.com  \ud83c\uddf2\ud83c\uddfd Espa\u00f1ol:  www.factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/easyawssetup/", 
            "text": "Link your AWS Account\n\n\nThis is the easiest and recommended way to link your AWS Account to Bipost API.\n\n\nIMPORTANT NOTICE: If you are planning to use Bipost Sync for production you may want to follow your IT department policies and use AWS security according to your needs.\n\n\n\n\nStep 1: Have an AWS Account?\n\n\nIf you don't have an AWS Account please proceed:\n\n\n\n\n\n\nCreate an AWS Account here \naws.amazon.com\n\n\n\n\n\n\n\n\nAWS usually makes an automated verification phone call, we suggest to provide a land line.\n\n\n\n\nProvide payment information.\n\n\nSelect Basic Support (free plan).\n\n\nCongrats you have an AWS account!\n\n\n\n\nNeed Help? \n--\n Write us.\n\n\n\n\nStep 2: Canonical User ID\n\n\nLogged in to AWS Account:\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nExpand Account Identifiers.\n\n\n\n\nCopy AWS Account ID (12-digit) and Canonical User ID (64-digit).\n\n\n\n\n\n\n\n\nEmail these numbers to \ninfo@factorbi.com\n so we can setup your dedicated Bucket.\n\n\n\n\n\n\nPlease stop here until you get a reply email from Factor BI.\n We will provide your \nbucket name\n which will be used on further steps.\n\n\n\n\nStep 3: Closest AWS Region\n\n\ncloudping.info\n\n\n\n\nClick the above link and hit \nHTTP Ping\n and look for the lowest latency.\n\n\nMaybe you want to try this at different times of the day.\n\n\nTake note of the closest region.\n\n\n\n\n\n\n\n\nStep 4: CloudFormation\n\n\nBased on the result from previous step, click the icon that is the closest Region to your location.\n\n\n\n\n\n\n\n\nAWS Region\n\n\nShort name\n\n\n\n\n\n\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\n\n\nUS East (Ohio)\n\n\nus-east-2\n\n\n\n\n\n\n\n\nUS West (California)\n\n\nus-west-1\n\n\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\n\n\nCanada (Central)\n\n\nca-central-1\n\n\n\n\n\n\n\n\nEurope (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\n\n\nEurope (London)\n\n\neu-west-2\n\n\n\n\n\n\n\n\nEurope (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\n\n\nEurope (Paris)\n\n\neu-west-3\n\n\n\n\n\n\n\n\nAsia Pacific (Mumbai)\n\n\nap-south-1\n\n\n\n\n\n\n\n\nAsia Pacific (Seoul)\n\n\nap-northeast-2\n\n\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\n\n\n\n\n4.1. Select Template\n\n\n\n\nThe template must be already selected, click \nNext\n lower-right blue button.\n\n\n\n\n\n\n4.2. Specify Details\n\n\n\n\nStack Name:\n this will be the prefix of all provisioned services.  Example: \nmycompany-prod\n\n\nBucketName:\n Paste the S3 bucket name that your received from Factor BI over email. It must look like this: \nbipostdata-acb123456789012\n\n\n\n\nDBAdminPassword:\n Type a complex password. Must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.\n\n\n\n\nPassword must be at least eight characters long. Can be any printable ASCII character except \"/\", \"\"\", or \"@\".\n\n\n\n\n\n\n\n\nDBAdminUsername:\n Database Admin Username, example: \nroot\n\n\n\n\nDBInstanceClass:\n for testing purposes select the smallest available, currently \ndb.t2.small\n\n\nEnvironment:\n Text to be included in the database cluster name.\n\n\nPublicSubnetACIDR:\n Leave default. Only modify the subnet address if multiple environments are needed, example: \n10.20.10.0/24\n\n\nPublicSubnetBCIDR:\n Leave default. Only modify the subnet address if multiple environments are needed, example: \n10.20.20.0/24\n\n\nSubnetsAZ:\n Select two availability zones to create the resources.\n\n\nVPCCIDR:\n Leave default. Only modify the address if multiple environment are needed, example: \n10.20.0.0/16\n\n\nClick \nNext\n, blue button blue button.\n\n\n\n\n\n\n4.3. Options\n\n\n\n\nLeave all defaults, many in blank.\n\n\nClick \nNext\n, blue button blue button lower right.\n\n\n\n\n4.4. Review\n\n\n\n\nCheck \nI acknowledge that AWS CloudFormation might create IAM resources with custom names.\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button. \n\n\nPlease wait until creation is complete. It will take about an hour.\n\n\nIf asked to switch to the new \nCloudFormation console\n click \nTry it now.\n\n\n\n\n4.5. Resources created\n\n\n\n\n\n\nGo to CloudFormation Stacks left menu and wait until Status is \nCREATE_COMPLETE\n.\n\n\nClick your newly created Stack name.\n\n\nOnce Stack status is CREATE_COMPLETE click \nOutputs\n tab.\n\n\nYou may want to copy and save on a secure place all Outputs, as you will use them for further configuration.\n\n\n\n\nCome back any time and open again the Outputs tab.\n --\n From AWS Console Home, search for CloudFormation.\n\n\n\n\n\n\nStep 5: Add Role to Cluster\n\n\n\n\nOpen \nRDS console.\n\n\nClick \nDatabases\n on left pane.\n\n\nClick DB identifier Role \nRegional\n.\n\n\n\n\nScroll down to section \nManage IAM roles.\n\n\n\n\n\n\n\n\nUnder \nAdd IAM roles to this cluster\n select the DBRole created on Outputs tab, Step 4.5, and click \nAdd role\n button.\n\n\n\n\n\n\n\n\nWait until you see Status \nACTIVE\n.\n\n\n\n\n\n\n\n\nStep 6: Test MySQL Connection\n\n\n\n\n\n\nDownload MySQL Workbench\n and install on your machine.\n\n\n\n\n\n\nOpen MySQL Workbench and setup a new connection.\n\n\n\n\n\n\n\n\nCopy and paste from the \nOutputs\n tab (Step 4.5):\n\n\n\n\nConnection Name:\n type any name of your preference.\n\n\nConnection Method:\n \nStandard (TCP/IP)\n\n\nHostname:\n Paste the \nAuroraEndpoint\n string.\n\n\nPort:\n \n3306\n\n\nUsername:\n Paste the \nDBUser\n string\n\n\nClick \nTest Connection\n and when prompt type the \nDBAdminPassword\n you used on Step 4.2.\n\n\nIf you have a successful connection then you are good to go!\n\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Register at Factor BI\n\n\nClick and follow steps to \ncreate your account with Factor BI.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/easyawssetup/#link-your-aws-account", 
            "text": "This is the easiest and recommended way to link your AWS Account to Bipost API.  IMPORTANT NOTICE: If you are planning to use Bipost Sync for production you may want to follow your IT department policies and use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/easyawssetup/#step-1-have-an-aws-account", 
            "text": "If you don't have an AWS Account please proceed:    Create an AWS Account here  aws.amazon.com     AWS usually makes an automated verification phone call, we suggest to provide a land line.   Provide payment information.  Select Basic Support (free plan).  Congrats you have an AWS account!   Need Help?  --  Write us.", 
            "title": "Step 1: Have an AWS Account?"
        }, 
        {
            "location": "/easyawssetup/#step-2-canonical-user-id", 
            "text": "Logged in to AWS Account:   Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Expand Account Identifiers.   Copy AWS Account ID (12-digit) and Canonical User ID (64-digit).     Email these numbers to  info@factorbi.com  so we can setup your dedicated Bucket.    Please stop here until you get a reply email from Factor BI.  We will provide your  bucket name  which will be used on further steps.", 
            "title": "Step 2: Canonical User ID"
        }, 
        {
            "location": "/easyawssetup/#step-3-closest-aws-region", 
            "text": "cloudping.info   Click the above link and hit  HTTP Ping  and look for the lowest latency.  Maybe you want to try this at different times of the day.  Take note of the closest region.", 
            "title": "Step 3: Closest AWS Region"
        }, 
        {
            "location": "/easyawssetup/#step-4-cloudformation", 
            "text": "Based on the result from previous step, click the icon that is the closest Region to your location.     AWS Region  Short name       US East (N. Virginia)  us-east-1     US East (Ohio)  us-east-2     US West (California)  us-west-1     US West (Oregon)  us-west-2     Canada (Central)  ca-central-1     Europe (Ireland)  eu-west-1     Europe (London)  eu-west-2     Europe (Frankfurt)  eu-central-1     Europe (Paris)  eu-west-3     Asia Pacific (Mumbai)  ap-south-1     Asia Pacific (Seoul)  ap-northeast-2     Asia Pacific (Singapore)  ap-southeast-1     Asia Pacific (Sydney)  ap-southeast-2     Asia Pacific (Tokyo)  ap-northeast-1     South America (S\u00e3o Paulo)  sa-east-1", 
            "title": "Step 4: CloudFormation"
        }, 
        {
            "location": "/easyawssetup/#41-select-template", 
            "text": "The template must be already selected, click  Next  lower-right blue button.", 
            "title": "4.1. Select Template"
        }, 
        {
            "location": "/easyawssetup/#42-specify-details", 
            "text": "Stack Name:  this will be the prefix of all provisioned services.  Example:  mycompany-prod  BucketName:  Paste the S3 bucket name that your received from Factor BI over email. It must look like this:  bipostdata-acb123456789012   DBAdminPassword:  Type a complex password. Must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.   Password must be at least eight characters long. Can be any printable ASCII character except \"/\", \"\"\", or \"@\".     DBAdminUsername:  Database Admin Username, example:  root   DBInstanceClass:  for testing purposes select the smallest available, currently  db.t2.small  Environment:  Text to be included in the database cluster name.  PublicSubnetACIDR:  Leave default. Only modify the subnet address if multiple environments are needed, example:  10.20.10.0/24  PublicSubnetBCIDR:  Leave default. Only modify the subnet address if multiple environments are needed, example:  10.20.20.0/24  SubnetsAZ:  Select two availability zones to create the resources.  VPCCIDR:  Leave default. Only modify the address if multiple environment are needed, example:  10.20.0.0/16  Click  Next , blue button blue button.", 
            "title": "4.2. Specify Details"
        }, 
        {
            "location": "/easyawssetup/#43-options", 
            "text": "Leave all defaults, many in blank.  Click  Next , blue button blue button lower right.", 
            "title": "4.3. Options"
        }, 
        {
            "location": "/easyawssetup/#44-review", 
            "text": "Check  I acknowledge that AWS CloudFormation might create IAM resources with custom names.     Click  Create  blue button.   Please wait until creation is complete. It will take about an hour.  If asked to switch to the new  CloudFormation console  click  Try it now.", 
            "title": "4.4. Review"
        }, 
        {
            "location": "/easyawssetup/#45-resources-created", 
            "text": "Go to CloudFormation Stacks left menu and wait until Status is  CREATE_COMPLETE .  Click your newly created Stack name.  Once Stack status is CREATE_COMPLETE click  Outputs  tab.  You may want to copy and save on a secure place all Outputs, as you will use them for further configuration.   Come back any time and open again the Outputs tab.  --  From AWS Console Home, search for CloudFormation.", 
            "title": "4.5. Resources created"
        }, 
        {
            "location": "/easyawssetup/#step-5-add-role-to-cluster", 
            "text": "Open  RDS console.  Click  Databases  on left pane.  Click DB identifier Role  Regional .   Scroll down to section  Manage IAM roles.     Under  Add IAM roles to this cluster  select the DBRole created on Outputs tab, Step 4.5, and click  Add role  button.     Wait until you see Status  ACTIVE .", 
            "title": "Step 5: Add Role to Cluster"
        }, 
        {
            "location": "/easyawssetup/#step-6-test-mysql-connection", 
            "text": "Download MySQL Workbench  and install on your machine.    Open MySQL Workbench and setup a new connection.     Copy and paste from the  Outputs  tab (Step 4.5):   Connection Name:  type any name of your preference.  Connection Method:   Standard (TCP/IP)  Hostname:  Paste the  AuroraEndpoint  string.  Port:   3306  Username:  Paste the  DBUser  string  Click  Test Connection  and when prompt type the  DBAdminPassword  you used on Step 4.2.  If you have a successful connection then you are good to go!", 
            "title": "Step 6: Test MySQL Connection"
        }, 
        {
            "location": "/easyawssetup/#step-7-register-at-factor-bi", 
            "text": "Click and follow steps to  create your account with Factor BI.", 
            "title": "Step 7: Register at Factor BI"
        }, 
        {
            "location": "/registration/", 
            "text": "Create your Account\n\n\nOpen \nFactor BI Console.\n \n\n\nClick \nCreate Your Account\n and follow steps.\n\n\nPassword must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.\n\n\n\n\n\n\nConfigure your first service\n\n\n\n\nLog in to the \nConsole.\n\n\nOn the left pane, go to \nService Numbers\n, click \nEdit\n then \nNew\n.\n\n\n\n\nEdit \nDatabase\n and \nDescription\n, then click \nEdit\n again to save.\n\n\nDatabase must be all lowercase. Can't contain special characters.\n\n\n\n\n\n\n\n\nGo to left pane \nRDS Instances\n and click under \nHostname\n.\n\n\nFill the following fields with the information you have on \nOutputs\n tab, \nStep 4.5\n: \n\n\n\n\nHostname or Endpoint\n\n\nUsername\n\n\nPassword\n\n\nPort\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Bipost Sync\n\n\nDownload Bipost Sync for Windows.", 
            "title": "Factor BI Console"
        }, 
        {
            "location": "/registration/#create-your-account", 
            "text": "Open  Factor BI Console.    Click  Create Your Account  and follow steps.  Password must be at least 8 characters containing uppercase and lowercase letters, numbers and symbols.", 
            "title": "Create your Account"
        }, 
        {
            "location": "/registration/#configure-your-first-service", 
            "text": "Log in to the  Console.  On the left pane, go to  Service Numbers , click  Edit  then  New .   Edit  Database  and  Description , then click  Edit  again to save.  Database must be all lowercase. Can't contain special characters.     Go to left pane  RDS Instances  and click under  Hostname .  Fill the following fields with the information you have on  Outputs  tab,  Step 4.5 :    Hostname or Endpoint  Username  Password  Port", 
            "title": "Configure your first service"
        }, 
        {
            "location": "/registration/#download-bipost-sync", 
            "text": "Download Bipost Sync for Windows.", 
            "title": "Download Bipost Sync"
        }, 
        {
            "location": "/installation/", 
            "text": "Prerequisites\n\n\nWindow XP \n Vista\n\n\nDownload and install: \n\n\n\n\nWindowsInstaller.\n\n\nPrerequisites for Windows 7 and 8.\n\n\n\n\nWindows 7 and 8\n\n\nFor most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.\n\n\n\n\nMicrosoft .NET Framework 4\n\n\nMicrosoft Visual C++ 2010\n\n\nVisual FoxPro OLEDB\n \n-- Only for DBF connection.\n\n\n\n\nDownload \n Install\n\n\n\n\n\n\nDownload latest version here.\n\n\n\n\n\n\nUnzip \nbiPost.zip\n to any folder on your Windows.\n\n\n\n\n\n\nRun \nbiPost.exe\n\n\n\n\n\n\nConfigure synchronization for \nSQL Server\n, \nFirebird\n or \ndBase", 
            "title": "Download Program"
        }, 
        {
            "location": "/installation/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/installation/#window-xp-vista", 
            "text": "Download and install:    WindowsInstaller.  Prerequisites for Windows 7 and 8.", 
            "title": "Window XP &amp; Vista"
        }, 
        {
            "location": "/installation/#windows-7-and-8", 
            "text": "For most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.   Microsoft .NET Framework 4  Microsoft Visual C++ 2010  Visual FoxPro OLEDB   -- Only for DBF connection.", 
            "title": "Windows 7 and 8"
        }, 
        {
            "location": "/installation/#download-install", 
            "text": "Download latest version here.    Unzip  biPost.zip  to any folder on your Windows.    Run  biPost.exe    Configure synchronization for  SQL Server ,  Firebird  or  dBase", 
            "title": "Download &amp; Install"
        }, 
        {
            "location": "/firebird/", 
            "text": "Synchronize Firebird to AWS\n\n\n\n\nContinually export Firebird data to AWS Aurora MySQL.\n\n\n\n\nGreat to consolidate/merge information on the cloud from multiple locations that use FirebirdSQL databases, then build your own Data Warehouse and deliver Business Intelligence.  \n\n\nOn top of AWS build web applications using data pulled from on-prem FirebirdSQL.\n\n\nSend data from AWS back to FirebirdSQL.\n\n\n\n\n\n\nBefore You Begin\n\n\n\n\nHave your AWS account linked it to our API. Not sure? \nfollow here.\n\n\nComplete registration on \nFactor BI console\n \n\n\nCreate Service and Activation keys and configure service, \n--\n details here.\n\n\nDownload Bipost Sync\n program for Windows.\n\n\n\n\n\n\nBipost Sync\n\n\n\n\n\n\nService No.:\n 36 digit number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n \nFirebird\n\n\nSystem:\n \nCustom...\n\n\n\n\n\n\n\n\n\n\nFirebird Connection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Connection\n\n\nLeave unchecked\n\n\nEnable ONLY when biPost.exe is not located on Firebird Server.\n\n\n\n\n\n\nServer\n\n\nLeave blank\n\n\nUse ONLY when Remote Connection is enabled: Type IP or name of Firebird Server.\n\n\n\n\n\n\nUser\n\n\nFirebird User\n\n\nSYSDBA or other user with read access\n\n\n\n\n\n\nPassword\n\n\n\n\n\n\n\n\n\n\nDatabase\n\n\nLocation of your .FDB file\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\nSpecific Bucket:\n\n\n\n\n\n\nEnter your \nBucket Name\n. It may look like this: \nbipostdata-acb123456789012\n\n\n\n\n\n\nIt is available on your AWS Account \\ CloudFormation Stack.\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Data\n\n\n\n\n\n\nEnable to download data from AWS Aurora to on-premises.\n\n\n\n\n\n\nDownloaded data will be available on \n%localappdata%/biPost/out_\n Windows folder.\n\n\n\n\n\n\nUpdate/insert data to Firebird by enabling \nProcess Data\n check box.\n\n\n\n\n\n\nMore about downloading data check here \n--\n Sync back to Windows\n\n\n\n\n\n\n\n\n\n\n\n\nRecursive Sync\n\n\n\n\n\n\nUse only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.\n\n\n\n\n\n\nAlways use along with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\n\n\n\n\nNOTE:\n For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==\n \nExamples here.\n\n\n\n\n\n\nMore info about this feature see \nRecursive Synchronization\n\n\n\n\n\n\nTenant\n\n\n\n\n\n\nThis allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with \ntenant_id\n. Great for consolidation!\n\n\n\n\n\n\nActivate and type a string to add a fixed column to every table that is synced.\n\n\n\n\n\n\nThis fixed column will be included in the PRIMARY KEY on Aurora-MySQL. \n\n\n\n\n\n\n\n\nTables to Sync\n\n\nIMPORTANT:\n Tables and subsets of data to synchronize are specified in \ncustomData.json\n file ==\n \nfollow here.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.\n\n\n\n\nUse Cases\n\n\nMany legacy and critical mission systems use Firebird SQL.\n\n\nMicrosip ERP\n and \nAspel\n are examples of systems built with Firebird. We have companies that use Bipost Sync + AWS to achieve sales \n financial consolidation and enable cloud Business Intelligence with \nGoogle Data Studio.\n\n\n\n\n\n\nContact Us\n\n\nQuestions? Send us an email: \ninfo@factorbi.com", 
            "title": "Firebird to MySQL Aurora"
        }, 
        {
            "location": "/firebird/#synchronize-firebird-to-aws", 
            "text": "Continually export Firebird data to AWS Aurora MySQL.   Great to consolidate/merge information on the cloud from multiple locations that use FirebirdSQL databases, then build your own Data Warehouse and deliver Business Intelligence.    On top of AWS build web applications using data pulled from on-prem FirebirdSQL.  Send data from AWS back to FirebirdSQL.", 
            "title": "Synchronize Firebird to AWS"
        }, 
        {
            "location": "/firebird/#before-you-begin", 
            "text": "Have your AWS account linked it to our API. Not sure?  follow here.  Complete registration on  Factor BI console    Create Service and Activation keys and configure service,  --  details here.  Download Bipost Sync  program for Windows.", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/firebird/#bipost-sync", 
            "text": "Service No.:  36 digit number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:   Firebird  System:   Custom...      Firebird Connection        Remote Connection  Leave unchecked  Enable ONLY when biPost.exe is not located on Firebird Server.    Server  Leave blank  Use ONLY when Remote Connection is enabled: Type IP or name of Firebird Server.    User  Firebird User  SYSDBA or other user with read access    Password      Database  Location of your .FDB file", 
            "title": "Bipost Sync"
        }, 
        {
            "location": "/firebird/#general-settings", 
            "text": "", 
            "title": "General Settings"
        }, 
        {
            "location": "/firebird/#specific-bucket", 
            "text": "Enter your  Bucket Name . It may look like this:  bipostdata-acb123456789012    It is available on your AWS Account \\ CloudFormation Stack.", 
            "title": "Specific Bucket:"
        }, 
        {
            "location": "/firebird/#download-data", 
            "text": "Enable to download data from AWS Aurora to on-premises.    Downloaded data will be available on  %localappdata%/biPost/out_  Windows folder.    Update/insert data to Firebird by enabling  Process Data  check box.    More about downloading data check here  --  Sync back to Windows", 
            "title": "Download Data"
        }, 
        {
            "location": "/firebird/#recursive-sync", 
            "text": "Use only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.    Always use along with  customData.json  so you can configure the date field to use for each table.    NOTE:  For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==   Examples here.    More info about this feature see  Recursive Synchronization", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/firebird/#tenant", 
            "text": "This allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with  tenant_id . Great for consolidation!    Activate and type a string to add a fixed column to every table that is synced.    This fixed column will be included in the PRIMARY KEY on Aurora-MySQL.", 
            "title": "Tenant"
        }, 
        {
            "location": "/firebird/#tables-to-sync", 
            "text": "IMPORTANT:  Tables and subsets of data to synchronize are specified in  customData.json  file ==   follow here.", 
            "title": "Tables to Sync"
        }, 
        {
            "location": "/firebird/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/firebird/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/firebird/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/firebird/#use-cases", 
            "text": "Many legacy and critical mission systems use Firebird SQL.  Microsip ERP  and  Aspel  are examples of systems built with Firebird. We have companies that use Bipost Sync + AWS to achieve sales   financial consolidation and enable cloud Business Intelligence with  Google Data Studio.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/firebird/#contact-us", 
            "text": "Questions? Send us an email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/sqlserver/", 
            "text": "Synchronize SQL Server to AWS\n\n\n\n\nContinually export SQL Server data to AWS Aurora MySQL.\n\n\n\n\nGreat to consolidate/merge information on the cloud from multiple locations that use SQL Server databases, then build your own Data Warehouse and deliver Business Intelligence.  \n\n\nOn top of AWS build web applications using data pulled from on-prem SQL Server.\n\n\nSend data from AWS back to SQL Server.\n\n\n\n\n\n\nBefore You Begin\n\n\n\n\nHave your AWS account linked it to our API. Not sure? \nfollow here.\n\n\nComplete registration on \nFactor BI console\n \n\n\nCreate Service and Activation keys and configure service, \n--\n details here.\n\n\nDownload Bipost Sync\n program for Windows.\n\n\n\n\n\n\nBipost Sync\n\n\n\n\n\n\nService No.:\n 36 digit number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n \nSQL\n\n\nSystem:\n \nCustom...\n\n\n\n\n\n\n\n\n\n\nSQL Connection\n\n\n\n\n\n\n\n\n\n\n\n\nServer\n\n\nIP or name of SQL Server.\n\n\n\n\n\n\nUser\n\n\nLogin account to SQL server.\n\n\n\n\n\n\nPassword\n\n\nType login password\n\n\n\n\n\n\nDatabase\n\n\nDatabase name\n\n\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\nSpecific Bucket:\n\n\n\n\n\n\nEnter your \nBucket Name\n. It may look like this: \nbipostdata-acb123456789012\n\n\n\n\n\n\nIt is available on your AWS Account \\ CloudFormation Stack.\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Data\n\n\n\n\n\n\nEnable to download data from AWS Aurora to on-premises.\n\n\n\n\n\n\nDownloaded data will be available on \n%localappdata%/biPost/out_\n Windows folder.\n\n\n\n\n\n\nUpdate/insert data to SQL Server by enabling \nProcess Data\n check box.\n\n\n\n\n\n\nMore about downloading data check here \n--\n Sync back to Windows\n\n\n\n\n\n\n\n\n\n\n\n\nRecursive Sync\n\n\n\n\n\n\nUse only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.\n\n\n\n\n\n\nAlways use along with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\n\n\n\n\nNOTE:\n For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==\n \nExamples here.\n\n\n\n\n\n\nMore info about this feature see \nRecursive Synchronization\n\n\n\n\n\n\nTenant\n\n\n\n\n\n\nThis allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with \ntenant_id\n. Great for consolidation!\n\n\n\n\n\n\nActivate and type a string to add a fixed column to every table that is synced.\n\n\n\n\n\n\nThis fixed column will be included in the PRIMARY KEY on Aurora-MySQL. \n\n\n\n\n\n\n\n\nTables to Sync\n\n\nIMPORTANT:\n Tables and subsets of data to synchronize are specified in \ncustomData.json\n file ==\n \nfollow here.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.\n\n\n\n\nUse Cases\n\n\nMany legacy and critical mission systems use Microsoft SQL Server.\n\n\nSoft Restaurant\n and \nIntelisis ERP\n are examples of systems built with SQL Server. We have companies that use Bipost Sync + AWS to achieve sales \n financial consolidation and enable cloud Business Intelligence with \nGoogle Data Studio.\n\n\n\n\n\n\nContact Us\n\n\nQuestions? Send us an email: \ninfo@factorbi.com", 
            "title": "SQL Server to MySQL Aurora"
        }, 
        {
            "location": "/sqlserver/#synchronize-sql-server-to-aws", 
            "text": "Continually export SQL Server data to AWS Aurora MySQL.   Great to consolidate/merge information on the cloud from multiple locations that use SQL Server databases, then build your own Data Warehouse and deliver Business Intelligence.    On top of AWS build web applications using data pulled from on-prem SQL Server.  Send data from AWS back to SQL Server.", 
            "title": "Synchronize SQL Server to AWS"
        }, 
        {
            "location": "/sqlserver/#before-you-begin", 
            "text": "Have your AWS account linked it to our API. Not sure?  follow here.  Complete registration on  Factor BI console    Create Service and Activation keys and configure service,  --  details here.  Download Bipost Sync  program for Windows.", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/sqlserver/#bipost-sync", 
            "text": "Service No.:  36 digit number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:   SQL  System:   Custom...      SQL Connection       Server  IP or name of SQL Server.    User  Login account to SQL server.    Password  Type login password    Database  Database name", 
            "title": "Bipost Sync"
        }, 
        {
            "location": "/sqlserver/#general-settings", 
            "text": "", 
            "title": "General Settings"
        }, 
        {
            "location": "/sqlserver/#specific-bucket", 
            "text": "Enter your  Bucket Name . It may look like this:  bipostdata-acb123456789012    It is available on your AWS Account \\ CloudFormation Stack.", 
            "title": "Specific Bucket:"
        }, 
        {
            "location": "/sqlserver/#download-data", 
            "text": "Enable to download data from AWS Aurora to on-premises.    Downloaded data will be available on  %localappdata%/biPost/out_  Windows folder.    Update/insert data to SQL Server by enabling  Process Data  check box.    More about downloading data check here  --  Sync back to Windows", 
            "title": "Download Data"
        }, 
        {
            "location": "/sqlserver/#recursive-sync", 
            "text": "Use only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.    Always use along with  customData.json  so you can configure the date field to use for each table.    NOTE:  For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==   Examples here.    More info about this feature see  Recursive Synchronization", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/sqlserver/#tenant", 
            "text": "This allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with  tenant_id . Great for consolidation!    Activate and type a string to add a fixed column to every table that is synced.    This fixed column will be included in the PRIMARY KEY on Aurora-MySQL.", 
            "title": "Tenant"
        }, 
        {
            "location": "/sqlserver/#tables-to-sync", 
            "text": "IMPORTANT:  Tables and subsets of data to synchronize are specified in  customData.json  file ==   follow here.", 
            "title": "Tables to Sync"
        }, 
        {
            "location": "/sqlserver/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/sqlserver/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/sqlserver/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/sqlserver/#use-cases", 
            "text": "Many legacy and critical mission systems use Microsoft SQL Server.  Soft Restaurant  and  Intelisis ERP  are examples of systems built with SQL Server. We have companies that use Bipost Sync + AWS to achieve sales   financial consolidation and enable cloud Business Intelligence with  Google Data Studio.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/sqlserver/#contact-us", 
            "text": "Questions? Send us an email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/dbase/", 
            "text": "Synchronize dBase to AWS\n\n\n\n\nContinually export dBase data to AWS Aurora MySQL.\n\n\n\n\nGreat to consolidate/merge information on the cloud from multiple locations that use dBase databases, then build your own Data Warehouse and deliver Business Intelligence.  \n\n\nOn top of AWS build web applications using data pulled from on-prem dBase.\n\n\n\n\n\n\nBefore You Begin\n\n\n\n\nHave your AWS account linked it to our API. Not sure? \nfollow here.\n\n\nComplete registration on \nFactor BI console\n \n\n\nCreate Service and Activation keys and configure service, \n--\n details here.\n\n\nDownload Bipost Sync\n program for Windows.\n\n\n\n\n\n\nBipost Sync\n\n\n\n\n\n\nService No.:\n 36 digit number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n \nDBF\n\n\nSystem:\n \nCustom...\n\n\nDBF Connection:\n Directory containing \n.dbf\n files.\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\nSpecific Bucket:\n\n\n\n\n\n\nEnter your \nBucket Name\n. It may look like this: \nbipostdata-acb123456789012\n\n\n\n\n\n\nIt is available on your AWS Account \\ CloudFormation Stack.\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Data\n\n\n\n\nLeave \nunchecked\n.\n\n\n\n\nRecursive Sync\n\n\n\n\n\n\nUse only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.\n\n\n\n\n\n\nAlways use along with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\n\n\n\n\nNOTE:\n For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==\n \nExamples here.\n\n\n\n\n\n\nMore info about this feature see \nRecursive Synchronization\n\n\n\n\n\n\nTenant\n\n\n\n\n\n\nThis allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with \ntenant_id\n. Great for consolidation!\n\n\n\n\n\n\nActivate and type a string to add a fixed column to every table that is synced.\n\n\n\n\n\n\nThis fixed column will be included in the PRIMARY KEY on Aurora-MySQL. \n\n\n\n\n\n\n\n\nTables to Sync\n\n\nIMPORTANT:\n Tables and subsets of data to synchronize are specified in \ncustomData.json\n file ==\n \nfollow here.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.\n\n\n\n\nPOSitouch Use Case\n\n\nPOSitouch\n Point of Sale creates DBF files at the end of the shift. You can daily export DBF data from several POSitouch restaurants and consolidate them on AWS at scale.\n\n\nMake POSitouch reporting simple. Create KPI's, Dashboards and Business Intelligence analytics using \nGoogle Data Studio\n or \nAWS QuickSight\n.\n\n\n\n\nMore about POSitouch AWS integration here.\n\n\nPOSitouch DBF database structure \n documentation download.\n\n\nPOSitouch Inventory Control and Food Cost Manual with Inventory Reports.\n\n\nPOSitouch Time and Attendance User Manual.\n\n\n\n\nKeywords: POSIDBF | Positouch database files | Positouch DBF files | Positouch database tables | merge multiple sites | merge locations\n\n\n\n\n\n\nContact Us\n\n\nQuestions? Send us an email: \ninfo@factorbi.com", 
            "title": "dBase to MySQL Aurora"
        }, 
        {
            "location": "/dbase/#synchronize-dbase-to-aws", 
            "text": "Continually export dBase data to AWS Aurora MySQL.   Great to consolidate/merge information on the cloud from multiple locations that use dBase databases, then build your own Data Warehouse and deliver Business Intelligence.    On top of AWS build web applications using data pulled from on-prem dBase.", 
            "title": "Synchronize dBase to AWS"
        }, 
        {
            "location": "/dbase/#before-you-begin", 
            "text": "Have your AWS account linked it to our API. Not sure?  follow here.  Complete registration on  Factor BI console    Create Service and Activation keys and configure service,  --  details here.  Download Bipost Sync  program for Windows.", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/dbase/#bipost-sync", 
            "text": "Service No.:  36 digit number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:   DBF  System:   Custom...  DBF Connection:  Directory containing  .dbf  files.", 
            "title": "Bipost Sync"
        }, 
        {
            "location": "/dbase/#general-settings", 
            "text": "", 
            "title": "General Settings"
        }, 
        {
            "location": "/dbase/#specific-bucket", 
            "text": "Enter your  Bucket Name . It may look like this:  bipostdata-acb123456789012    It is available on your AWS Account \\ CloudFormation Stack.", 
            "title": "Specific Bucket:"
        }, 
        {
            "location": "/dbase/#download-data", 
            "text": "Leave  unchecked .", 
            "title": "Download Data"
        }, 
        {
            "location": "/dbase/#recursive-sync", 
            "text": "Use only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.    Always use along with  customData.json  so you can configure the date field to use for each table.    NOTE:  For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==   Examples here.    More info about this feature see  Recursive Synchronization", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/dbase/#tenant", 
            "text": "This allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with  tenant_id . Great for consolidation!    Activate and type a string to add a fixed column to every table that is synced.    This fixed column will be included in the PRIMARY KEY on Aurora-MySQL.", 
            "title": "Tenant"
        }, 
        {
            "location": "/dbase/#tables-to-sync", 
            "text": "IMPORTANT:  Tables and subsets of data to synchronize are specified in  customData.json  file ==   follow here.", 
            "title": "Tables to Sync"
        }, 
        {
            "location": "/dbase/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/dbase/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/dbase/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/dbase/#positouch-use-case", 
            "text": "POSitouch  Point of Sale creates DBF files at the end of the shift. You can daily export DBF data from several POSitouch restaurants and consolidate them on AWS at scale.  Make POSitouch reporting simple. Create KPI's, Dashboards and Business Intelligence analytics using  Google Data Studio  or  AWS QuickSight .   More about POSitouch AWS integration here.  POSitouch DBF database structure   documentation download.  POSitouch Inventory Control and Food Cost Manual with Inventory Reports.  POSitouch Time and Attendance User Manual.   Keywords: POSIDBF | Positouch database files | Positouch DBF files | Positouch database tables | merge multiple sites | merge locations", 
            "title": "POSitouch Use Case"
        }, 
        {
            "location": "/dbase/#contact-us", 
            "text": "Questions? Send us an email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/visualfoxpro/", 
            "text": "Synchronize Visual FoxPro to AWS\n\n\n\n\nContinually export Visual FoxPro data to AWS Aurora MySQL.\n\n\n\n\nGreat to consolidate/merge information on the cloud from multiple locations that use Visual FoxPro databases, then build your own Data Warehouse and deliver Business Intelligence.  \n\n\nOn top of AWS build web applications using data pulled from on-prem Visual FoxPro.\n\n\n\n\n\n\nBefore You Begin\n\n\n\n\nHave your AWS account linked it to our API. Not sure? \nfollow here.\n\n\nComplete registration on \nFactor BI console\n \n\n\nCreate Service and Activation keys and configure service, \n--\n details here.\n\n\nDownload Bipost Sync\n program for Windows.\n\n\n\n\n\n\nBipost Sync\n\n\n\n\n\n\nService No.:\n 36 digit number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n \nDBF\n\n\nSystem:\n \nCustom...\n\n\nDBF Connection:\n Directory containing \n.dbf\n files.\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\nSpecific Bucket:\n\n\n\n\n\n\nEnter your \nBucket Name\n. It may look like this: \nbipostdata-acb123456789012\n\n\n\n\n\n\nIt is available on your AWS Account \\ CloudFormation Stack.\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Data\n\n\n\n\nLeave \nunchecked\n.\n\n\n\n\nRecursive Sync\n\n\n\n\n\n\nUse only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.\n\n\n\n\n\n\nAlways use along with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\n\n\n\n\nNOTE:\n For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==\n \nExamples here.\n\n\n\n\n\n\nMore info about this feature see \nRecursive Synchronization\n\n\n\n\n\n\nTenant\n\n\n\n\n\n\nThis allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with \ntenant_id\n. Great for consolidation!\n\n\n\n\n\n\nActivate and type a string to add a fixed column to every table that is synced.\n\n\n\n\n\n\nThis fixed column will be included in the PRIMARY KEY on Aurora-MySQL. \n\n\n\n\n\n\n\n\nTables to Sync\n\n\nIMPORTANT:\n Tables and subsets of data to synchronize are specified in \ncustomData.json\n file ==\n \nfollow here.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.\n\n\n\n\nContact Us\n\n\nQuestions? Send us an email: \ninfo@factorbi.com", 
            "title": "Visual FoxPro to MySQL Aurora"
        }, 
        {
            "location": "/visualfoxpro/#synchronize-visual-foxpro-to-aws", 
            "text": "Continually export Visual FoxPro data to AWS Aurora MySQL.   Great to consolidate/merge information on the cloud from multiple locations that use Visual FoxPro databases, then build your own Data Warehouse and deliver Business Intelligence.    On top of AWS build web applications using data pulled from on-prem Visual FoxPro.", 
            "title": "Synchronize Visual FoxPro to AWS"
        }, 
        {
            "location": "/visualfoxpro/#before-you-begin", 
            "text": "Have your AWS account linked it to our API. Not sure?  follow here.  Complete registration on  Factor BI console    Create Service and Activation keys and configure service,  --  details here.  Download Bipost Sync  program for Windows.", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/visualfoxpro/#bipost-sync", 
            "text": "Service No.:  36 digit number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:   DBF  System:   Custom...  DBF Connection:  Directory containing  .dbf  files.", 
            "title": "Bipost Sync"
        }, 
        {
            "location": "/visualfoxpro/#general-settings", 
            "text": "", 
            "title": "General Settings"
        }, 
        {
            "location": "/visualfoxpro/#specific-bucket", 
            "text": "Enter your  Bucket Name . It may look like this:  bipostdata-acb123456789012    It is available on your AWS Account \\ CloudFormation Stack.", 
            "title": "Specific Bucket:"
        }, 
        {
            "location": "/visualfoxpro/#download-data", 
            "text": "Leave  unchecked .", 
            "title": "Download Data"
        }, 
        {
            "location": "/visualfoxpro/#recursive-sync", 
            "text": "Use only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.    Always use along with  customData.json  so you can configure the date field to use for each table.    NOTE:  For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==   Examples here.    More info about this feature see  Recursive Synchronization", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/visualfoxpro/#tenant", 
            "text": "This allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with  tenant_id . Great for consolidation!    Activate and type a string to add a fixed column to every table that is synced.    This fixed column will be included in the PRIMARY KEY on Aurora-MySQL.", 
            "title": "Tenant"
        }, 
        {
            "location": "/visualfoxpro/#tables-to-sync", 
            "text": "IMPORTANT:  Tables and subsets of data to synchronize are specified in  customData.json  file ==   follow here.", 
            "title": "Tables to Sync"
        }, 
        {
            "location": "/visualfoxpro/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/visualfoxpro/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/visualfoxpro/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/visualfoxpro/#contact-us", 
            "text": "Questions? Send us an email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/sqlanywhere/", 
            "text": "Synchronize Sybase SQL Anywhere to AWS\n\n\n\n\nContinually export Sybase SQL Anywhere data to AWS Aurora MySQL.\n\n\n\n\nGreat to consolidate/merge information on the cloud from multiple locations that use SQL Anywhere databases, then build your own Data Warehouse and deliver Business Intelligence.  \n\n\nOn top of AWS build web applications using data pulled from on-prem SQL Anywhere.\n\n\n\n\n\n\nBefore You Begin\n\n\n\n\nHave your AWS account linked it to our API. Not sure? \nfollow here.\n\n\nComplete registration on \nFactor BI console\n \n\n\nCreate Service and Activation keys and configure service, \n--\n details here.\n\n\nDownload Bipost Sync\n program for Windows.\n\n\n\n\n\n\nBipost Sync\n\n\n\n\n\n\nService No.:\n 36 digit number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n \nSQL Anywhere\n\n\nSystem:\n \nCustom...\n\n\n\n\n\n\n\n\n\n\nSQL Anywhere\n\n\n\n\n\n\n\n\n\n\n\n\nDSN\n\n\nSystem DSN name, 32 bits\n\n\n\n\n\n\nUser\n\n\n\n\n\n\n\n\nPassword\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\nSpecific Bucket:\n\n\n\n\n\n\nEnter your \nBucket Name\n. It may look like this: \nbipostdata-acb123456789012\n\n\n\n\n\n\nIt is available on your AWS Account \\ CloudFormation Stack.\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Data\n\n\n\n\nLeave \nunchecked\n.\n\n\n\n\nRecursive Sync\n\n\n\n\n\n\nUse only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.\n\n\n\n\n\n\nAlways use along with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\n\n\n\n\nNOTE:\n For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==\n \nExamples here.\n\n\n\n\n\n\nMore info about this feature see \nRecursive Synchronization\n\n\n\n\n\n\nTenant\n\n\n\n\n\n\nThis allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with \ntenant_id\n. Great for consolidation!\n\n\n\n\n\n\nActivate and type a string to add a fixed column to every table that is synced.\n\n\n\n\n\n\nThis fixed column will be included in the PRIMARY KEY on Aurora-MySQL. \n\n\n\n\n\n\n\n\nTables to Sync\n\n\nIMPORTANT:\n Tables and subsets of data to synchronize are specified in \ncustomData.json\n file ==\n \nfollow here.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.\n\n\n\n\nOracle Micros RES 3700 POS Business Intelligence Use Case\n\n\nOracle Hospitality\n Point of Sale system works with SQL Anywhere database. You can daily export data from several restaurants and consolidate them on AWS at scale.\n\n\nMake Micros POS reporting affordable at scale. Create KPI's, Dashboards and Business Intelligence analytics using \nGoogle Data Studio\n or \nAWS QuickSight\n.\n\n\n\n\nOracle Micros RES database tables \n documentation download.\n\n\nOracle Hospitality RES 3700 Installation Guide.\n\n\n\n\nKeywords: Micros 3700 RES | Micros database files | Micros cloud reporting | merge multiple locations | Micros AWS | iAnywhere.Data.SQLAnywhere.v4.0.dll\n\n\n\n\n\n\nContact Us\n\n\nQuestions? Send us an email: \ninfo@factorbi.com", 
            "title": "Sybase SQL Anywhere to MySQL Aurora"
        }, 
        {
            "location": "/sqlanywhere/#synchronize-sybase-sql-anywhere-to-aws", 
            "text": "Continually export Sybase SQL Anywhere data to AWS Aurora MySQL.   Great to consolidate/merge information on the cloud from multiple locations that use SQL Anywhere databases, then build your own Data Warehouse and deliver Business Intelligence.    On top of AWS build web applications using data pulled from on-prem SQL Anywhere.", 
            "title": "Synchronize Sybase SQL Anywhere to AWS"
        }, 
        {
            "location": "/sqlanywhere/#before-you-begin", 
            "text": "Have your AWS account linked it to our API. Not sure?  follow here.  Complete registration on  Factor BI console    Create Service and Activation keys and configure service,  --  details here.  Download Bipost Sync  program for Windows.", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/sqlanywhere/#bipost-sync", 
            "text": "Service No.:  36 digit number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:   SQL Anywhere  System:   Custom...      SQL Anywhere       DSN  System DSN name, 32 bits    User     Password", 
            "title": "Bipost Sync"
        }, 
        {
            "location": "/sqlanywhere/#general-settings", 
            "text": "", 
            "title": "General Settings"
        }, 
        {
            "location": "/sqlanywhere/#specific-bucket", 
            "text": "Enter your  Bucket Name . It may look like this:  bipostdata-acb123456789012    It is available on your AWS Account \\ CloudFormation Stack.", 
            "title": "Specific Bucket:"
        }, 
        {
            "location": "/sqlanywhere/#download-data", 
            "text": "Leave  unchecked .", 
            "title": "Download Data"
        }, 
        {
            "location": "/sqlanywhere/#recursive-sync", 
            "text": "Use only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.    Always use along with  customData.json  so you can configure the date field to use for each table.    NOTE:  For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==   Examples here.    More info about this feature see  Recursive Synchronization", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/sqlanywhere/#tenant", 
            "text": "This allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with  tenant_id . Great for consolidation!    Activate and type a string to add a fixed column to every table that is synced.    This fixed column will be included in the PRIMARY KEY on Aurora-MySQL.", 
            "title": "Tenant"
        }, 
        {
            "location": "/sqlanywhere/#tables-to-sync", 
            "text": "IMPORTANT:  Tables and subsets of data to synchronize are specified in  customData.json  file ==   follow here.", 
            "title": "Tables to Sync"
        }, 
        {
            "location": "/sqlanywhere/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/sqlanywhere/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/sqlanywhere/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/sqlanywhere/#oracle-micros-res-3700-pos-business-intelligence-use-case", 
            "text": "Oracle Hospitality  Point of Sale system works with SQL Anywhere database. You can daily export data from several restaurants and consolidate them on AWS at scale.  Make Micros POS reporting affordable at scale. Create KPI's, Dashboards and Business Intelligence analytics using  Google Data Studio  or  AWS QuickSight .   Oracle Micros RES database tables   documentation download.  Oracle Hospitality RES 3700 Installation Guide.   Keywords: Micros 3700 RES | Micros database files | Micros cloud reporting | merge multiple locations | Micros AWS | iAnywhere.Data.SQLAnywhere.v4.0.dll", 
            "title": "Oracle Micros RES 3700 POS Business Intelligence Use Case"
        }, 
        {
            "location": "/sqlanywhere/#contact-us", 
            "text": "Questions? Send us an email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/mysql/", 
            "text": "Synchronize MySQL on-prem to AWS\n\n\n\n\nContinually export MySQL On-Premises data to AWS Aurora MySQL.\n\n\n\n\nGreat to consolidate/merge information on the cloud from multiple locations that use MySQL databases, then build your own Data Warehouse and deliver Business Intelligence.  \n\n\nOn top of AWS build web applications using data pulled from on-prem MySQL.\n\n\n\n\n\n\nBefore You Begin\n\n\n\n\nHave your AWS account linked it to our API. Not sure? \nfollow here.\n\n\nComplete registration on \nFactor BI console\n \n\n\nCreate Service and Activation keys and configure service, \n--\n details here.\n\n\nDownload Bipost Sync\n program for Windows.\n\n\n\n\n\n\nBipost Sync\n\n\n\n\n\n\nService No.:\n 36 digit number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n \nMySQL\n\n\nSystem:\n \nCustom...\n\n\n\n\n\n\n\n\n\n\nMySQL Connection\n\n\n\n\n\n\n\n\n\n\n\n\nServer\n\n\nHost name\n\n\n\n\n\n\nUser\n\n\n\n\n\n\n\n\nPassword\n\n\n\n\n\n\n\n\nDatabase\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\nSpecific Bucket:\n\n\n\n\n\n\nEnter your \nBucket Name\n. It may look like this: \nbipostdata-acb123456789012\n\n\n\n\n\n\nIt is available on your AWS Account \\ CloudFormation Stack.\n\n\n\n\n\n\n\n\n\n\n\n\nDownload Data\n\n\n\n\nLeave \nunchecked\n.\n\n\n\n\nRecursive Sync\n\n\n\n\n\n\nUse only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.\n\n\n\n\n\n\nAlways use along with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\n\n\n\n\nNOTE:\n For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==\n \nExamples here.\n\n\n\n\n\n\nMore info about this feature see \nRecursive Synchronization\n\n\n\n\n\n\nTenant\n\n\n\n\n\n\nThis allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with \ntenant_id\n. Great for consolidation!\n\n\n\n\n\n\nActivate and type a string to add a fixed column to every table that is synced.\n\n\n\n\n\n\nThis fixed column will be included in the PRIMARY KEY on Aurora-MySQL. \n\n\n\n\n\n\n\n\nTables to Sync\n\n\nIMPORTANT:\n Tables and subsets of data to synchronize are specified in \ncustomData.json\n file ==\n \nfollow here.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of Bipost Sync, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of Bipost Sync can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nSync multiple databases\n\n\nIf you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained \nhere.\n\n\n\n\nContact\n\n\nQuestions? Send us an email: \ninfo@factorbi.com", 
            "title": "MySQL on-prem to AWS Aurora"
        }, 
        {
            "location": "/mysql/#synchronize-mysql-on-prem-to-aws", 
            "text": "Continually export MySQL On-Premises data to AWS Aurora MySQL.   Great to consolidate/merge information on the cloud from multiple locations that use MySQL databases, then build your own Data Warehouse and deliver Business Intelligence.    On top of AWS build web applications using data pulled from on-prem MySQL.", 
            "title": "Synchronize MySQL on-prem to AWS"
        }, 
        {
            "location": "/mysql/#before-you-begin", 
            "text": "Have your AWS account linked it to our API. Not sure?  follow here.  Complete registration on  Factor BI console    Create Service and Activation keys and configure service,  --  details here.  Download Bipost Sync  program for Windows.", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/mysql/#bipost-sync", 
            "text": "Service No.:  36 digit number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:   MySQL  System:   Custom...      MySQL Connection       Server  Host name    User     Password     Database", 
            "title": "Bipost Sync"
        }, 
        {
            "location": "/mysql/#general-settings", 
            "text": "", 
            "title": "General Settings"
        }, 
        {
            "location": "/mysql/#specific-bucket", 
            "text": "Enter your  Bucket Name . It may look like this:  bipostdata-acb123456789012    It is available on your AWS Account \\ CloudFormation Stack.", 
            "title": "Specific Bucket:"
        }, 
        {
            "location": "/mysql/#download-data", 
            "text": "Leave  unchecked .", 
            "title": "Download Data"
        }, 
        {
            "location": "/mysql/#recursive-sync", 
            "text": "Use only to upload historic data. It optimizes upload by extracting and uploading one day at a time for a given date range.    Always use along with  customData.json  so you can configure the date field to use for each table.    NOTE:  For daily sync, instead of using Recursive Sync, dynamically parse a date range into customData.json ==   Examples here.    More info about this feature see  Recursive Synchronization", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/mysql/#tenant", 
            "text": "This allows to sync several on-premise databases to a single Aurora-MySQL database and differentiate them with  tenant_id . Great for consolidation!    Activate and type a string to add a fixed column to every table that is synced.    This fixed column will be included in the PRIMARY KEY on Aurora-MySQL.", 
            "title": "Tenant"
        }, 
        {
            "location": "/mysql/#tables-to-sync", 
            "text": "IMPORTANT:  Tables and subsets of data to synchronize are specified in  customData.json  file ==   follow here.", 
            "title": "Tables to Sync"
        }, 
        {
            "location": "/mysql/#schedule", 
            "text": "If you want automated execution of Bipost Sync, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/mysql/#check-for-updates", 
            "text": "New versions of Bipost Sync can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/mysql/#sync-multiple-databases", 
            "text": "If you are going to synchronize two or more databases from the same Windows host, create separate Bipost Sync folders for each database. Then customize each folder with the desired data set as explained  here.", 
            "title": "Sync multiple databases"
        }, 
        {
            "location": "/mysql/#contact", 
            "text": "Questions? Send us an email:  info@factorbi.com", 
            "title": "Contact"
        }, 
        {
            "location": "/customdatajson/", 
            "text": "customData.json\n\n\nThis file is where you write SQL querys to specify tables, fields and filters that will be executed on the extraction process to the on-prem database.\n\n\nUse query syntax according your database engine.\n\n\nExample 1, tables with 100,000 rows or less:\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"CentroCostos\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"MovTipo\",\n        \"fields\": \"Modulo, Mov, Clave\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Cta\",\n        \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]\n\n\n\n\n\n\n\nThe above JSON will send all data for the specified tables, as \njoin\n and \nfilter\n are not in use.\n\n\nLeaving \n\"active\": \"\",\n empty means \nfalse\n.\n\n\n\n\n\n\nExample 2, dynamic filter using the current time of your database.\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"Cte\",\n        \"fields\": \"Cte.Cliente, Cte.Nombre, Cte.Tipo, Cte.Categoria, Cte.Estatus, Cte.Estado, Cte.Pais\",\n        \"join\": \"Venta WITH (NOLOCK) ON Cte.Cliente = Venta.Cliente\",\n        \"filter\": \"CONVERT(date,Venta.UltimoCambio) BETWEEN DATEADD(day, -5, CONVERT(date,getdate())) AND CONVERT(date,getdate())\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]\n\n\n\n\n\njoin\n, \nfilter\n and \norder\n can use any syntax supported by your database engine.\n\n\n\n\nExample 3, using order and limit:\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_ENTRE_SIS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"DOCTO_DEST_ID DESC\",\n        \"limit\": \"50000\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]\n\n\n\n\n\nTables \n Primary Keys\n\n\nTake note of the following:\n\n\n\n\nYou can synchronize tables or views.\n\n\nTables must have a \nPrimary Key\n and these must be listed on \n\"fields\"\n\n\nIf a table does not have a Primary Key then use \ncustomSchema.json\n to simulate one.\n\n\nThe above also applies to Views.\n\n\n\"fields\"\n parameter must \nonly\n include fields that exist on \n\"table\":\n\n\n\n\n\n\nRecursive Synchronization\n\n\nWhen this parameter is enabled the execution will extract one day at a time on a cycle, so it will make one upload to AWS for every day in the specified date range.\n\n\nThis feature is very useful to optimize the uploading process to Aurora-MySQL, as it is not recommended to upload more than 1.5 million rows on a single execution.\n\n\n\n\nGiven \nFrom Date:\n \nJuly 01, 2018\n   and   \nTo Date:\n \nJuly 31, 2018\n, the execution will make 31 uploads to AWS. On customData.json \nVenta.FechaEmision\n is used to parse these dates, example:\n\n\n[\n    {\n        \"active\": \"true\",\n        \"table\": \"Venta\",\n        \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n        \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"VentaD\",\n        \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente\",\n        \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n        \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]\n\n\n\nIt is suitable to remove time part from the field used inside \n\"recursiveDateField\":\n.\n\n\n\n\ncustomSchema.json\n\n\nUse this file when tables \ndo not\n have PRIMARY KEY or when syncing views.\n\n\nExample:\n\n\n[\n    {\n        \"table\": \"CASHOUT\",\n        \"primaryKey\": [\"store\", \"date\", \"shift\", \"sh_start\", \"sh_end\", \"co_type\", \"co_number\", \"drawer\", \"cost_centr\"]\n    },\n    {\n        \"table\": \"CASHOUT2\",\n        \"primaryKey\": [\"store\", \"date\", \"shift\", \"sh_start\", \"sh_end\", \"co_type\", \"co_number\", \"drawer\", \"cost_centr\"],\n        \"notes\": \"code comment\"\n    },\n    {\n        \"table\": \"CATSALES\",\n        \"primaryKey\": [\"store\", \"inv_number\"]\n    },\n    {\n        \"table\": \"CHKHDR\",\n        \"primaryKey\": [\"store\", \"date\", \"check_num\", \"disc_num\"]\n    },\n    {\n        \"table\": \"CHKITEMS\",\n        \"primaryKey\": [\"store\", \"date\", \"check_num\", \"seq_main\", \"option\", \"item_num\"]\n    },\n    {\n        \"table\": \"CONTROL\",\n        \"primaryKey\": [\"fiscl_year\"]\n    },\n    {\n        \"table\": \"FCOSTN\",\n        \"primaryKey\": [\"store\", \"inv_number\"]\n    },\n    {\n        \"table\": \"\",\n        \"primaryKey\": [\"\", \"\"]\n    }\n]\n\n\n\n\n\ncomment1, comment2\n\n\nThese two parameters can be used as \"tag\" instructions available before and after data is loaded to Aurora-MySQL.\n\n\nA database named \nbipost_system\n is created on Aurora-MySQL and has information about every sync process. \nbipost_sync_table\n stores comment1 and comment2 for ever table and sync execution.\n\n\nMake the following query to know what kind of information is stored:\n\n\nSELECT * FROM `bipost_system`.bipost_sync_info ORDER BY id DESC LIMIT 100;\nSELECT * FROM `bipost_system`.bipost_sync_table ORDER BY id DESC, rid LIMIT 100;\n\n\n\nSee examples of how to use comment1 and comment2 before data is loaded \n==\n here.\n\n\n\n\nSync for multiple scenarios\n\n\nIt is very common to make changes to customData.json to upload different sets of data.\n\n\nUse Cases:\n\n\n\n\n\n\nSome tables may be uploaded once since that data is rarely changed, e.g. config \n company tables.\n\n\n\n\n\n\nHistoric data may be uploaded once, e.g. transactions from previous years.\n\n\n\n\n\n\nRecently created/modified data may be uploaded daily, e.g. invoices, quotes, purchase orders, customers, items, vendors, etc.\n\n\n\n\n\n\nFor all these reasons it may be useful to make copies of Bipost Sync folder and just change \ncustomData.json.\n Moreover you may want to have different sync schedules.", 
            "title": "Tables & Data to Sync"
        }, 
        {
            "location": "/customdatajson/#customdatajson", 
            "text": "This file is where you write SQL querys to specify tables, fields and filters that will be executed on the extraction process to the on-prem database.  Use query syntax according your database engine.  Example 1, tables with 100,000 rows or less:  [\n    {\n        \"active\": \"true\",\n        \"table\": \"CentroCostos\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"MovTipo\",\n        \"fields\": \"Modulo, Mov, Clave\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"Cta\",\n        \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]    The above JSON will send all data for the specified tables, as  join  and  filter  are not in use.  Leaving  \"active\": \"\",  empty means  false .    Example 2, dynamic filter using the current time of your database.  [\n    {\n        \"active\": \"true\",\n        \"table\": \"Cte\",\n        \"fields\": \"Cte.Cliente, Cte.Nombre, Cte.Tipo, Cte.Categoria, Cte.Estatus, Cte.Estado, Cte.Pais\",\n        \"join\": \"Venta WITH (NOLOCK) ON Cte.Cliente = Venta.Cliente\",\n        \"filter\": \"CONVERT(date,Venta.UltimoCambio) BETWEEN DATEADD(day, -5, CONVERT(date,getdate())) AND CONVERT(date,getdate())\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]   join ,  filter  and  order  can use any syntax supported by your database engine.   Example 3, using order and limit:  [\n    {\n        \"active\": \"true\",\n        \"table\": \"DOCTOS_ENTRE_SIS\",\n        \"fields\": \"*\",\n        \"join\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\",\n        \"order\": \"DOCTO_DEST_ID DESC\",\n        \"limit\": \"50000\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]", 
            "title": "customData.json"
        }, 
        {
            "location": "/customdatajson/#tables-primary-keys", 
            "text": "Take note of the following:   You can synchronize tables or views.  Tables must have a  Primary Key  and these must be listed on  \"fields\"  If a table does not have a Primary Key then use  customSchema.json  to simulate one.  The above also applies to Views.  \"fields\"  parameter must  only  include fields that exist on  \"table\":", 
            "title": "Tables &amp; Primary Keys"
        }, 
        {
            "location": "/customdatajson/#recursive-synchronization", 
            "text": "When this parameter is enabled the execution will extract one day at a time on a cycle, so it will make one upload to AWS for every day in the specified date range.  This feature is very useful to optimize the uploading process to Aurora-MySQL, as it is not recommended to upload more than 1.5 million rows on a single execution.   Given  From Date:   July 01, 2018    and    To Date:   July 31, 2018 , the execution will make 31 uploads to AWS. On customData.json  Venta.FechaEmision  is used to parse these dates, example:  [\n    {\n        \"active\": \"true\",\n        \"table\": \"Venta\",\n        \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n        \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n        \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"VentaD\",\n        \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente\",\n        \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS'\",\n        \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC')\",\n        \"recursiveDateField\": \"Venta.FechaEmision\",\n        \"order\": \"\",\n        \"limit\": \"\",\n        \"comment1\": \"\",\n        \"comment2\": \"\"\n    }\n]  It is suitable to remove time part from the field used inside  \"recursiveDateField\": .", 
            "title": "Recursive Synchronization"
        }, 
        {
            "location": "/customdatajson/#customschemajson", 
            "text": "Use this file when tables  do not  have PRIMARY KEY or when syncing views.  Example:  [\n    {\n        \"table\": \"CASHOUT\",\n        \"primaryKey\": [\"store\", \"date\", \"shift\", \"sh_start\", \"sh_end\", \"co_type\", \"co_number\", \"drawer\", \"cost_centr\"]\n    },\n    {\n        \"table\": \"CASHOUT2\",\n        \"primaryKey\": [\"store\", \"date\", \"shift\", \"sh_start\", \"sh_end\", \"co_type\", \"co_number\", \"drawer\", \"cost_centr\"],\n        \"notes\": \"code comment\"\n    },\n    {\n        \"table\": \"CATSALES\",\n        \"primaryKey\": [\"store\", \"inv_number\"]\n    },\n    {\n        \"table\": \"CHKHDR\",\n        \"primaryKey\": [\"store\", \"date\", \"check_num\", \"disc_num\"]\n    },\n    {\n        \"table\": \"CHKITEMS\",\n        \"primaryKey\": [\"store\", \"date\", \"check_num\", \"seq_main\", \"option\", \"item_num\"]\n    },\n    {\n        \"table\": \"CONTROL\",\n        \"primaryKey\": [\"fiscl_year\"]\n    },\n    {\n        \"table\": \"FCOSTN\",\n        \"primaryKey\": [\"store\", \"inv_number\"]\n    },\n    {\n        \"table\": \"\",\n        \"primaryKey\": [\"\", \"\"]\n    }\n]", 
            "title": "customSchema.json"
        }, 
        {
            "location": "/customdatajson/#comment1-comment2", 
            "text": "These two parameters can be used as \"tag\" instructions available before and after data is loaded to Aurora-MySQL.  A database named  bipost_system  is created on Aurora-MySQL and has information about every sync process.  bipost_sync_table  stores comment1 and comment2 for ever table and sync execution.  Make the following query to know what kind of information is stored:  SELECT * FROM `bipost_system`.bipost_sync_info ORDER BY id DESC LIMIT 100;\nSELECT * FROM `bipost_system`.bipost_sync_table ORDER BY id DESC, rid LIMIT 100;  See examples of how to use comment1 and comment2 before data is loaded  ==  here.", 
            "title": "comment1, comment2"
        }, 
        {
            "location": "/customdatajson/#sync-for-multiple-scenarios", 
            "text": "It is very common to make changes to customData.json to upload different sets of data.  Use Cases:    Some tables may be uploaded once since that data is rarely changed, e.g. config   company tables.    Historic data may be uploaded once, e.g. transactions from previous years.    Recently created/modified data may be uploaded daily, e.g. invoices, quotes, purchase orders, customers, items, vendors, etc.    For all these reasons it may be useful to make copies of Bipost Sync folder and just change  customData.json.  Moreover you may want to have different sync schedules.", 
            "title": "Sync for multiple scenarios"
        }, 
        {
            "location": "/bipostapi/", 
            "text": "Working with Aurora-MySQL\n\n\nEvery time Bipost Sync sends data to AWS it works along with an API that runs 4 steps.\n\n\n1. Create-Alter Schemas\n\n\n\n\n\n\nIf it doesn't exist, database is created with:\n\n\n\n\n\n\nName: The one you specified on \nFactor BI Console.\n\n\n\n\n\n\nEncoding: \ncp1252 West European (latin1)\n\n\n\n\n\n\nCollation: \nlatin1_spanish_ci\n\n\n\n\n\n\n\n\n\n\nTables are created with all the columns found on source db.\n\n\n\n\n\n\nAlter tables to match source schemas. Columns are never deleted.\n\n\n\n\n\n\nOnly fields specified in \ncustomData.json\n will be populated.\n\n\n\n\n\n\n\n\n2. Prepare Database Before Loading\n\n\nAfter schemas are created/altered and \nbefore\n new data is loaded, you can run a stored procedure.\n\n\nThis is very useful if you need to delete, truncate or make any changes before data is loaded.\n\n\nInclude all desired statements in a stored procedure named \nspPostInitial\n.\n\n\n\n\n_serviceID\n and \n_syncID\n variables are automatically delivered from the API.\n\n\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`(\n  _serviceID varchar(36),\n  _syncID int\n  )\npostinitial:BEGIN\n\n  DECLARE _SQL  longtext;\n  DECLARE _timestamp datetime;\n  DECLARE _syncDate date;\n  DECLARE _timezone varchar(64);\n  DECLARE _rid, _prevId, _count_bipost_sync_info, _count_table int;\n  DECLARE _numDays decimal(10,0);\n  DECLARE _tableName, _comment1, _comment2 varchar(255);\n\n  SET lc_time_names = 'en_US';\n  SET group_concat_max_len = 4294967295;\n  SET _count_bipost_sync_info = 0;\n\n  SELECT COUNT(*) INTO _count_bipost_sync_info FROM `bipost_system`.bipost_sync_info WHERE id = _syncID AND serviceID = _serviceID;\n\n  IF IFNULL(_count_bipost_sync_info,0) = 0 THEN\n    INSERT INTO logPostInitial (message, serviceID, syncID) VALUES ('execution parameters do not match', _serviceID, _syncID);\n    SELECT 'execution parameters do not match' AS message;\n    LEAVE postinitial;\n  END IF;\n\n  SELECT NULLIF(TRIM(timezone),'') INTO _timezone FROM syncInfoStores WHERE serviceID = _serviceID;\n  IF _timezone IS NULL THEN\n    SELECT IFNULL(NULLIF(TRIM(timezone),''),'US/Eastern') INTO _timezone FROM syncInfo WHERE serviceID = _serviceID;\n  END IF;\n  SELECT CONVERT_TZ(syncDate, 'UTC', _timezone) INTO _timestamp FROM `bipost_system`.bipost_sync_info WHERE id = _syncID;\n  SELECT IFNULL(CAST(_timestamp AS date),'0000-00-00 00:00:00.0000') INTO _syncDate;\n\n  IF _syncDate \n '0000-00-00 00:00:00.0000' THEN\n\n    INSERT INTO logPostInitial (message, serviceID, syncID) VALUES ('ok', _serviceID, _syncID);\n\n    SET _prevId = 0, _count_table = 0;\n\n    table_id: WHILE(1=1) DO\n      SELECT MIN(rid)\n        INTO _rid\n        FROM `bipost_system`.bipost_sync_table\n        WHERE rid \n _prevId AND id = _syncID AND comment1 = '-1';\n\n      IF _rid IS NULL THEN\n        LEAVE table_id;\n      END IF;\n\n      SELECT tableName, comment1\n        INTO _tableName, _comment1\n        FROM `bipost_system`.bipost_sync_table\n        WHERE rid = _rid;\n\n      SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n      IF _count_table \n 0 THEN\n        SET _SQL = CONCAT(\n          'TRUNCATE TABLE `', _tableName, '`;');\n        SET @SQL = _SQL; PREPARE stmt3 FROM @SQL; EXECUTE stmt3; DEALLOCATE PREPARE stmt3; SET _SQL = NULL;\n      END IF;\n\n      SET _prevId = _rid, _count_table = 0;\n    END WHILE;\n\n  #***************** Delete child tables first *****************\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'claves_articulos';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays \n 0 AND _count_table \n 0 THEN\n      DELETE claves_articulos\n        FROM claves_articulos\n        JOIN articulos ON claves_articulos.ARTICULO_ID = articulos.ARTICULO_ID\n        WHERE ((CAST(articulos.FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(articulos.FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'dirs_clientes';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays \n 0 AND _count_table \n 0 THEN\n      DELETE dirs_clientes\n        FROM dirs_clientes\n        JOIN clientes ON dirs_clientes.CLIENTE_ID = clientes.CLIENTE_ID\n        WHERE ((CAST(clientes.FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(clientes.FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n  #***************** Delete header/parent tables *****************\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'articulos';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays \n 0 AND _count_table \n 0 THEN\n      DELETE FROM articulos\n        WHERE ((CAST(FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'clientes';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays \n 0 AND _count_table \n 0 THEN\n      DELETE FROM clientes\n        WHERE ((CAST(FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n  #***************** Tables with special comments *****************\n    SET _numDays = 0, _count_table = 0, _tableName = 'saldos_co';\n    SELECT comment2 INTO _comment2 FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _comment2 = 'last year + ytd' AND _count_table \n 0 THEN\n      DELETE FROM saldos_co\n        WHERE ANO \n= YEAR(_syncDate)-1;\n    END IF;\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'saldos_in';\n    SELECT comment2 INTO _comment2 FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _comment2 = 'last month + mtd' AND _count_table \n 0 THEN\n      DELETE FROM saldos_in\n        WHERE ANO = YEAR(_syncDate) AND MES IN (MONTH(_syncDate)-1, MONTH(_syncDate));\n    END IF;\n\n  END IF;\n\nEND$$\nDELIMITER ;\n\n\n\n\n\n3. Load Data\n\n\nData loading is performed by Aurora. Matching primary keys rows are updated and the rest are inserted.\n\n\nYou can verify which tables where loaded by querying \naurora_s3_load_history\n table like this:\n\n\nSELECT \n  LOWER(REPLACE(RIGHT(e.file_name,LOCATE('/',REVERSE(e.file_name))-1),'.csv','')) AS \"table\",\n  CONVERT_TZ(load_timestamp,'UTC','US/Eastern') AS \"local_load_timestamp\",\n  e.load_timestamp,\n  SUBSTRING(e.load_prefix, LOCATE('bipostdata',e.load_prefix), LOCATE('/',e.load_prefix, LOCATE('bipostdata',e.load_prefix)+1)-LOCATE('bipostdata',e.load_prefix)) AS \"bucket\",\n  LEFT(e.file_name,LOCATE('/', e.file_name)-1) AS \"serviceNumber\"\nFROM mysql.aurora_s3_load_history AS e\nWHERE \n--  e.file_name REGEXP 'your-service-numer-bc9a-c0123def4567'\n  file_name REGEXP 'mytable.csv'\n--  AND e.load_timestamp\n'2019-01-01 09:41:41'\nORDER BY e.load_timestamp DESC LIMIT 100 ;\n\n\n\n\n\n4. Transform Data After Loading\n\n\nAfter new data is loaded to Aurora-MySQL and \nbefore\n it begins the downloading process to your on-prem, you can run a stored procedure.\n\n\nThis is very useful to analyze and populate new tables.\n\n\nIt is also useful to prepare tables with data sets for the downloading process back to on-premises.\n\n\nInclude all desired statements in a stored procedure named \nspPostFinal\n.\n\n\n\n\n_serviceID\n and \n_syncID\n variables are automatically delivered from the API.\n\n\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`(\n  _serviceID varchar(36),\n  _syncID int\n  )\npostfinal:BEGIN\n\n  DECLARE _timestamp datetime;\n  DECLARE _syncDate date;\n  DECLARE _timezone varchar(64);\n  DECLARE _count_bipost_sync_info int;\n\n  SET lc_time_names = 'en_US';\n  SET group_concat_max_len = 4294967295;\n\n  SELECT COUNT(*) INTO _count_bipost_sync_info FROM `bipost_system`.bipost_sync_info WHERE id = _syncID AND serviceID = _serviceID;\n\n  IF _count_bipost_sync_info = 0 THEN\n    INSERT INTO logPostFinal (message, serviceID, syncID) VALUES ('execution parameters do not match', _serviceID, _syncID);\n    SELECT 'execution parameters do not match' AS message;\n    LEAVE postfinal;\n  END IF;\n\n  SELECT NULLIF(TRIM(timezone),'') INTO _timezone FROM syncInfoStores WHERE serviceID = _serviceID;\n  IF _timezone IS NULL THEN\n    SELECT IFNULL(NULLIF(TRIM(timezone),''),'US/Eastern') INTO _timezone FROM syncInfo WHERE serviceID = _serviceID;\n  END IF;\n  SELECT CONVERT_TZ(syncDate, 'UTC', _timezone) INTO _timestamp FROM `bipost_system`.bipost_sync_info WHERE id = _syncID;\n  SELECT IFNULL(CAST(_timestamp AS date),'0000-00-00 00:00:00.0000') INTO _syncDate;\n\n  UPDATE syncInfo\n    SET syncDate = _syncDate, `timestamp` = _timestamp, lastSyncId = _syncID\n    WHERE serviceID = _serviceID;\n\n  call spReport1(_serviceID);\n  call spReport2(_serviceID);\n\n  INSERT INTO logPostFinal (message, serviceID, syncID) VALUES ('ok', _serviceID, _syncID);\n  SELECT 'ok' AS message;\n\nEND$$\nDELIMITER ;\n\n\n\n\n\nExecution information\n\n\nA database named \nbipost_system\n is created on Aurora-MySQL and has information about every sync process. \nbipost_sync_table\n stores comment1 and comment2 for ever table and sync execution.\n\n\nMake the following query to know what kind of information is stored:\n\n\nSELECT * FROM `bipost_system`.bipost_sync_info ORDER BY id DESC LIMIT 100;\nSELECT * FROM `bipost_system`.bipost_sync_table ORDER BY id DESC, rid LIMIT 100;\n\n\n\n\n\nMore examples\n\n\nCheck more transformation examples on \nthis repository.", 
            "title": "Loading and Transformation"
        }, 
        {
            "location": "/bipostapi/#working-with-aurora-mysql", 
            "text": "Every time Bipost Sync sends data to AWS it works along with an API that runs 4 steps.", 
            "title": "Working with Aurora-MySQL"
        }, 
        {
            "location": "/bipostapi/#1-create-alter-schemas", 
            "text": "If it doesn't exist, database is created with:    Name: The one you specified on  Factor BI Console.    Encoding:  cp1252 West European (latin1)    Collation:  latin1_spanish_ci      Tables are created with all the columns found on source db.    Alter tables to match source schemas. Columns are never deleted.    Only fields specified in  customData.json  will be populated.", 
            "title": "1. Create-Alter Schemas"
        }, 
        {
            "location": "/bipostapi/#2-prepare-database-before-loading", 
            "text": "After schemas are created/altered and  before  new data is loaded, you can run a stored procedure.  This is very useful if you need to delete, truncate or make any changes before data is loaded.  Include all desired statements in a stored procedure named  spPostInitial .   _serviceID  and  _syncID  variables are automatically delivered from the API.   Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`(\n  _serviceID varchar(36),\n  _syncID int\n  )\npostinitial:BEGIN\n\n  DECLARE _SQL  longtext;\n  DECLARE _timestamp datetime;\n  DECLARE _syncDate date;\n  DECLARE _timezone varchar(64);\n  DECLARE _rid, _prevId, _count_bipost_sync_info, _count_table int;\n  DECLARE _numDays decimal(10,0);\n  DECLARE _tableName, _comment1, _comment2 varchar(255);\n\n  SET lc_time_names = 'en_US';\n  SET group_concat_max_len = 4294967295;\n  SET _count_bipost_sync_info = 0;\n\n  SELECT COUNT(*) INTO _count_bipost_sync_info FROM `bipost_system`.bipost_sync_info WHERE id = _syncID AND serviceID = _serviceID;\n\n  IF IFNULL(_count_bipost_sync_info,0) = 0 THEN\n    INSERT INTO logPostInitial (message, serviceID, syncID) VALUES ('execution parameters do not match', _serviceID, _syncID);\n    SELECT 'execution parameters do not match' AS message;\n    LEAVE postinitial;\n  END IF;\n\n  SELECT NULLIF(TRIM(timezone),'') INTO _timezone FROM syncInfoStores WHERE serviceID = _serviceID;\n  IF _timezone IS NULL THEN\n    SELECT IFNULL(NULLIF(TRIM(timezone),''),'US/Eastern') INTO _timezone FROM syncInfo WHERE serviceID = _serviceID;\n  END IF;\n  SELECT CONVERT_TZ(syncDate, 'UTC', _timezone) INTO _timestamp FROM `bipost_system`.bipost_sync_info WHERE id = _syncID;\n  SELECT IFNULL(CAST(_timestamp AS date),'0000-00-00 00:00:00.0000') INTO _syncDate;\n\n  IF _syncDate   '0000-00-00 00:00:00.0000' THEN\n\n    INSERT INTO logPostInitial (message, serviceID, syncID) VALUES ('ok', _serviceID, _syncID);\n\n    SET _prevId = 0, _count_table = 0;\n\n    table_id: WHILE(1=1) DO\n      SELECT MIN(rid)\n        INTO _rid\n        FROM `bipost_system`.bipost_sync_table\n        WHERE rid   _prevId AND id = _syncID AND comment1 = '-1';\n\n      IF _rid IS NULL THEN\n        LEAVE table_id;\n      END IF;\n\n      SELECT tableName, comment1\n        INTO _tableName, _comment1\n        FROM `bipost_system`.bipost_sync_table\n        WHERE rid = _rid;\n\n      SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n      IF _count_table   0 THEN\n        SET _SQL = CONCAT(\n          'TRUNCATE TABLE `', _tableName, '`;');\n        SET @SQL = _SQL; PREPARE stmt3 FROM @SQL; EXECUTE stmt3; DEALLOCATE PREPARE stmt3; SET _SQL = NULL;\n      END IF;\n\n      SET _prevId = _rid, _count_table = 0;\n    END WHILE;\n\n  #***************** Delete child tables first *****************\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'claves_articulos';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays   0 AND _count_table   0 THEN\n      DELETE claves_articulos\n        FROM claves_articulos\n        JOIN articulos ON claves_articulos.ARTICULO_ID = articulos.ARTICULO_ID\n        WHERE ((CAST(articulos.FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(articulos.FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'dirs_clientes';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays   0 AND _count_table   0 THEN\n      DELETE dirs_clientes\n        FROM dirs_clientes\n        JOIN clientes ON dirs_clientes.CLIENTE_ID = clientes.CLIENTE_ID\n        WHERE ((CAST(clientes.FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(clientes.FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n  #***************** Delete header/parent tables *****************\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'articulos';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays   0 AND _count_table   0 THEN\n      DELETE FROM articulos\n        WHERE ((CAST(FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'clientes';\n    SELECT CAST(comment1 AS decimal(10,0))*(-1) INTO _numDays FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _numDays   0 AND _count_table   0 THEN\n      DELETE FROM clientes\n        WHERE ((CAST(FECHA_HORA_CREACION AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate) OR (CAST(FECHA_HORA_ULT_MODIF AS date) BETWEEN date_add(_syncDate, INTERVAL _numDays day) AND _syncDate));\n    END IF;\n\n  #***************** Tables with special comments *****************\n    SET _numDays = 0, _count_table = 0, _tableName = 'saldos_co';\n    SELECT comment2 INTO _comment2 FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _comment2 = 'last year + ytd' AND _count_table   0 THEN\n      DELETE FROM saldos_co\n        WHERE ANO  = YEAR(_syncDate)-1;\n    END IF;\n\n    SET _numDays = 0, _count_table = 0, _tableName = 'saldos_in';\n    SELECT comment2 INTO _comment2 FROM `bipost_system`.bipost_sync_table WHERE id = _syncID AND tableName = _tableName;\n    SELECT COUNT(*) INTO _count_table FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_name = _tableName AND table_schema = schema();\n    IF _comment2 = 'last month + mtd' AND _count_table   0 THEN\n      DELETE FROM saldos_in\n        WHERE ANO = YEAR(_syncDate) AND MES IN (MONTH(_syncDate)-1, MONTH(_syncDate));\n    END IF;\n\n  END IF;\n\nEND$$\nDELIMITER ;", 
            "title": "2. Prepare Database Before Loading"
        }, 
        {
            "location": "/bipostapi/#3-load-data", 
            "text": "Data loading is performed by Aurora. Matching primary keys rows are updated and the rest are inserted.  You can verify which tables where loaded by querying  aurora_s3_load_history  table like this:  SELECT \n  LOWER(REPLACE(RIGHT(e.file_name,LOCATE('/',REVERSE(e.file_name))-1),'.csv','')) AS \"table\",\n  CONVERT_TZ(load_timestamp,'UTC','US/Eastern') AS \"local_load_timestamp\",\n  e.load_timestamp,\n  SUBSTRING(e.load_prefix, LOCATE('bipostdata',e.load_prefix), LOCATE('/',e.load_prefix, LOCATE('bipostdata',e.load_prefix)+1)-LOCATE('bipostdata',e.load_prefix)) AS \"bucket\",\n  LEFT(e.file_name,LOCATE('/', e.file_name)-1) AS \"serviceNumber\"\nFROM mysql.aurora_s3_load_history AS e\nWHERE \n--  e.file_name REGEXP 'your-service-numer-bc9a-c0123def4567'\n  file_name REGEXP 'mytable.csv'\n--  AND e.load_timestamp '2019-01-01 09:41:41'\nORDER BY e.load_timestamp DESC LIMIT 100 ;", 
            "title": "3. Load Data"
        }, 
        {
            "location": "/bipostapi/#4-transform-data-after-loading", 
            "text": "After new data is loaded to Aurora-MySQL and  before  it begins the downloading process to your on-prem, you can run a stored procedure.  This is very useful to analyze and populate new tables.  It is also useful to prepare tables with data sets for the downloading process back to on-premises.  Include all desired statements in a stored procedure named  spPostFinal .   _serviceID  and  _syncID  variables are automatically delivered from the API.   Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`(\n  _serviceID varchar(36),\n  _syncID int\n  )\npostfinal:BEGIN\n\n  DECLARE _timestamp datetime;\n  DECLARE _syncDate date;\n  DECLARE _timezone varchar(64);\n  DECLARE _count_bipost_sync_info int;\n\n  SET lc_time_names = 'en_US';\n  SET group_concat_max_len = 4294967295;\n\n  SELECT COUNT(*) INTO _count_bipost_sync_info FROM `bipost_system`.bipost_sync_info WHERE id = _syncID AND serviceID = _serviceID;\n\n  IF _count_bipost_sync_info = 0 THEN\n    INSERT INTO logPostFinal (message, serviceID, syncID) VALUES ('execution parameters do not match', _serviceID, _syncID);\n    SELECT 'execution parameters do not match' AS message;\n    LEAVE postfinal;\n  END IF;\n\n  SELECT NULLIF(TRIM(timezone),'') INTO _timezone FROM syncInfoStores WHERE serviceID = _serviceID;\n  IF _timezone IS NULL THEN\n    SELECT IFNULL(NULLIF(TRIM(timezone),''),'US/Eastern') INTO _timezone FROM syncInfo WHERE serviceID = _serviceID;\n  END IF;\n  SELECT CONVERT_TZ(syncDate, 'UTC', _timezone) INTO _timestamp FROM `bipost_system`.bipost_sync_info WHERE id = _syncID;\n  SELECT IFNULL(CAST(_timestamp AS date),'0000-00-00 00:00:00.0000') INTO _syncDate;\n\n  UPDATE syncInfo\n    SET syncDate = _syncDate, `timestamp` = _timestamp, lastSyncId = _syncID\n    WHERE serviceID = _serviceID;\n\n  call spReport1(_serviceID);\n  call spReport2(_serviceID);\n\n  INSERT INTO logPostFinal (message, serviceID, syncID) VALUES ('ok', _serviceID, _syncID);\n  SELECT 'ok' AS message;\n\nEND$$\nDELIMITER ;", 
            "title": "4. Transform Data After Loading"
        }, 
        {
            "location": "/bipostapi/#execution-information", 
            "text": "A database named  bipost_system  is created on Aurora-MySQL and has information about every sync process.  bipost_sync_table  stores comment1 and comment2 for ever table and sync execution.  Make the following query to know what kind of information is stored:  SELECT * FROM `bipost_system`.bipost_sync_info ORDER BY id DESC LIMIT 100;\nSELECT * FROM `bipost_system`.bipost_sync_table ORDER BY id DESC, rid LIMIT 100;", 
            "title": "Execution information"
        }, 
        {
            "location": "/bipostapi/#more-examples", 
            "text": "Check more transformation examples on  this repository.", 
            "title": "More examples"
        }, 
        {
            "location": "/synctowindows/", 
            "text": "Download Data to On-Prem\n\n\nIn some cases Aurora-MySQL is used to make transactions with web applications. In this case it is useful to download data sets from Aurora-MySQL to on-premises.\n\n\n\n\n\n\n\n\nWith this option you are able to download data from Aurora to on-premises Windows.\n\n\n\n\n\n\nData to retrieve from Aurora is specified on \noutData.json\n\n\n\n\n\n\nProcess Data is used to update/insert returned data to SQL Server or Firebird SQL. This option is not supported for DBF files.\n\n\n\n\n\n\n\n\noutData.json\n\n\n\n\n\n\nWith this file you are able to specify tables, fields and filter criteria to extract data from Aurora.\n\n\n\n\n\n\nMySQL tables are case-sensitive, so this is important on \n\"table\":\n parameter.\n\n\n\n\n\n\nDownloaded data will be available on \n%localappdata%/biPost/out_\n Windows folder.\n\n\n\n\n\n\nrecursiveDateField\n parameter not supported.\n\n\n\n\n\n\nWe highly recommend to prepare your data set on separate non-transactional tables, and process them with \nBipost API Final Statement.\n\n\nExample 1, using outData.json\n\n\n{\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}\n\n\n\nExample 2, using outData.json\n\n\n{\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}\n\n\n\n\n\nNote that you can leave \n\"active\": \"\",\n empty.\n\n\n\n\nExample of downloaded data on \n%localappdata%/bipost\n folder:\n\n\n\n\n\n\nProcess Data\n\n\n\n\n\n\nWhen this option is on, table schema's are created/altered and data uploaded to your SQL Server or Firebird SQL.\n\n\n\n\n\n\nSame names of output tables on Aurora are created on SQL Server/Firebird.\n\n\n\n\n\n\nWe highly recommend to create specific output tables on Aurora and give them a name that is not in use on your SQL Server/Firebird database.\n\n\n\n\n\n\nPrimary keys on Aurora are used on SQL Server/Firebird to avoid duplicates.\n\n\n\n\n\n\nYou are able to query views from Aurora as output. In this case tables on SQL Server/Firebird are always deleted before any new data load.\n\n\n\n\n\n\nSchema changes on Aurora are applied to SQL Server/Firebird, except for deleting and renaming columns.\n\n\n\n\n\n\nRun queries before and after data is loaded to SQL Server/Firebird, more instructions below.\n\n\n\n\n\n\nFirebird SQL does not naturally support creating a column name starting with underscore,\n so avoid that on Aurora if your on-prem DB is Firebird.\n\n\n\n\n\n\nExample of processed data: tables were created and data loaded.\n\n\n\n\n\n\nInitial and Final Query\n\n\nBefore and after data is loaded to SQL Server/Firebird you're able to run queries, example:\n\n\n{\n      \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n      \"initialQuery\": \"execute procedure spInitial;\",\n      \"finalQuery\": \"execute procedure spFinal;\"\n}\n\n\n\ninitialQuery\n and \nfinalQuery\n are used to transform your data in any way you want.", 
            "title": "Two-way Synchronization"
        }, 
        {
            "location": "/synctowindows/#download-data-to-on-prem", 
            "text": "In some cases Aurora-MySQL is used to make transactions with web applications. In this case it is useful to download data sets from Aurora-MySQL to on-premises.     With this option you are able to download data from Aurora to on-premises Windows.    Data to retrieve from Aurora is specified on  outData.json    Process Data is used to update/insert returned data to SQL Server or Firebird SQL. This option is not supported for DBF files.", 
            "title": "Download Data to On-Prem"
        }, 
        {
            "location": "/synctowindows/#outdatajson", 
            "text": "With this file you are able to specify tables, fields and filter criteria to extract data from Aurora.    MySQL tables are case-sensitive, so this is important on  \"table\":  parameter.    Downloaded data will be available on  %localappdata%/biPost/out_  Windows folder.    recursiveDateField  parameter not supported.    We highly recommend to prepare your data set on separate non-transactional tables, and process them with  Bipost API Final Statement.  Example 1, using outData.json  {\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_ve_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"true\",\n        \"table\": \"export_doctos_cm_det\",\n        \"fields\": \"*\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}  Example 2, using outData.json  {\n    \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    },\n    {\n        \"active\": \"\",\n        \"table\": \"\",\n        \"fields\": \"\",\n        \"filter\": \"\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n    \"initialQuery\": \"\",\n    \"finalQuery\": \"\"\n}   Note that you can leave  \"active\": \"\",  empty.   Example of downloaded data on  %localappdata%/bipost  folder:", 
            "title": "outData.json"
        }, 
        {
            "location": "/synctowindows/#process-data", 
            "text": "When this option is on, table schema's are created/altered and data uploaded to your SQL Server or Firebird SQL.    Same names of output tables on Aurora are created on SQL Server/Firebird.    We highly recommend to create specific output tables on Aurora and give them a name that is not in use on your SQL Server/Firebird database.    Primary keys on Aurora are used on SQL Server/Firebird to avoid duplicates.    You are able to query views from Aurora as output. In this case tables on SQL Server/Firebird are always deleted before any new data load.    Schema changes on Aurora are applied to SQL Server/Firebird, except for deleting and renaming columns.    Run queries before and after data is loaded to SQL Server/Firebird, more instructions below.    Firebird SQL does not naturally support creating a column name starting with underscore,  so avoid that on Aurora if your on-prem DB is Firebird.    Example of processed data: tables were created and data loaded.", 
            "title": "Process Data"
        }, 
        {
            "location": "/synctowindows/#initial-and-final-query", 
            "text": "Before and after data is loaded to SQL Server/Firebird you're able to run queries, example:  {\n      \"out\": [\n    {\n        \"active\": \"true\",\n        \"table\": \"biArticulos\",\n        \"fields\": \"*\",\n        \"filter\": \"linea = 'TVs'\",\n        \"recursiveDateField\": \"\"\n    }\n    ],\n      \"initialQuery\": \"execute procedure spInitial;\",\n      \"finalQuery\": \"execute procedure spFinal;\"\n}  initialQuery  and  finalQuery  are used to transform your data in any way you want.", 
            "title": "Initial and Final Query"
        }, 
        {
            "location": "/aspel/", 
            "text": "Aspel SAE\n\n\n\n\nIntegramos Aspel con Amazon Web Services para crear tableros de mando con indicadores de gesti\u00f3n y seguimiento diario y desarrollar soluciones a la medida.\n\n\nConsolidamos tambi\u00e9n informaci\u00f3n de distintas sucursales que utilizan bases de datos separadas de Aspel, de razones sociales distintas y de la misma raz\u00f3n social.\n\n\nBuscas un Business Intelligence? Escr\u00edbemos! \ninfo@factorbi.com\n\n\nwww.factorbi.com\n\n\n\n\nDemo Business Intelligence\n\n\nTablero de Mando Aspel SAE\n\n\n\n\n\n\nConsolidaci\u00f3n\n\n\nConsolida informaci\u00f3n de Aspel de distintas razones sociales y sucursales que utilizan bases de datos independientes. Sube informaci\u00f3n de Excel como metas de ventas y presupuestos para realizar un an\u00e1lisis completo.\n\n\n\n\n\n\nExcel Tablas Aspel SAE\n\n\n Abre aqu\u00ed \nExcel tablas Aspel SAE.\n\n\nNOTA:\n La lista del link anterior puede no estar completa.\n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? Escr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Aspel"
        }, 
        {
            "location": "/aspel/#aspel-sae", 
            "text": "Integramos Aspel con Amazon Web Services para crear tableros de mando con indicadores de gesti\u00f3n y seguimiento diario y desarrollar soluciones a la medida.  Consolidamos tambi\u00e9n informaci\u00f3n de distintas sucursales que utilizan bases de datos separadas de Aspel, de razones sociales distintas y de la misma raz\u00f3n social.  Buscas un Business Intelligence? Escr\u00edbemos!  info@factorbi.com  www.factorbi.com", 
            "title": "Aspel SAE"
        }, 
        {
            "location": "/aspel/#demo-business-intelligence", 
            "text": "Tablero de Mando Aspel SAE", 
            "title": "Demo Business Intelligence"
        }, 
        {
            "location": "/aspel/#consolidacion", 
            "text": "Consolida informaci\u00f3n de Aspel de distintas razones sociales y sucursales que utilizan bases de datos independientes. Sube informaci\u00f3n de Excel como metas de ventas y presupuestos para realizar un an\u00e1lisis completo.", 
            "title": "Consolidaci\u00f3n"
        }, 
        {
            "location": "/aspel/#excel-tablas-aspel-sae", 
            "text": "Abre aqu\u00ed  Excel tablas Aspel SAE.  NOTA:  La lista del link anterior puede no estar completa.  Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Excel Tablas Aspel SAE"
        }, 
        {
            "location": "/aspel/#contacto", 
            "text": "\u00bfNecesitas ayuda? Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/microsip/", 
            "text": "Microsip ERP\n\n\n\n\nIntegramos Microsip ERP con Amazon Web Services para crear tableros de mando, realizar an\u00e1lisis contable y fiscal y desarrollar soluciones a la medida.\n\n\nConsolidamos tambi\u00e9n informaci\u00f3n de distintas sucursales que utilizan bases de datos separadas de Microsip, de razones sociales distintas y de la misma raz\u00f3n social (cuando no usas Replix). \n\n\nBuscas un Business Intelligence? Visitanos! \nwww.factorbi.com\n\n\nEscr\u00edbemos y te ayudamos en lo que necesitas: \ninfo@factorbi.com\n\n\n\n\nCasos de Uso\n\n\n\n\nDashboards Punto de Venta, Ventas, Pedidos, Cotizaciones, Remisiones.\n\n\nFlujo de Efectivo + Cuentas por Cobrar + Cuentas por Pagar.\n\n\nEstados Financieros Gr\u00e1ficos.\n\n\nConsolidaci\u00f3n de Empresas y Sucursales.\n\n\nContabilidad Avanzada.\n\n\nOrigen y Aplicaci\u00f3n de Recursos.\n\n\nTableros Compras e Inventarios.\n\n\n\n\nVer casos aqu\u00ed.\n\n\n\n\nDemo Business Intelligence\n\n\nTablero de Mando Ventas Microsip\n\n\n\n\n\n\n\n\nConsolidaci\u00f3n\n\n\nConsolida informaci\u00f3n de Microsip de distintas razones sociales y sucursales que utilizan bases de datos independientes. Sube informaci\u00f3n de Excel como metas de ventas y presupuestos para realizar un an\u00e1lisis completo.\n\n\n\n\n\n\nExcel Tablas Microsip\n\n\n Abre aqu\u00ed \nExcel tablas Microsip.\n\n\nNOTA:\n La lista del link anterior puede no estar completa.\n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nDescarga el Programa de Sincronizaci\u00f3n\n\n\n\n\n\n\nDescarga aqu\u00ed.\n\n\n\n\n\n\nEn tu servidor de Firebird crea una carpeta, por ejemplo \n C:\\Bipost\\ \n y copia los archivos.\n\n\n\n\n\n\n\n\nConfigura la Sincronizaci\u00f3n\n\n\nLa siguiente informaci\u00f3n es para Distribuidores Certificados de Factor BI.\n\n\nSi eres usuario de Microsip por favor env\u00edanos un correo y nos pondremos en contacto: \ninfo@factorbi.com\n\n\n1. Llaves de sincronizaci\u00f3n\n\n\nEntra a la \nConsola Factor BI\n y en el men\u00fa \nService Numbers\n copia \nService No.\n y \nActivation No.\n que correspondan con la base de datos a sincronizar.\n\n\n2. Configura biPost\n\n\nAbre biPost.exe y oprime \nConfiguration\n.\n\n\n\n\n\n\nService No.:\n Pega el texto que obtuviste de la consola.\n\n\nActivation No.:\n Pega el texto que obtuviste de la consola.\n\n\nEngine:\n \nFirebird\n\n\nSystem:\n \nCustom...\n \n-- IMPORTANTE usar este valor.\n\n\n\n\n\n\n\n\n\n\nPesta\u00f1a Firebird Connection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Connection\n\n\nApagado\n\n\nActivar SOLAMENTE cuando biPost.exe no est\u00e1 en el servidor de Firebird\n\n\n\n\n\n\nServer\n\n\nEn blanco\n\n\nUsar SOLAMENTE cuando Remote Connection est\u00e1 activado. Ingresar IP o nombre de red de la PC donde est\u00e1 el servidor Firebird.\n\n\n\n\n\n\nUser\n\n\nUsuario Firebird\n\n\nPor lo regular SYSDBA\n\n\n\n\n\n\nPassword\n\n\nContrase\u00f1a del usuario\n\n\n\n\n\n\n\n\nDatabase\n\n\nRuta archivo FDB\n\n\nCom\u00fanmente C:\\Datos Microsip\n\n\n\n\n\n\n\n\n3. Pesta\u00f1a General Settings\n\n\n\n\n\n\nActiva \nSpecific Bucket\n.\n\n\nIngresa el texto que recibiste por email de Factor BI.\n\n\n\n\nOprime \nSave Changes\n.\n\n\n\n\nDescarga Archivos de Sincronizaci\u00f3n\n\n\n\n\n\n\nDescarga aqu\u00ed.\n\n\n\n\n\n\nPara visualizar correctamente los archivos recomendamos utilizar \nSublime Text.\n\n\n\n\n\n\nPara validar un archivo JSON que modifiques, utiliza \nJSONLint.\n\n\n\n\n\n\n\n\nSincronizaci\u00f3n Primera Vez\n\n\n\n\n\n\nDescomprime customData-Microsip.zip\n\n\n\n\n\n\nReemplaza el archivo \ncustomSchema.json\n en tu carpeta Bipost.\n\n\n\n\n\n\n\n\nAbre \ncustomData-catalogos.json\n del ZIP copia y pega el contenido en \ncustomData.json\n\n\n\n\n\n\n\n\nAbre biPost.exe y sincroniza con el bot\u00f3n \nSync Now\n. Este proceso puede durar algunos minutos. Recibir\u00e1s el mensaje \nSync Completed\n cuando termine.\n\n\n\n\n\n\nRepite los pasos 3 y 4 para los archivos (respetando el orden mostrado): \n\n\n\n\ncustomData-movimientos_CM_IN.json\n\n\ncustomData-movimientos_CO_NO.json\n\n\ncustomData-movimientos_PV.json\n\n\ncustomData-movimientos_VE.json\n\n\ncustomData-movimientos_BA_CC_CP.json\n\n\ncustomData-30dias.json\n\n\n\n\n\n\n\n\nAgenda la sincronizaci\u00f3n diaria, ver siguiente secci\u00f3n \ud83d\udc47\n\n\n\n\n\n\n\n\nAgenda\n\n\nNOTA:\n Este proceso se realiza despu\u00e9s de sincronizar por primera vez la base de datos.\n\n\n\n\n\n\nAbre la carpeta donde colocaste Bipost, usualmente C:\\Bipost.\n\n\n\n\n\n\nAbre el archivo \ncustomData.json\n y verifica que el contenido es para sincronizar 30 d\u00edas. Esto se puede ver r\u00e1pidamente en alguna tabla como \"ARTICULOS\", ejemplo:\n\n\n\n\n\n\n\n\nAbre biPost.exe en la pesta\u00f1a \nSchedule\n\n\n\n\nOprime \nConfiguration\n, establece el horario para sincronizar y oprime \nSchedule\n. Oprime \nSave Changes\n.\n\n\nSe sugiere usar un horario despu\u00e9s que la empresa termine sus operaciones, por ejemplo en la madrugada. \n\n\nLo anterior crea una tarea en el \nProgramador de Tareas\n de Windows. Si requieres sincronizar m\u00e1s de una vez al d\u00eda, ve al men\u00fa de Windows y abre \"Programador de Tareas\".\n\n\nSeleccionar la primer carpeta del lado izquierdo y dar doble click a la tarea \nbiPost\n.\n\n\n\n\nEn la pesta\u00f1a \nDesencadenadores\n puedes crear m\u00e1s horarios: Nuevo \\ Diariamente \\ Inicio: establecer fecha y horario nuevo.\n\n\n\n\n\n\n\n\nTenant\n\n\nSi vas a consolidar m\u00faltiples bases de datos de Microsip, por favor comun\u00edcate con nosotros para explicar el uso correcto de la opci\u00f3n \ntenant_id\n.\n\n\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? Escr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Microsip"
        }, 
        {
            "location": "/microsip/#microsip-erp", 
            "text": "Integramos Microsip ERP con Amazon Web Services para crear tableros de mando, realizar an\u00e1lisis contable y fiscal y desarrollar soluciones a la medida.  Consolidamos tambi\u00e9n informaci\u00f3n de distintas sucursales que utilizan bases de datos separadas de Microsip, de razones sociales distintas y de la misma raz\u00f3n social (cuando no usas Replix).   Buscas un Business Intelligence? Visitanos!  www.factorbi.com  Escr\u00edbemos y te ayudamos en lo que necesitas:  info@factorbi.com", 
            "title": "Microsip ERP"
        }, 
        {
            "location": "/microsip/#casos-de-uso", 
            "text": "Dashboards Punto de Venta, Ventas, Pedidos, Cotizaciones, Remisiones.  Flujo de Efectivo + Cuentas por Cobrar + Cuentas por Pagar.  Estados Financieros Gr\u00e1ficos.  Consolidaci\u00f3n de Empresas y Sucursales.  Contabilidad Avanzada.  Origen y Aplicaci\u00f3n de Recursos.  Tableros Compras e Inventarios.   Ver casos aqu\u00ed.", 
            "title": "Casos de Uso"
        }, 
        {
            "location": "/microsip/#demo-business-intelligence", 
            "text": "Tablero de Mando Ventas Microsip", 
            "title": "Demo Business Intelligence"
        }, 
        {
            "location": "/microsip/#consolidacion", 
            "text": "Consolida informaci\u00f3n de Microsip de distintas razones sociales y sucursales que utilizan bases de datos independientes. Sube informaci\u00f3n de Excel como metas de ventas y presupuestos para realizar un an\u00e1lisis completo.", 
            "title": "Consolidaci\u00f3n"
        }, 
        {
            "location": "/microsip/#excel-tablas-microsip", 
            "text": "Abre aqu\u00ed  Excel tablas Microsip.  NOTA:  La lista del link anterior puede no estar completa.  Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Excel Tablas Microsip"
        }, 
        {
            "location": "/microsip/#descarga-el-programa-de-sincronizacion", 
            "text": "Descarga aqu\u00ed.    En tu servidor de Firebird crea una carpeta, por ejemplo   C:\\Bipost\\   y copia los archivos.", 
            "title": "Descarga el Programa de Sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#configura-la-sincronizacion", 
            "text": "La siguiente informaci\u00f3n es para Distribuidores Certificados de Factor BI.  Si eres usuario de Microsip por favor env\u00edanos un correo y nos pondremos en contacto:  info@factorbi.com", 
            "title": "Configura la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#1-llaves-de-sincronizacion", 
            "text": "Entra a la  Consola Factor BI  y en el men\u00fa  Service Numbers  copia  Service No.  y  Activation No.  que correspondan con la base de datos a sincronizar.", 
            "title": "1. Llaves de sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#2-configura-bipost", 
            "text": "Abre biPost.exe y oprime  Configuration .    Service No.:  Pega el texto que obtuviste de la consola.  Activation No.:  Pega el texto que obtuviste de la consola.  Engine:   Firebird  System:   Custom...   -- IMPORTANTE usar este valor.      Pesta\u00f1a Firebird Connection        Remote Connection  Apagado  Activar SOLAMENTE cuando biPost.exe no est\u00e1 en el servidor de Firebird    Server  En blanco  Usar SOLAMENTE cuando Remote Connection est\u00e1 activado. Ingresar IP o nombre de red de la PC donde est\u00e1 el servidor Firebird.    User  Usuario Firebird  Por lo regular SYSDBA    Password  Contrase\u00f1a del usuario     Database  Ruta archivo FDB  Com\u00fanmente C:\\Datos Microsip", 
            "title": "2. Configura biPost"
        }, 
        {
            "location": "/microsip/#3-pestana-general-settings", 
            "text": "Activa  Specific Bucket .  Ingresa el texto que recibiste por email de Factor BI.   Oprime  Save Changes .", 
            "title": "3. Pesta\u00f1a General Settings"
        }, 
        {
            "location": "/microsip/#descarga-archivos-de-sincronizacion", 
            "text": "Descarga aqu\u00ed.    Para visualizar correctamente los archivos recomendamos utilizar  Sublime Text.    Para validar un archivo JSON que modifiques, utiliza  JSONLint.", 
            "title": "Descarga Archivos de Sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#sincronizacion-primera-vez", 
            "text": "Descomprime customData-Microsip.zip    Reemplaza el archivo  customSchema.json  en tu carpeta Bipost.     Abre  customData-catalogos.json  del ZIP copia y pega el contenido en  customData.json     Abre biPost.exe y sincroniza con el bot\u00f3n  Sync Now . Este proceso puede durar algunos minutos. Recibir\u00e1s el mensaje  Sync Completed  cuando termine.    Repite los pasos 3 y 4 para los archivos (respetando el orden mostrado):    customData-movimientos_CM_IN.json  customData-movimientos_CO_NO.json  customData-movimientos_PV.json  customData-movimientos_VE.json  customData-movimientos_BA_CC_CP.json  customData-30dias.json     Agenda la sincronizaci\u00f3n diaria, ver siguiente secci\u00f3n \ud83d\udc47", 
            "title": "Sincronizaci\u00f3n Primera Vez"
        }, 
        {
            "location": "/microsip/#agenda", 
            "text": "NOTA:  Este proceso se realiza despu\u00e9s de sincronizar por primera vez la base de datos.    Abre la carpeta donde colocaste Bipost, usualmente C:\\Bipost.    Abre el archivo  customData.json  y verifica que el contenido es para sincronizar 30 d\u00edas. Esto se puede ver r\u00e1pidamente en alguna tabla como \"ARTICULOS\", ejemplo:     Abre biPost.exe en la pesta\u00f1a  Schedule   Oprime  Configuration , establece el horario para sincronizar y oprime  Schedule . Oprime  Save Changes .  Se sugiere usar un horario despu\u00e9s que la empresa termine sus operaciones, por ejemplo en la madrugada.   Lo anterior crea una tarea en el  Programador de Tareas  de Windows. Si requieres sincronizar m\u00e1s de una vez al d\u00eda, ve al men\u00fa de Windows y abre \"Programador de Tareas\".  Seleccionar la primer carpeta del lado izquierdo y dar doble click a la tarea  biPost .   En la pesta\u00f1a  Desencadenadores  puedes crear m\u00e1s horarios: Nuevo \\ Diariamente \\ Inicio: establecer fecha y horario nuevo.", 
            "title": "Agenda"
        }, 
        {
            "location": "/microsip/#tenant", 
            "text": "Si vas a consolidar m\u00faltiples bases de datos de Microsip, por favor comun\u00edcate con nosotros para explicar el uso correcto de la opci\u00f3n  tenant_id .", 
            "title": "Tenant"
        }, 
        {
            "location": "/microsip/#contacto", 
            "text": "\u00bfNecesitas ayuda? Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/intelisis/", 
            "text": "Intelisis ERP\n\n\n\n\nIntegramos Intelisis con Amazon Web Services para crear tableros de mando, realizar an\u00e1lisis contable y fiscal y desarrollar soluciones Web a la medida.\n\n\nBuscas un Business Intelligence sin pagar licencias por usuario? Visitanos! \nwww.factorbi.com\n\n\nGoogle Data Studio es una excelente alternativa vs Qlik View, Power BI y Cognos BI.\n\n\nEscr\u00edbemos y te ayudamos! \ninfo@factorbi.com\n\n\n\n\nDemo\n\n\nTablero de Mando Ventas Intelisis\n\n\n\n\n\n\nContacto\n\n\n\u00bfNecesitas ayuda? Escr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Intelisis"
        }, 
        {
            "location": "/intelisis/#intelisis-erp", 
            "text": "Integramos Intelisis con Amazon Web Services para crear tableros de mando, realizar an\u00e1lisis contable y fiscal y desarrollar soluciones Web a la medida.  Buscas un Business Intelligence sin pagar licencias por usuario? Visitanos!  www.factorbi.com  Google Data Studio es una excelente alternativa vs Qlik View, Power BI y Cognos BI.  Escr\u00edbemos y te ayudamos!  info@factorbi.com", 
            "title": "Intelisis ERP"
        }, 
        {
            "location": "/intelisis/#demo", 
            "text": "Tablero de Mando Ventas Intelisis", 
            "title": "Demo"
        }, 
        {
            "location": "/intelisis/#contacto", 
            "text": "\u00bfNecesitas ayuda? Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/soft-restaurant/", 
            "text": "Soft Restaurant Punto de Venta\n\n\n\n\nIntegramos Soft Restaurant Punto de Venta con Amazon Web Services para consolidar informaci\u00f3n de m\u00faltiples restaurantes y crear tableros de mando con an\u00e1lisis de ventas, costos de alimentos, marcaci\u00f3n y m\u00e1s.\n\n\nIntegra tambi\u00e9n tu ERP y operaciones de CEDIS y WMS con la informaci\u00f3n que se recaba autom\u00e1ticamente de cada restaurante.\n\n\n\n\nEscr\u00edbemos y te ayudamos en lo que necesitas: \ninfo@factorbi.com\n\n\nwww.factorbi.com\n\n\n\n\nDemo Business Intelligence\n\n\nTablero de Ventas Soft Restaurant\n\n\n\n\n\n\n\n\n\n\nExcel Tablas Soft Restaurant\n\n\n Abre aqu\u00ed \nExcel tablas Soft Restaurant.\n\n\n\n\nContacto\n\n\n\u00bfM\u00e1s informaci\u00f3n? Escr\u00edbemos! \ninfo@factorbi.com", 
            "title": "Soft Restaurant"
        }, 
        {
            "location": "/soft-restaurant/#soft-restaurant-punto-de-venta", 
            "text": "Integramos Soft Restaurant Punto de Venta con Amazon Web Services para consolidar informaci\u00f3n de m\u00faltiples restaurantes y crear tableros de mando con an\u00e1lisis de ventas, costos de alimentos, marcaci\u00f3n y m\u00e1s.  Integra tambi\u00e9n tu ERP y operaciones de CEDIS y WMS con la informaci\u00f3n que se recaba autom\u00e1ticamente de cada restaurante.   Escr\u00edbemos y te ayudamos en lo que necesitas:  info@factorbi.com  www.factorbi.com", 
            "title": "Soft Restaurant Punto de Venta"
        }, 
        {
            "location": "/soft-restaurant/#demo-business-intelligence", 
            "text": "Tablero de Ventas Soft Restaurant", 
            "title": "Demo Business Intelligence"
        }, 
        {
            "location": "/soft-restaurant/#excel-tablas-soft-restaurant", 
            "text": "Abre aqu\u00ed  Excel tablas Soft Restaurant.", 
            "title": "Excel Tablas Soft Restaurant"
        }, 
        {
            "location": "/soft-restaurant/#contacto", 
            "text": "\u00bfM\u00e1s informaci\u00f3n? Escr\u00edbemos!  info@factorbi.com", 
            "title": "Contacto"
        }, 
        {
            "location": "/positouch/", 
            "text": "POSitouch Restaurant Point of Sale\n\n\n\n\nConnect POSitouch restaurant POS to Amazon Web Services and analyze Sales, Alcohol Mix, Food Cost, Ticket Time and Time and Attendance information from across multiple restaurant locations.\n\n\nWe have the full solution for:\n\n\n\n\nAutomatically export daily information from each restaurant location.\n\n\nAutomatically merge, apply consolidation rules and create summaries on AWS.\n\n\nCalculate KPI's: Alcohol Mix, COGS, % Disc / Sales, Avg Check, Guests, Day of Week vs Hour, Send Time, etc.\n\n\nDeliver Cloud Business Intelligence Dashboards with Google Data Studio, a FREE B.I. tool from Google.\n\n\nUpload \n merge information from other POS systems like Micros or sources like Excel spread sheets and ERP's.\n\n\nSend daily email reports with PDF attachments.\n\n\n\n\n\n\nContact us: \ninfo@factorbi.com\n\n\nwww.factorbi.com\n\n\n\n\nBusiness Intelligence Demo\n\n\nPOSitouch Business Intelligence Demo\n\n\n\n\n\n\n\n\n\n\nPOSitouch Manuals and Resources\n\n\n\n\n\n\nPOSitouch DBF database structure \n documentation download.\n\n\n\n\n\n\nPOSitouch Inventory Control and Food Cost Manual with Inventory Reports.\n\n\n\n\n\n\nPOSitouch Time and Attendance User Manual.\n\n\n\n\n\n\n\n\nContact Us\n\n\n\n\n\n\nCompany page: \nwww.factorbi.com\n\n\n\n\n\n\nEmail: \ninfo@factorbi.com", 
            "title": "POSitouch"
        }, 
        {
            "location": "/positouch/#positouch-restaurant-point-of-sale", 
            "text": "Connect POSitouch restaurant POS to Amazon Web Services and analyze Sales, Alcohol Mix, Food Cost, Ticket Time and Time and Attendance information from across multiple restaurant locations.  We have the full solution for:   Automatically export daily information from each restaurant location.  Automatically merge, apply consolidation rules and create summaries on AWS.  Calculate KPI's: Alcohol Mix, COGS, % Disc / Sales, Avg Check, Guests, Day of Week vs Hour, Send Time, etc.  Deliver Cloud Business Intelligence Dashboards with Google Data Studio, a FREE B.I. tool from Google.  Upload   merge information from other POS systems like Micros or sources like Excel spread sheets and ERP's.  Send daily email reports with PDF attachments.    Contact us:  info@factorbi.com  www.factorbi.com", 
            "title": "POSitouch Restaurant Point of Sale"
        }, 
        {
            "location": "/positouch/#business-intelligence-demo", 
            "text": "POSitouch Business Intelligence Demo", 
            "title": "Business Intelligence Demo"
        }, 
        {
            "location": "/positouch/#positouch-manuals-and-resources", 
            "text": "POSitouch DBF database structure   documentation download.    POSitouch Inventory Control and Food Cost Manual with Inventory Reports.    POSitouch Time and Attendance User Manual.", 
            "title": "POSitouch Manuals and Resources"
        }, 
        {
            "location": "/positouch/#contact-us", 
            "text": "Company page:  www.factorbi.com    Email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/businessintelligence/", 
            "text": "Use Case\n\n\nUse Bipost Sync to power your company with cloud Dashboards and KPI's using \nGoogle Data Studio\n or \nAWS QuickSight\n\n\n\n\nWant to see a demo?\n Go to --\n \nGoogle Data Studio Demo.\n\n\n\n\nContact Us\n\n\nInterested? Send us an email: \ninfo@factorbi.com", 
            "title": "Business Intelligence"
        }, 
        {
            "location": "/businessintelligence/#use-case", 
            "text": "Use Bipost Sync to power your company with cloud Dashboards and KPI's using  Google Data Studio  or  AWS QuickSight   Want to see a demo?  Go to --   Google Data Studio Demo.", 
            "title": "Use Case"
        }, 
        {
            "location": "/businessintelligence/#contact-us", 
            "text": "Interested? Send us an email:  info@factorbi.com", 
            "title": "Contact Us"
        }, 
        {
            "location": "/csvtomysql/", 
            "text": "CSV to MySQL Aurora\n\n\nIf your are looking for: \n\n\n\n\nAn API to continually load CSV files to MySQL. \n\n\nDynamically create/alter schema tables according to CSV structure.\n\n\nPut CSV objects on S3 and dynamically call LOAD DATA FROM S3 to MySQL.\n\n\nA cloud managed solution.\n\n\nPay only for what you use.\n\n\n\n\n\n\nTell us about your project. We can help --\n \ninfo@factorbi.com", 
            "title": "CSV to MySQL Aurora"
        }, 
        {
            "location": "/csvtomysql/#csv-to-mysql-aurora", 
            "text": "If your are looking for:    An API to continually load CSV files to MySQL.   Dynamically create/alter schema tables according to CSV structure.  Put CSV objects on S3 and dynamically call LOAD DATA FROM S3 to MySQL.  A cloud managed solution.  Pay only for what you use.    Tell us about your project. We can help --   info@factorbi.com", 
            "title": "CSV to MySQL Aurora"
        }, 
        {
            "location": "/setupaws/", 
            "text": "Link your AWS Account\n\n\nThese instructions are for experienced AWS administrators only.\n\n\nFollow these instructions only if you want to manually link your AWS Account to Bipost API. Strong knowledge of AWS is required.\n We strongly recommend to use instead the automated \nCloudFormation template\n.\n\n\nIMPORTANT NOTICE: If you are planning to use the following AWS resources for production you may want to follow your company policies and understand how to use AWS security according to your needs.\n\n\n\n\nCanonical User ID\n\n\nSign in with the \nroot\n AWS account.\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nExpand Account Identifiers.\n\n\n\n\nCopy AWS Account ID (12-digit) and Canonical User ID (64-digit).\n\n\n\n\n\n\n\n\nEmail these numbers to \ninfo@factorbi.com\n so we can setup your dedicated Bucket.\n\n\n\n\n\n\nStop here until you get a reply email from Factor BI.\n We will provide your \nbucket name\n which will be used on further steps.\n\n\n\n\nIAM Policy to Grant Access to S3\n\n\nFrom this point on you need the Bucket name that we provided over email on the previous step.\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nPolicies.\n\n\nCreate policy\n blue button.\n\n\n\n\nClick \nJSON\n tab.\n\n\n\n\n\n\n\n\nCopy and paste the following.\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-abc123456789012\", \n                \"arn:aws:s3:::bipostdata-abc123456789012/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"lambda:InvokeFunction\",\n            \"Resource\": \"arn:aws:lambda:us-east-1:951464950892:function:bipost-getOutData\"\n        }                \n    ]\n}\n\n\n\n\n\n\n\nReplace the text \nbipostdata-abc123456789012\n with the Bucket Name you received from us over email.\n\n\n\n\n\n\nClick \nReview policy\n blue button on the lower right.\n\n\n\n\n\n\nEnter the following on the Review policy screen.\n\n\n\n\nName: \nauroraToS3Policy\n\n\nName: \nConnection to Factor BI bucket\n\n\n\n\n\n\n\n\nClick \nCreate policy\n blue button.\n\n\n\n\n\n\nFurther information from AWS go to: \nAllowing Amazon Aurora to Access Amazon S3 Resources\n\n\n\n\nIAM Role to Load Data From S3\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nRoles.\n\n\nCreate role\n blue button.\n\n\n\n\nChoose \nAWS service,\n then \nRDS\n\n\n\n\n\n\n\n\n\n\n\n\nUnder \nSelect your use case\n click \nRDS - CloudHSM and Directory Service,\n click \nNext: Permissions\n blue button.\n\n\n\n\nClick \nNext: Tags\n and then \nNext: Review\n.\n\n\nSet \nRole name:\n \nRDSLoadFromS3\n and click \nCreate role.\n\n\n\n\nNow from the navigation details, click the role you just created.\n\n\n\n\n\n\n\n\n\n\n\n\nUnder permissions tab, detach by clicking \nX\n the following:\n\n\n\n\nAmazonRDSDirectoryServiceAccess\n\n\nRDSCloudHsmAuthorizationRole\n\n\n\n\n\n\n\n\n\n\nNow click \nAttach policies\n blue button.\n\n\n\n\n\n\n\n\n\n\n\n\nFilter policies by Customer managed.\n\n\n\n\n\n\n\n\n\n\n\n\nSelect \nauroraToS3Policy\n and click \nAttach policy\n blue button.\n\n\n\n\n\n\nCopy \nRole ARN\n string and save it for further use. It may look like this: \narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\nFurther information from AWS go to: \nCreating an IAM Role to Allow Amazon Aurora to Access AWS Services\n\n\n\n\nIAM User to Save Data to S3\n\n\nThis step will provide access to files created by the \nSELECT INTO OUTFILE S3\n command.\n\n\n\n\nOpen \nIAM console.\n\n\nIn the left navigation pane choose \nUsers.\n\n\nClick \nAdd user\n blue button, upper left corner.\n\n\nUser name: \nauroraToS3\n\n\nAccess type: \nProgrammatic access\n\n\nClick \nNext: Permissions\n blue button lower right corner.\n\n\nSelect \nAttach existing policies directly\n\n\n\n\nFilter policies by Customer managed.\n\n\n\n\n\n\n\n\n\n\n\n\nSelect \nauroraToS3Policy\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nNext: Tags\n and then \nNext: Review\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate user\n blue button lower right corner.\n\n\n\n\nClick \nDownload .csv\n.\n\n\n\n\nEmail the CSV to \ninfo@factorbi.com\n so we can setup the Access key.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClosest AWS Region\n\n\n\n\n\n\nClick the following image and hit \nHTTP Ping\n and look for the lowest latency.\n\n\n\n\n\n\n\n\n\n\n\n\nTry several times and at different times of the day.\n\n\n\n\n\n\nLogin to your \nAWS Account Console Home\n and select the closest region to your location.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter groups.\n\n\n\n\nClick \nCreate parameter group\n orange button on top.\n\n\n\n\nParameter group family: \naurora-mysql5.7\n\n\nType: \nDB Cluster Parameter Group\n\n\nGroup name: \nClusterAllowAWSAccess\n\n\nDescription: \nBipost Aurora Database Cluster Parameter Group\n\n\n\n\n\n\n\n\nClick \nCreate\n orange button and refresh browser.\n\n\n\n\nClick check box on your new \nclusterallowawsaccess\n parameter group and click \nParameter group actions\n and then \nEdit.\n\n\nMake sure you have your ARN role string \n(not sure? click here)\n and replace it below.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nValues\n\n\nExample\n\n\n\n\n\n\n\n\n\n\naurora_load_from_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naurora_select_into_s3_role\n\n\npaste Role ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_lambda_role\n\n\npaste Role ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_s3_role\n\n\npaste Role ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave changes\n orange button.\n\n\n\n\n\n\nClick \nPreview changes\n and it should look like this:\n\n\n\n\n\n\n\n\nFurther information from AWS go to: \nAssociating an IAM Role with a DB Cluster\n\n\n\n\nDB Parameter Group\n\n\n\n\nGo to \nRDS console.\n\n\nOn left pane go to \nParameter groups.\n\n\n\n\nClick \nCreate parameter group\n orange button on top.\n\n\n\n\nParameter group family: \naurora-mysql5.7\n\n\nType: \nDB Parameter Group\n\n\nGroup name: \nInstanceAllowAWSAccess\n\n\nDescription: \nBipost Aurora Parameter Group\n\n\n\n\n\n\n\n\nClick \nCreate\n orange button and refresh browser.\n\n\n\n\nClick check box on your new \ninstanceallowawsaccess\n parameter group and click \nParameter group actions\n and then \nEdit.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nValues\n\n\n\n\n\n\n\n\n\n\nlog_bin_trust_function_creators\n\n\n1\n\n\n\n\n\n\nmax_allowed_packet\n\n\n1073741824\n\n\n\n\n\n\nmax_connections\n\n\n16000\n\n\n\n\n\n\nmax_user_connections\n\n\n4294967295\n\n\n\n\n\n\nevent_scheduler\n\n\nON\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave changes\n orange button.\n\n\n\n\n\n\nClick \nPreview changes\n and it should look like this:\n\n\n\n\n\n\n\n\n\n\nAurora Instance\n\n\nCreate Instance\n\n\n\n\nGo to \nRDS Console\n and click Databases.\n\n\nClick \nCreate database\n, orange button.\n\n\n\n\nSelect Engine: \nAmazon Aurora\n, scroll down, Edition: \nMySQL 5.7-compatible\n click \nNext\n orange button.\n\n\n\n\n\n\nSpecify DB details\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nSet to:\n\n\n\n\n\n\n\n\n\n\nCapacity type\n\n\nProvisioned\n\n\n\n\n\n\nDB engine version\n\n\nAurora (MySQL)-5.7.12\n\n\n\n\n\n\nDB instance class\n\n\ndb.t2.small\n\n\n\n\n\n\nMulti-AZ deployment\n\n\nNo\n\n\n\n\n\n\nDB instance identifier\n\n\nSet a lower-case name with no special characters\n\n\n\n\n\n\nMaster username\n\n\nroot\n\n\n\n\n\n\nMaster password\n\n\nCombine upper and lower case, numbers and special characters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nNext\n orange button.\n\n\n\n\n\n\nConfigure advanced settings\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nSet to:\n\n\n\n\n\n\n\n\n\n\nVirtual Private Cloud (VPC)\n\n\nCreate new VPC\n\n\n\n\n\n\nSubnet group\n\n\nCreate new DB Subnet Group\n\n\n\n\n\n\nPublic accessibility\n\n\nYes\n\n\n\n\n\n\nAvailability zone\n\n\nNo preference\n\n\n\n\n\n\nVPC security groups\n\n\nCreate new VPC security group\n\n\n\n\n\n\nDB cluster identifier\n\n\nleave blank\n\n\n\n\n\n\nDatabase name\n\n\nleave blank\n\n\n\n\n\n\nPort\n\n\n3306\n\n\n\n\n\n\nDB parameter group\n\n\ninstanceallowawsaccess\n\n\n\n\n\n\nDB cluster parameter group\n\n\nclusterallowawsaccess\n\n\n\n\n\n\nOption group\n\n\nleave default\n\n\n\n\n\n\nEncryption\n\n\nDisable encryption\n\n\n\n\n\n\nFailover Priority\n\n\ntier-0\n\n\n\n\n\n\nBackup retention period\n\n\n1 day\n\n\n\n\n\n\nMonitoring\n\n\nDisable enhanced monitoring\n\n\n\n\n\n\nLog exports\n\n\nAll unchecked\n\n\n\n\n\n\nAuto minor version upgrade\n\n\nEnable auto minor version upgrade\n\n\n\n\n\n\nMaintenance windows\n\n\nleave defaults\n\n\n\n\n\n\nEnable deletion protection\n\n\nClear check box\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate database\n orange button. This process may take a few minutes.\n\n\n\n\n\n\n\n\nRDS Instance Security Group\n\n\n\n\nGo to \nRDS Console\n and click Databases.\n\n\nOnce the new instance (Writer Role) has \nStatus:\n \nAvailable\n proceed:\n\n\nClick your new instance (Writer Role).\n\n\nConnectivity\n tab. \n\n\n\n\nUnder \nSecurity\n click the blue string that looks like this\n\n\n\n\nrds-launch-wizard (sg-XXXXXXXX)\n\n\n\n\n\n\n\n\n\n\nYou are now on EC2 Management Console and Security Group ID is already selected.\n\n\n\n\nClick \nActions \\ Edit inbound rules\n\n\nRemove the default Custom TCP rule created.\n\n\nClick \nAdd Rule\n, under Type select \nMYSQL/Aurora\n\n\n\n\nSource \nCustom\n and enter this value: \n0.0.0.0/0\n\n\n\n\n\n\nRepeat steps 9 \n 10 and enter this value \n::/0\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n blue button.\n\n\n\n\nClick \nActions \\ Edit outbound rules\n\n\nVerify if Type: \nAll traffic\n, Destination: \nCustom\n and value: \n0.0.0.0/0\n is already set, if not, add the rule.\n\n\nGo back to \nRDS console\n, select your instance (Writer Role), click \nActions \\ Reboot\n, confirm with orange button on the right.\n\n\n\n\nWait until \nStatus\n is \nAvailable\n \n\n\n\n\n\n\n\n\n\n\n\n\nClick your DB Instance (Writer Role), Connectivity, Security, and check if \nVPS security groups\n are \n( active )\n\n\n\n\n\n\n\n\nAdd IAM Role to Aurora Cluster\n\n\n\n\nGo to \nRDS Console\n and click Databases.\n\n\n\n\nClick your new DB identifier \nRole Regional\n, then scroll down to \nManage IAM roles\n.\n\n\n\n\n\n\n\n\n\n\n\n\nUnder \nAdd IAM roles to this cluster\n select the role you created: \nRDSLoadFromS3\n and click \nAdd role\n button.\n\n\n\n\n\n\n\n\n\n\n\n\nWait until you see Status \nACTIVE\n.\n\n\n\n\n\n\n\n\nVerify Instance Configuration\n\n\n\n\nGo to \nRDS console\n and click Databases.\n\n\nVerify instance (Writer Role) Status is \nAvailable\n\n\nClick your instance (Writer Role).\n\n\n\n\nConnectivity tab, verify the following:\n\n\n\n\nVPC security groups: \nrds-launch-wizard (sg-XXXXXXXX) ( active )\n\n\nPublic accessibility: \nYes\n\n\n\n\n\n\n\n\nConfiguration tab, verify the following:\n\n\n\n\nParameter group: \ninstanceallowawsaccess (in-sync)\n\n\n\n\n\n\n\n\nGo back to Databases and click your cluster (Regional Role), verify the following:\n\n\n\n\nDB cluster parameter group: \nclusterallowawsaccess (in-sync)\n\n\n\n\n\n\n\n\n\n\nTest MySQL Connection\n\n\n\n\n\n\nDownload and install any \nMySQL client\n of your preference:\n\n\nFor Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"\n\n\n\n\n\n\n\nGo to \nRDS console\n, then Databases, click your new cluster (Regional Role).\n\n\n\n\n\n\nUnder Connectivity tab copy the \nWriter\n endpoint name string.\n\n\n\n\n\n\nLaunch your MySQL client and configure a new connection:\n\n\n\n\nHost:\n Paste the Writer endpoint string.\n\n\nUsername:\n root\n\n\nPassword:\n type the Master Password\n\n\nPort:\n 3306\n\n\nDatabase:\n Leave blank\n\n\nConnect using SSL:\n No\n\n\n\n\n\n\n\n\nClick Connect and verify that you can successfully connect to your RDS instance.\n\n\n\n\n\n\n\n\nSetup Factor BI Console\n\n\nClick and follow steps to \ncreate your account with Factor BI.\n\n\n\n\nUsing other MySQL Users\n\n\nIf you don't want to use \nroot\n account for the synchronization API, then you may create a new user and password on MySQL and:\n\n\n\n\nGive the new user: \nGRANT LOAD FROM S3 ON *.* TO 'mynewuser';\n\n\n\n\nSet the following Global Privileges:\n\n\n\n\n\n\n\n\n\n\nConsole Access to Bucket\n\n\nBipost synchronization uses S3 to upload the data that is extracted from the on-prem database. The bucket is located within Factor BI AWS account so we can efficiently handle API calls, patches and new releases.\n\n\nWe create a unique S3 bucket for each customer so nothing gets mixed up.\n\n\nSometimes you may want to access this bucket and review files and folders.\n\n\nWrite us to provide this access: \ninfo@factorbi.com", 
            "title": "Advanced AWS Setup"
        }, 
        {
            "location": "/setupaws/#link-your-aws-account", 
            "text": "These instructions are for experienced AWS administrators only.  Follow these instructions only if you want to manually link your AWS Account to Bipost API. Strong knowledge of AWS is required.  We strongly recommend to use instead the automated  CloudFormation template .  IMPORTANT NOTICE: If you are planning to use the following AWS resources for production you may want to follow your company policies and understand how to use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#canonical-user-id", 
            "text": "Sign in with the  root  AWS account.   Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Expand Account Identifiers.   Copy AWS Account ID (12-digit) and Canonical User ID (64-digit).     Email these numbers to  info@factorbi.com  so we can setup your dedicated Bucket.    Stop here until you get a reply email from Factor BI.  We will provide your  bucket name  which will be used on further steps.", 
            "title": "Canonical User ID"
        }, 
        {
            "location": "/setupaws/#iam-policy-to-grant-access-to-s3", 
            "text": "From this point on you need the Bucket name that we provided over email on the previous step.   Open  IAM Console.  In the left navigation pane choose  Policies.  Create policy  blue button.   Click  JSON  tab.     Copy and paste the following.  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": [\n                \"arn:aws:s3:::bipostdata-abc123456789012\", \n                \"arn:aws:s3:::bipostdata-abc123456789012/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"lambda:InvokeFunction\",\n            \"Resource\": \"arn:aws:lambda:us-east-1:951464950892:function:bipost-getOutData\"\n        }                \n    ]\n}    Replace the text  bipostdata-abc123456789012  with the Bucket Name you received from us over email.    Click  Review policy  blue button on the lower right.    Enter the following on the Review policy screen.   Name:  auroraToS3Policy  Name:  Connection to Factor BI bucket     Click  Create policy  blue button.    Further information from AWS go to:  Allowing Amazon Aurora to Access Amazon S3 Resources", 
            "title": "IAM Policy to Grant Access to S3"
        }, 
        {
            "location": "/setupaws/#iam-role-to-load-data-from-s3", 
            "text": "Open  IAM Console.  In the left navigation pane choose  Roles.  Create role  blue button.   Choose  AWS service,  then  RDS       Under  Select your use case  click  RDS - CloudHSM and Directory Service,  click  Next: Permissions  blue button.   Click  Next: Tags  and then  Next: Review .  Set  Role name:   RDSLoadFromS3  and click  Create role.   Now from the navigation details, click the role you just created.       Under permissions tab, detach by clicking  X  the following:   AmazonRDSDirectoryServiceAccess  RDSCloudHsmAuthorizationRole      Now click  Attach policies  blue button.       Filter policies by Customer managed.       Select  auroraToS3Policy  and click  Attach policy  blue button.    Copy  Role ARN  string and save it for further use. It may look like this:  arn:aws:iam::123456789012:role/RDSLoadFromS3    Further information from AWS go to:  Creating an IAM Role to Allow Amazon Aurora to Access AWS Services", 
            "title": "IAM Role to Load Data From S3"
        }, 
        {
            "location": "/setupaws/#iam-user-to-save-data-to-s3", 
            "text": "This step will provide access to files created by the  SELECT INTO OUTFILE S3  command.   Open  IAM console.  In the left navigation pane choose  Users.  Click  Add user  blue button, upper left corner.  User name:  auroraToS3  Access type:  Programmatic access  Click  Next: Permissions  blue button lower right corner.  Select  Attach existing policies directly   Filter policies by Customer managed.       Select  auroraToS3Policy       Click  Next: Tags  and then  Next: Review .       Click  Create user  blue button lower right corner.   Click  Download .csv .   Email the CSV to  info@factorbi.com  so we can setup the Access key.", 
            "title": "IAM User to Save Data to S3"
        }, 
        {
            "location": "/setupaws/#closest-aws-region", 
            "text": "Click the following image and hit  HTTP Ping  and look for the lowest latency.       Try several times and at different times of the day.    Login to your  AWS Account Console Home  and select the closest region to your location.", 
            "title": "Closest AWS Region"
        }, 
        {
            "location": "/setupaws/#cluster-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Parameter groups.   Click  Create parameter group  orange button on top.   Parameter group family:  aurora-mysql5.7  Type:  DB Cluster Parameter Group  Group name:  ClusterAllowAWSAccess  Description:  Bipost Aurora Database Cluster Parameter Group     Click  Create  orange button and refresh browser.   Click check box on your new  clusterallowawsaccess  parameter group and click  Parameter group actions  and then  Edit.  Make sure you have your ARN role string  (not sure? click here)  and replace it below.   Set the following:     Name  Values  Example      aurora_load_from_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aurora_select_into_s3_role  paste Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_lambda_role  paste Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_s3_role  paste Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3       Click  Save changes  orange button.    Click  Preview changes  and it should look like this:     Further information from AWS go to:  Associating an IAM Role with a DB Cluster", 
            "title": "Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#db-parameter-group", 
            "text": "Go to  RDS console.  On left pane go to  Parameter groups.   Click  Create parameter group  orange button on top.   Parameter group family:  aurora-mysql5.7  Type:  DB Parameter Group  Group name:  InstanceAllowAWSAccess  Description:  Bipost Aurora Parameter Group     Click  Create  orange button and refresh browser.   Click check box on your new  instanceallowawsaccess  parameter group and click  Parameter group actions  and then  Edit.   Set the following:     Name  Values      log_bin_trust_function_creators  1    max_allowed_packet  1073741824    max_connections  16000    max_user_connections  4294967295    event_scheduler  ON       Click  Save changes  orange button.    Click  Preview changes  and it should look like this:", 
            "title": "DB Parameter Group"
        }, 
        {
            "location": "/setupaws/#aurora-instance", 
            "text": "", 
            "title": "Aurora Instance"
        }, 
        {
            "location": "/setupaws/#create-instance", 
            "text": "Go to  RDS Console  and click Databases.  Click  Create database , orange button.   Select Engine:  Amazon Aurora , scroll down, Edition:  MySQL 5.7-compatible  click  Next  orange button.    Specify DB details      Parameter  Set to:      Capacity type  Provisioned    DB engine version  Aurora (MySQL)-5.7.12    DB instance class  db.t2.small    Multi-AZ deployment  No    DB instance identifier  Set a lower-case name with no special characters    Master username  root    Master password  Combine upper and lower case, numbers and special characters.        Click  Next  orange button.    Configure advanced settings      Parameter  Set to:      Virtual Private Cloud (VPC)  Create new VPC    Subnet group  Create new DB Subnet Group    Public accessibility  Yes    Availability zone  No preference    VPC security groups  Create new VPC security group    DB cluster identifier  leave blank    Database name  leave blank    Port  3306    DB parameter group  instanceallowawsaccess    DB cluster parameter group  clusterallowawsaccess    Option group  leave default    Encryption  Disable encryption    Failover Priority  tier-0    Backup retention period  1 day    Monitoring  Disable enhanced monitoring    Log exports  All unchecked    Auto minor version upgrade  Enable auto minor version upgrade    Maintenance windows  leave defaults    Enable deletion protection  Clear check box        Click  Create database  orange button. This process may take a few minutes.", 
            "title": "Create Instance"
        }, 
        {
            "location": "/setupaws/#rds-instance-security-group", 
            "text": "Go to  RDS Console  and click Databases.  Once the new instance (Writer Role) has  Status:   Available  proceed:  Click your new instance (Writer Role).  Connectivity  tab.    Under  Security  click the blue string that looks like this   rds-launch-wizard (sg-XXXXXXXX)      You are now on EC2 Management Console and Security Group ID is already selected.   Click  Actions \\ Edit inbound rules  Remove the default Custom TCP rule created.  Click  Add Rule , under Type select  MYSQL/Aurora   Source  Custom  and enter this value:  0.0.0.0/0    Repeat steps 9   10 and enter this value  ::/0       Click  Save  blue button.   Click  Actions \\ Edit outbound rules  Verify if Type:  All traffic , Destination:  Custom  and value:  0.0.0.0/0  is already set, if not, add the rule.  Go back to  RDS console , select your instance (Writer Role), click  Actions \\ Reboot , confirm with orange button on the right.   Wait until  Status  is  Available         Click your DB Instance (Writer Role), Connectivity, Security, and check if  VPS security groups  are  ( active )", 
            "title": "RDS Instance Security Group"
        }, 
        {
            "location": "/setupaws/#add-iam-role-to-aurora-cluster", 
            "text": "Go to  RDS Console  and click Databases.   Click your new DB identifier  Role Regional , then scroll down to  Manage IAM roles .       Under  Add IAM roles to this cluster  select the role you created:  RDSLoadFromS3  and click  Add role  button.       Wait until you see Status  ACTIVE .", 
            "title": "Add IAM Role to Aurora Cluster"
        }, 
        {
            "location": "/setupaws/#verify-instance-configuration", 
            "text": "Go to  RDS console  and click Databases.  Verify instance (Writer Role) Status is  Available  Click your instance (Writer Role).   Connectivity tab, verify the following:   VPC security groups:  rds-launch-wizard (sg-XXXXXXXX) ( active )  Public accessibility:  Yes     Configuration tab, verify the following:   Parameter group:  instanceallowawsaccess (in-sync)     Go back to Databases and click your cluster (Regional Role), verify the following:   DB cluster parameter group:  clusterallowawsaccess (in-sync)", 
            "title": "Verify Instance Configuration"
        }, 
        {
            "location": "/setupaws/#test-mysql-connection", 
            "text": "Download and install any  MySQL client  of your preference:  For Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"    Go to  RDS console , then Databases, click your new cluster (Regional Role).    Under Connectivity tab copy the  Writer  endpoint name string.    Launch your MySQL client and configure a new connection:   Host:  Paste the Writer endpoint string.  Username:  root  Password:  type the Master Password  Port:  3306  Database:  Leave blank  Connect using SSL:  No     Click Connect and verify that you can successfully connect to your RDS instance.", 
            "title": "Test MySQL Connection"
        }, 
        {
            "location": "/setupaws/#setup-factor-bi-console", 
            "text": "Click and follow steps to  create your account with Factor BI.", 
            "title": "Setup Factor BI Console"
        }, 
        {
            "location": "/setupaws/#using-other-mysql-users", 
            "text": "If you don't want to use  root  account for the synchronization API, then you may create a new user and password on MySQL and:   Give the new user:  GRANT LOAD FROM S3 ON *.* TO 'mynewuser';   Set the following Global Privileges:", 
            "title": "Using other MySQL Users"
        }, 
        {
            "location": "/setupaws/#console-access-to-bucket", 
            "text": "Bipost synchronization uses S3 to upload the data that is extracted from the on-prem database. The bucket is located within Factor BI AWS account so we can efficiently handle API calls, patches and new releases.  We create a unique S3 bucket for each customer so nothing gets mixed up.  Sometimes you may want to access this bucket and review files and folders.  Write us to provide this access:  info@factorbi.com", 
            "title": "Console Access to Bucket"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting\n\n\nA successful synchronization shows the following dialog.\n\n\n\n\nIf you face any errors, try looking here.\n\n\n\n\nSyntax error\n\n\nMisspelled fields or tables on customData.json may appear as:\n\n\n\n\n\n\nNo Internet Connection\n\n\nWhen no internet connection available, the following message appears:\n\n\n\n\n\n\nFirewall Restrictions\n\n\nIf your internet connection has a firewall, it may show different errors like:\n\n\n\n\nThe remote name could not be resolved.\n\n\nA WebException with status SendFailure was thrown.\n\n\nA WebException with status NameResolutionFailure was thrown.\n\n\nError making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.\n\n\n\n\n\n\nGrant Firewall to reach Amazon S3\n\n\nCreate a policy to Allow to:\n\n\n\n\n54.230.0.0/15\n\n\n52.192.0.0/11\n\n\n\n\n\n\nAWS can have multiple IP addresses for S3 service, so in case the above IP's don't work check the \nAWS Public IP Address Ranges documentation\n \n and look for\n\n\n  \"region\": \"GLOBAL\",\n  \"service\": \"AMAZON\"\n\n\n\nand\n\n\n  \"region\": \"us-east-1\",\n  \"service\": \"AMAZON\"\n\n\n\nhttps://ip-ranges.amazonaws.com/ip-ranges.json\n\n\n\n\nNo information to Sync\n\n\nIf \nNo information to Sync\n message appears, verify that customData.json is set to send at least one table.\n\n\n\n\nWaiting time\n\n\nBipost Sync may take from a few seconds to several minutes to extract from on-prem DB and upload to AWS. While this is happening no messages/icons will show that biPost.exe is working and maybe you'll see \n(Not responding)\n on the top of the window, this is normal.\n\n\nIf you launch Windows Task Manager probably you'll see that \nbiPost.exe *32\n is running and consuming a considerable amount of CPU.\n\n\nOnce the information is uploaded to AWS, it is usually available on Aurora-MySQL very fast. If a big data set was uploaded it may take up to 5 minutes to be available on Aurora.\n\n\nVerify which tables where loaded by querying \naurora_s3_load_history\n table like this:\n\n\nSELECT * FROM mysql.aurora_s3_load_history WHERE file_name REGEXP 'mytablename' ORDER BY load_timestamp desc;\n\n\n\nOptionally convert \nload_timestamp\n to your local time, e.g.: \nCONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')\n\n\n\n\nUpload Limit\n\n\nDepending on the number of rows and columns on each table, it is possible that a large amount of data sent on a single sync may not load to Aurora-MySQL.\n\n\nWe have tested up to 1.5 million rows on a single sync or 280 MB uncompressed files.\n\n\nWe recommend using \nRecursive Sync\n for big tables that have a date field available.\n\n\n\n\nSpecial Characters\n\n\nLine breaks are not supported and thus removed.\n\n\n\n\nSchema Limitations\n\n\nOnly tables with a \nPRIMARY KEY\n are available to synchronize. If a table does not have a PRIMARY KEY an error message will appear.\n\n\nAs a workaround, you can manually create the tables with a PRIMARY KEY on Aurora-MySQL and then synchronize. On-prem schema changes (e.g. adding columns) will not synchronize unless on-prem tables use primary keys.\n\n\n\n\nMySQL schemas are created but no data is loaded\n\n\nTwo things might be causing this problem:\n\n\n1. RDS instance cannot reach S3 bucket.\n\n\nWhen we look at our CloudWatch logs, we see \nUnable to initialize S3Stream\n, so do the following:\n\n\n\n\nGo to \nRDS Clusters\n and check if \nIAM Role\n is listed and active for your cluster.\n\n\n\n\nIf you \nmanually\n created AWS Services then:\n\n\n\n\n\n\nCheck if your \nIAM Policy to Grant Access to S3\n is set correctly using the S3 bucket ARN we provided. Also double check the policy document (JSON).\n\n\n\n\n\n\nCheck that \nIAM Role\n has attached the former IAM Policy. Copy ARN Role to a notepad for next steps.\n\n\n\n\n\n\nGo to \nRDS Parameter Groups\n, select the cluster group and click \nCompare Parameters\n, it should show the IAM ARN Role (the one you just copied on a notepad) on the parameters shown \nhere.\n\n\n\n\n\n\nDouble check IAM roles attached to your instance querying \nshow global variables like '%role%'\n\n\n\n\n\n\n\n\nAfter this, if you still experience this error, check out \nManually debugging S3Stream.\n\n\n2. Name of your MySQL database must be all lower case.\n\n\nWhen we look at our CloudWatch logs, we see:\n\n\nSequelizeConnectionError: ER_BAD_DB_ERROR: Unknown database\n\n\nDouble check that your DB name is all lower case.\n\n\n\n\nDebugging S3Stream\n\n\nIn this section we will manually upload data to Aurora-MySQL. The goal here is to see whether an error is shown while directly importing data from S3 to Aurora-MySQL.\n\n\nCreate Dummy Table\n\n\nUsing MySQL Workbench open a connection to your MySQL instance using \nroot\n account.\n\n\nLet's create a dummy table:\n\n\nCREATE TABLE `dummytable` (\n  `CIUDAD_ID` int(4) NOT NULL,\n  `NOMBRE` varchar(50) COLLATE latin1_spanish_ci NOT NULL,\n  `CLAVE_FISCAL` varchar(3) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `ES_PREDET` char(1) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `ESTADO_ID` int(4) NOT NULL,\n  `USUARIO_CREADOR` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `FECHA_HORA_CREACION` datetime DEFAULT NULL,\n  `USUARIO_AUT_CREACION` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `USUARIO_ULT_MODIF` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `FECHA_HORA_ULT_MODIF` datetime DEFAULT NULL,\n  `USUARIO_AUT_MODIF` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  PRIMARY KEY (`CIUDAD_ID`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1 COLLATE=latin1_spanish_ci;\n\n\n\nLOAD DATA FROM S3\n\n\nReplace curly brackets with your parameters:\n\n\n\n\nmy-bucket-name\n: Name of your dedicated bucket.\n\n\nmy-database-name\n: Name of your database on MySQL.\n\n\n\n\nNow run on Workbench:\n\n\nLOAD DATA FROM S3 's3-us-east-1://{my-bucket-name}/DUMMYTABLE.csv'\nREPLACE INTO TABLE `{my-database-name}`.`dummytable`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CIUDAD_ID,NOMBRE,CLAVE_FISCAL,ES_PREDET,ESTADO_ID,USUARIO_CREADOR,FECHA_HORA_CREACION,USUARIO_AUT_CREACION,USUARIO_ULT_MODIF,FECHA_HORA_ULT_MODIF,USUARIO_AUT_MODIF);\n\n\n\nExample\n\n\nLOAD DATA FROM S3 's3-us-east-1://bipostdata-123456789012/DUMMYTABLE.csv'\nREPLACE INTO TABLE `mytestdb`.`dummytable`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CIUDAD_ID,NOMBRE,CLAVE_FISCAL,ES_PREDET,ESTADO_ID,USUARIO_CREADOR,FECHA_HORA_CREACION,USUARIO_AUT_CREACION,USUARIO_ULT_MODIF,FECHA_HORA_ULT_MODIF,USUARIO_AUT_MODIF);\n\n\n\n\n\n\n\nLOAD FROM S3 privileges\n\n\nThe Aurora user that executes \nLOAD DATA FROM S3\n requires the following privilege:\n\n\nGRANT LOAD FROM S3 ON *.* TO 'your-user-name';\n\n\nBy default this privilege is set to your \nMaster Username\n when you \ncreated your Aurora instance.\n\n\nIf you are using a different user and the privilege is not set, the following error appears:\n\n\nAccess denied; you need (at least one of) the LOAD FROM S3 privilege(s) for this operation\n\n\nThe only way to see this error is executing \nLOAD FROM S3\n \nmanually.\n\n\nIf your MySQL user already has this privilege and you see the following error, try \nthese steps.\n\n\nAccess denied for user 'your-user-name'@'xx.xx.xxx.xxx' (using password: YES)\n\n\n\n\nFirebird column name starts with underscore\n\n\nFirebird SQL does not naturally support creating colums names starting with underscore, so avoid that on Aurora if your on-prem DB is Firebird.\n\n\nToken unknown - line 1\n\n\n\n\n\n\nNeed more help?\n\n\nPlease send us an email to: \ninfo@factorbi.com", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "A successful synchronization shows the following dialog.   If you face any errors, try looking here.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#syntax-error", 
            "text": "Misspelled fields or tables on customData.json may appear as:", 
            "title": "Syntax error"
        }, 
        {
            "location": "/troubleshooting/#no-internet-connection", 
            "text": "When no internet connection available, the following message appears:", 
            "title": "No Internet Connection"
        }, 
        {
            "location": "/troubleshooting/#firewall-restrictions", 
            "text": "If your internet connection has a firewall, it may show different errors like:   The remote name could not be resolved.  A WebException with status SendFailure was thrown.  A WebException with status NameResolutionFailure was thrown.  Error making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.", 
            "title": "Firewall Restrictions"
        }, 
        {
            "location": "/troubleshooting/#grant-firewall-to-reach-amazon-s3", 
            "text": "Create a policy to Allow to:   54.230.0.0/15  52.192.0.0/11    AWS can have multiple IP addresses for S3 service, so in case the above IP's don't work check the  AWS Public IP Address Ranges documentation    and look for    \"region\": \"GLOBAL\",\n  \"service\": \"AMAZON\"  and    \"region\": \"us-east-1\",\n  \"service\": \"AMAZON\"  https://ip-ranges.amazonaws.com/ip-ranges.json", 
            "title": "Grant Firewall to reach Amazon S3"
        }, 
        {
            "location": "/troubleshooting/#no-information-to-sync", 
            "text": "If  No information to Sync  message appears, verify that customData.json is set to send at least one table.", 
            "title": "No information to Sync"
        }, 
        {
            "location": "/troubleshooting/#waiting-time", 
            "text": "Bipost Sync may take from a few seconds to several minutes to extract from on-prem DB and upload to AWS. While this is happening no messages/icons will show that biPost.exe is working and maybe you'll see  (Not responding)  on the top of the window, this is normal.  If you launch Windows Task Manager probably you'll see that  biPost.exe *32  is running and consuming a considerable amount of CPU.  Once the information is uploaded to AWS, it is usually available on Aurora-MySQL very fast. If a big data set was uploaded it may take up to 5 minutes to be available on Aurora.  Verify which tables where loaded by querying  aurora_s3_load_history  table like this:  SELECT * FROM mysql.aurora_s3_load_history WHERE file_name REGEXP 'mytablename' ORDER BY load_timestamp desc;  Optionally convert  load_timestamp  to your local time, e.g.:  CONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')", 
            "title": "Waiting time"
        }, 
        {
            "location": "/troubleshooting/#upload-limit", 
            "text": "Depending on the number of rows and columns on each table, it is possible that a large amount of data sent on a single sync may not load to Aurora-MySQL.  We have tested up to 1.5 million rows on a single sync or 280 MB uncompressed files.  We recommend using  Recursive Sync  for big tables that have a date field available.", 
            "title": "Upload Limit"
        }, 
        {
            "location": "/troubleshooting/#special-characters", 
            "text": "Line breaks are not supported and thus removed.", 
            "title": "Special Characters"
        }, 
        {
            "location": "/troubleshooting/#schema-limitations", 
            "text": "Only tables with a  PRIMARY KEY  are available to synchronize. If a table does not have a PRIMARY KEY an error message will appear.  As a workaround, you can manually create the tables with a PRIMARY KEY on Aurora-MySQL and then synchronize. On-prem schema changes (e.g. adding columns) will not synchronize unless on-prem tables use primary keys.", 
            "title": "Schema Limitations"
        }, 
        {
            "location": "/troubleshooting/#mysql-schemas-are-created-but-no-data-is-loaded", 
            "text": "Two things might be causing this problem:", 
            "title": "MySQL schemas are created but no data is loaded"
        }, 
        {
            "location": "/troubleshooting/#1-rds-instance-cannot-reach-s3-bucket", 
            "text": "When we look at our CloudWatch logs, we see  Unable to initialize S3Stream , so do the following:   Go to  RDS Clusters  and check if  IAM Role  is listed and active for your cluster.   If you  manually  created AWS Services then:    Check if your  IAM Policy to Grant Access to S3  is set correctly using the S3 bucket ARN we provided. Also double check the policy document (JSON).    Check that  IAM Role  has attached the former IAM Policy. Copy ARN Role to a notepad for next steps.    Go to  RDS Parameter Groups , select the cluster group and click  Compare Parameters , it should show the IAM ARN Role (the one you just copied on a notepad) on the parameters shown  here.    Double check IAM roles attached to your instance querying  show global variables like '%role%'     After this, if you still experience this error, check out  Manually debugging S3Stream.", 
            "title": "1. RDS instance cannot reach S3 bucket."
        }, 
        {
            "location": "/troubleshooting/#2-name-of-your-mysql-database-must-be-all-lower-case", 
            "text": "When we look at our CloudWatch logs, we see:  SequelizeConnectionError: ER_BAD_DB_ERROR: Unknown database  Double check that your DB name is all lower case.", 
            "title": "2. Name of your MySQL database must be all lower case."
        }, 
        {
            "location": "/troubleshooting/#debugging-s3stream", 
            "text": "In this section we will manually upload data to Aurora-MySQL. The goal here is to see whether an error is shown while directly importing data from S3 to Aurora-MySQL.", 
            "title": "Debugging S3Stream"
        }, 
        {
            "location": "/troubleshooting/#create-dummy-table", 
            "text": "Using MySQL Workbench open a connection to your MySQL instance using  root  account.  Let's create a dummy table:  CREATE TABLE `dummytable` (\n  `CIUDAD_ID` int(4) NOT NULL,\n  `NOMBRE` varchar(50) COLLATE latin1_spanish_ci NOT NULL,\n  `CLAVE_FISCAL` varchar(3) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `ES_PREDET` char(1) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `ESTADO_ID` int(4) NOT NULL,\n  `USUARIO_CREADOR` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `FECHA_HORA_CREACION` datetime DEFAULT NULL,\n  `USUARIO_AUT_CREACION` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `USUARIO_ULT_MODIF` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  `FECHA_HORA_ULT_MODIF` datetime DEFAULT NULL,\n  `USUARIO_AUT_MODIF` varchar(31) COLLATE latin1_spanish_ci DEFAULT NULL,\n  PRIMARY KEY (`CIUDAD_ID`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1 COLLATE=latin1_spanish_ci;", 
            "title": "Create Dummy Table"
        }, 
        {
            "location": "/troubleshooting/#load-data-from-s3", 
            "text": "Replace curly brackets with your parameters:   my-bucket-name : Name of your dedicated bucket.  my-database-name : Name of your database on MySQL.   Now run on Workbench:  LOAD DATA FROM S3 's3-us-east-1://{my-bucket-name}/DUMMYTABLE.csv'\nREPLACE INTO TABLE `{my-database-name}`.`dummytable`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CIUDAD_ID,NOMBRE,CLAVE_FISCAL,ES_PREDET,ESTADO_ID,USUARIO_CREADOR,FECHA_HORA_CREACION,USUARIO_AUT_CREACION,USUARIO_ULT_MODIF,FECHA_HORA_ULT_MODIF,USUARIO_AUT_MODIF);  Example  LOAD DATA FROM S3 's3-us-east-1://bipostdata-123456789012/DUMMYTABLE.csv'\nREPLACE INTO TABLE `mytestdb`.`dummytable`\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY '|'\nENCLOSED BY '\\\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES\n(CIUDAD_ID,NOMBRE,CLAVE_FISCAL,ES_PREDET,ESTADO_ID,USUARIO_CREADOR,FECHA_HORA_CREACION,USUARIO_AUT_CREACION,USUARIO_ULT_MODIF,FECHA_HORA_ULT_MODIF,USUARIO_AUT_MODIF);", 
            "title": "LOAD DATA FROM S3"
        }, 
        {
            "location": "/troubleshooting/#load-from-s3-privileges", 
            "text": "The Aurora user that executes  LOAD DATA FROM S3  requires the following privilege:  GRANT LOAD FROM S3 ON *.* TO 'your-user-name';  By default this privilege is set to your  Master Username  when you  created your Aurora instance.  If you are using a different user and the privilege is not set, the following error appears:  Access denied; you need (at least one of) the LOAD FROM S3 privilege(s) for this operation  The only way to see this error is executing  LOAD FROM S3   manually.  If your MySQL user already has this privilege and you see the following error, try  these steps.  Access denied for user 'your-user-name'@'xx.xx.xxx.xxx' (using password: YES)", 
            "title": "LOAD FROM S3 privileges"
        }, 
        {
            "location": "/troubleshooting/#firebird-column-name-starts-with-underscore", 
            "text": "Firebird SQL does not naturally support creating colums names starting with underscore, so avoid that on Aurora if your on-prem DB is Firebird.  Token unknown - line 1", 
            "title": "Firebird column name starts with underscore"
        }, 
        {
            "location": "/troubleshooting/#need-more-help", 
            "text": "Please send us an email to:  info@factorbi.com", 
            "title": "Need more help?"
        }, 
        {
            "location": "/about/", 
            "text": "About Us\n\n\nBipost Sync is developed by \nFactor BI\n \ud83c\uddf2\ud83c\uddfd a company dedicated to connect on-premises ERP's and POS systems to AWS Cloud.\n\n\n\n\nEnterprise Applications\n\n\nWe also build tailored made serverless enterprise web application connected to on-premises Relational Database Systems.\n\n\n\n\n\n\nContact Us\n\n\nTell us about your project: \ninfo@factorbi.com", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-us", 
            "text": "Bipost Sync is developed by  Factor BI  \ud83c\uddf2\ud83c\uddfd a company dedicated to connect on-premises ERP's and POS systems to AWS Cloud.", 
            "title": "About Us"
        }, 
        {
            "location": "/about/#enterprise-applications", 
            "text": "We also build tailored made serverless enterprise web application connected to on-premises Relational Database Systems.", 
            "title": "Enterprise Applications"
        }, 
        {
            "location": "/about/#contact-us", 
            "text": "Tell us about your project:  info@factorbi.com", 
            "title": "Contact Us"
        }
    ]
}