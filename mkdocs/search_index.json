{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nBipost is a simple database synchronization tool built for developers in mind. \n\n\nCurrently it can synchronize \nMicrosoft SQL Server\u00ae\n and \nFirebird SQL\n (running on Windows) to any given \nAmazon Aurora MySQL\n database.\n\n\nIt is created to keep your databases on-premises while providing a way to asynchronous extract and load specific sets of data to AWS Aurora.\n\n\nIt uses a similar \nExtract, Transform \n Load (ETL)\n technique with recurring syncing.\n\n\n\n\n\n\nHow it works\n\n\n\n\nOn every sync biPost.exe reads table schema's and data, creates a compressed file and uploads it to S3.\n\n\nDestination bucket triggers an API which reads schema's, creates database and tables (if they don't exist) and loads the data.\n\n\nOn every upload MySQL uses a \nREPLACE\n command so if you set the appropriate primary keys on source database, data would be updated in destination (and avoid duplicates).\n\n\nYou can customize query criteria by using \ncustomData.json\n file.\n\n\nbiPost.exe can run manually or automatically with a scheduled \nWindows Task.\n\n\n\n\n\n\nPrivate Cloud\n\n\nBipost API allows to load data to private AWS accounts.\n\n\nEach RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.\n\n\n\n\nArchitecture\n\n\n\n\n\n\nBipost Sync is built with \nAWS SDK for .NET\n\n\nUpload to S3 is done through AWS SDK technology.\n\n\nS3 triggers an event and calls Bipost API built with \nLambda.\n\n\nLambda reads data and credentials and proceeds to connect to MySQL-Aurora.\n\n\nMySQL-Aurora\n is a managed database service, you don't have to worry about servers, software patching and other managing tasks.\n\n\nAurora loads data from S3 dedicated bucket.\n\n\n\n\nDone! Start building something amazing on top! \n\n\nCheck the \nBusiness Intelligence use case.\n\n\n\n\nRoad Map\n\n\nWe are currently developing a secure web page to automate all settings that are exchanged over email. Thank you for your patience and we look forward to provide a world-class service.\n\n\n\n\nRelease Notes\n\n\n0.4.0\n\n\n\n\nCustom connections added.\n\n\nInitial statement\n added to API.\n\n\nSpecial characters are deleted on string columns of 100 characters length or up.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Bipost is a simple database synchronization tool built for developers in mind.   Currently it can synchronize  Microsoft SQL Server\u00ae  and  Firebird SQL  (running on Windows) to any given  Amazon Aurora MySQL  database.  It is created to keep your databases on-premises while providing a way to asynchronous extract and load specific sets of data to AWS Aurora.  It uses a similar  Extract, Transform   Load (ETL)  technique with recurring syncing.", 
            "title": "Overview"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "On every sync biPost.exe reads table schema's and data, creates a compressed file and uploads it to S3.  Destination bucket triggers an API which reads schema's, creates database and tables (if they don't exist) and loads the data.  On every upload MySQL uses a  REPLACE  command so if you set the appropriate primary keys on source database, data would be updated in destination (and avoid duplicates).  You can customize query criteria by using  customData.json  file.  biPost.exe can run manually or automatically with a scheduled  Windows Task.", 
            "title": "How it works"
        }, 
        {
            "location": "/#private-cloud", 
            "text": "Bipost API allows to load data to private AWS accounts.  Each RDS Aurora instance loads data by accessing a dedicated bucket, exclusive to your AWS account.", 
            "title": "Private Cloud"
        }, 
        {
            "location": "/#architecture", 
            "text": "Bipost Sync is built with  AWS SDK for .NET  Upload to S3 is done through AWS SDK technology.  S3 triggers an event and calls Bipost API built with  Lambda.  Lambda reads data and credentials and proceeds to connect to MySQL-Aurora.  MySQL-Aurora  is a managed database service, you don't have to worry about servers, software patching and other managing tasks.  Aurora loads data from S3 dedicated bucket.   Done! Start building something amazing on top!   Check the  Business Intelligence use case.", 
            "title": "Architecture"
        }, 
        {
            "location": "/#road-map", 
            "text": "We are currently developing a secure web page to automate all settings that are exchanged over email. Thank you for your patience and we look forward to provide a world-class service.", 
            "title": "Road Map"
        }, 
        {
            "location": "/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/#040", 
            "text": "Custom connections added.  Initial statement  added to API.  Special characters are deleted on string columns of 100 characters length or up.", 
            "title": "0.4.0"
        }, 
        {
            "location": "/setupaws/", 
            "text": "Link your AWS Account\n\n\nFollow these instructions to link your AWS Account and RDS Aurora instance to Bipost API.\n\n\nIMPORTANT NOTICE: Many settings suggested here are for testing purposes. If you are to use the following AWS services for production you may want to follow your company policies and understand how to use AWS security according to your needs.\n\n\n\n\nDon't have an AWS account?\n\n\n\n\n\n\nCreate an AWS account here \naws.amazon.com\n\n\n\n\n\n\n\n\nAWS usually makes an automated verification phone call, we suggest to provide a land line.\n\n\n\n\nProvide payment information.\n\n\nSelect Basic Support (free plan).\n\n\nCheck if you can open \nRDS Dashboard\n, by searching under \nAWS services\n.\n\n\nCongrats you have an AWS account!\n\n\n\n\n\n\nCheck closest AWS Region to you location\n\n\ncloudping.info\n\n\nClick the link above and hit \nHTTP Ping\n and look for the lowest latency.\n\n\nMaybe you want to try this at different times of the day.\n\n\nTake note of the closest region.\n\n\n\n\n\n\nGet Canonical User ID from your IAM Home\n\n\n\n\nUpper right corner of your AWS console, click your account name (or follow next link).\n\n\nMy Security Credentials.\n\n\nClick \nContinue to Security Credentials\n if dialog appears.\n\n\nAccount Identifiers.\n\n\n\n\nCopy AWS Account ID and Canonical User ID.\n\n\n\n\nCopy the entire string that starts with \nAWS Account ID\n up to the very end of a long hex string of 64 characters.\n\n\n\n\n\n\n\n\n\n\nEmail the above to \njaime@factorbi.com\n so we can configure a dedicated bucket for your API calls.\n\n\n\n\n\n\nQ: Is it secure to provide these numbers?\n\n\nYes, we use your \nCanonical User ID\n to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.\n\n\n\n\nCreate Aurora Instance\n\n\nAurora DB Details\n\n\n\n\nFrom AWS Console Home, upper right corner (next to you name) be sure to select the \nclosest region to your location.\n\n\nFrom AWS Console Home, search RDS.\n\n\nFrom \nRDS Dashboard\n, click Instances.\n\n\nLaunch DB Instance, blue button.\n\n\nSelect Engine: \nAmazon Aurora\n, click select.\n\n\nDB Instance Class: for testing purposes select the smallest available, currently \nt2.small\n\n\nMulti-AZ Deployment: for testing purposes select \nNo\n\n\nDB Instance Identifier: assign a name, lower-case and no special characters.\n\n\nMaster Username: \nroot\n\n\nMaster Password: assign a hard password and store it in a secure place.\n\n\nConfirm Password.\n\n\nOn your left pane it is displayed an estimated monthly cost. For further information check On-Demand Pricing: \nRDS Pricing\n\n\nClick Next Step, blue button.\n\n\n\n\n\n\nAurora Network \n Security\n\n\n\n\nVPC: \nCreate new VPC\n\n\nSubnet Group: \nCreate new DB Subnet Group\n\n\nPublicly Accessible: \nYes\n\n\nAvailability Zone: \nNo Preference\n\n\nVPC Security Group: \nCreate new Security Group\n\n\n\n\n\n\nAurora Database Options\n\n\n\n\nDB Cluster Identifier: \nleave blank\n\n\nDatabase Name: \nleave blank\n\n\nDatabase Port: \n3306\n\n\nDB Parameter Group: \ndefault.aurora5.6\n\n\nDB Cluster Parameter Group: \ndefault.aurora5.6\n\n\nOption Group: \nleave default\n\n\nEnable Encryption: \nNo\n\n\n\n\nAurora Failover\n\n\n\n\nPriority: \ntier-0\n\n\n\n\nAurora Backup\n\n\n\n\nBackup Retention Period: \n1 day\n\n\n\n\nAurora Monitoring\n\n\n\n\nEnable Enhanced Monitoring: \nNo\n\n\n\n\nAurora Maintenance\n\n\n\n\nAuto Minor Version Upgrade: \nYes\n\n\nMaintenance Windows: \nNo Preference\n\n\n\n\n\n\nLaunch DB Instance\n\n\n\n\n\n\nClick Launch DB Instance blue button.\n\n\n\n\n\n\nThis process may take a while, sometimes 30 minutes or more.\n\n\n\n\n\n\nYou can check \nStatus\n of your instance by going to \nInstances\n on left navigation pane.\n\n\n\n\n\n\n\n\nRDS Instance Security Group\n\n\nOnce the instance has \nStatus:\n \navailable\n proceed:\n\n\n\n\nClick check box way left of your DB Instance name.\n\n\nClick \nInstance Actions \\ See Details\n gray button, on top.\n\n\n\n\nLookup for \nSecurity Groups\n and click the blue string to the right, it may appear as \n\n\n\n\ndefault (sg-XXXXXXXX)\n\n\n\n\n\n\n\n\n\n\nYou are now on EC2 Dashboard and Security Group ID is already selected.\n\n\n\n\nClick \nActions \\ Edit inbound rules\n\n\nClick \nAdd Rule\n, under Type select \nMYSQL/Aurora\n\n\nSource \nCustom\n and type value: \n0.0.0.0/0\n\n\n\n\nRepeat steps 6 \n 7, and type value \n::/0\n\n\n\n\n\n\n\n\nClick \nSave\n blue button.\n\n\n\n\nClick \nActions \\ Edit outbound rules\n\n\nVerify if Type: \nAll traffic\n, Destination: \nCustom\n and value: \n0.0.0.0/0\n is already set, if not, add the rule. \n\n\nGo back to \nRDS Dashboard\n, select your instance, click \nInstance Actions \\ Reboot\n, confirm with blue button on the right.\n\n\nWait until \nStatus\n is \navailable\n and check if \nSecurity Groups\n are \n( active )\n\n\n\n\n\n\nCreate IAM Policy to Grant Access to S3\n\n\nFrom this point on you need the newly S3 bucket ARN that we provided over email. \n\n\nIf you haven't emailed us with your Canonical User ID, please follow \nthese steps.\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nPolicies.\n\n\nCreate policy\n blue button.\n\n\n\n\nSelect \nPolicy Generator\n\n\n\n\nEffect: \nAllow\n\n\nAWS Service: \nAmazon S3\n\n\nActions: check \nGetObject\n and \nGetObjectVersion\n\n\nAmazon Resource Name (ARN): \narn you received over email\n, example: \n\n\narn:aws:s3:::bipost-000111222\n\n\n\n\n\n\n\n\nClick \nAdd Statement\n\n\n\n\n\n\nRepeat step 4 adding \n/*\n at the end of ARN bucket string, as follows:\n\n\n\n\nEffect: \nAllow\n\n\nAWS Service: \nAmazon S3\n\n\nActions: check \nGetObject\n and \nGetObjectVersion\n\n\nAmazon Resource Name (ARN): example: \narn:aws:s3:::bipost-000111222/*\n\n\n\n\n\n\n\n\n\n\nClick \nNext Step\n blue button.\n\n\n\n\nPolicy Name:\n \nAllowAuroraToS3\n\n\nOptionally add \nDescription\n.\n\n\n\n\nPolicy Document\n: double check that JSON looks like this:\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt9999999999999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222\"\n            ]\n        },\n        {\n            \"Sid\": \"Stmt9999999777999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222/*\"\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\n\nClick \nCreate Policy\n\n\n\n\n\n\nFurther information from AWS go to: \nAllowing Amazon Aurora to Access Amazon S3 Resources\n\n\n\n\nCreate IAM Role to Allow RDS Access to S3\n\n\n\n\nOpen \nIAM Console.\n\n\nIn the left navigation pane choose \nRoles.\n\n\nCreate New Role\n blue button.\n\n\n\n\nChoose \nAWS Service Role,\n scroll down and select \nAmazon RDS\n\n\n\n\n\n\n\n\nAttach Policy,\n leave blank and click \nNext Step\n blue button.\n\n\n\n\nRole name: \nRDSLoadFromS3\n\n\nClick \nCreate role\n blue button.\n\n\nClick on your newly created role. This will open a Summary.\n\n\n\n\nUnder Permissions, click \nAttach Policy\n blue button.\n\n\n\n\n\n\n\n\nUse Filter and select \nPolicy Type: Customer Managed\n\n\n\n\nClick the check box of your newly created Policy: \nAllowAuroraToS3\n\n\nClick \nAttach Policy\n blue button.\n\n\nCopy \nRole ARN\n string and save it for further use. It may look like this: \narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\nFurther information from AWS go to: \nCreating an IAM Role to Allow Amazon Aurora to Access AWS Services\n\n\n\n\nSet IAM Role to Aurora Cluster\n\n\n\n\nOpen \nRDS console.\n\n\nChoose \nClusters\n on left pane.\n\n\nClick check box of your newly cluster.\n\n\nClick \nManage IAM Roles\n gray button, on top.\n\n\nSelect the role you just created: \nRDSLoadFromS3\n and click \nDone\n, blue button.\n\n\n\n\n\n\nCreate Cluster Parameter Group\n\n\nIf you are already using a custom DB Cluster Parameter Group, you can select that group instead of creating a new DB Cluster Parameter Group.\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate Parameter Group\n blue button on top.\n\n\n\n\nParameter Group Family: \naurora5.6\n\n\nType: \nDB Cluster Parameter Group\n\n\nGroup Name: \nAuroraClusterAllowAWSAccess\n\n\nDescription: \nAllow cluster access to Amazon S3\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button.\n\n\n\n\nClick check box on your new \nauroraclusterallowawsaccess\n parameter group and click \nEdit Parameters\n gray button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\nExample\n\n\n\n\n\n\n\n\n\n\naurora_load_from_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naurora_select_into_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\naws_default_s3_role\n\n\npaste \nRole ARN string\n\n\narn:aws:iam::123456789012:role/RDSLoadFromS3\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n blue button.\n\n\n\n\n\n\nFurther information from AWS go to: \nAssociating an IAM Role with a DB Cluster\n\n\n\n\nCreate DB Parameter Group\n\n\nIf you are already using a custom DB Parameter Group, you can select that group instead of creating a new DB Parameter Group.\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nParameter Groups.\n\n\n\n\nClick \nCreate Parameter Group\n blue button on top.\n\n\n\n\nParameter Group Family: \naurora5.6\n\n\nType: \nDB Parameter Group\n\n\nGroup Name: \nAuroraInstanceAllowAWSAccess\n\n\nDescription: \nAllow instance access to Amazon S3\n\n\n\n\n\n\n\n\nClick \nCreate\n blue button.\n\n\n\n\nClick check box on your new \naurorainstanceallowawsaccess\n parameter group and click \nEdit Parameters\n gray button on top.\n\n\n\n\nSet the following:\n\n\n\n\n\n\n\n\nName\n\n\nEdit Values\n\n\n\n\n\n\n\n\n\n\nlog_bin_trust_function_creators\n\n\n1\n\n\n\n\n\n\nmax_allowed_packet\n\n\n1073741824\n\n\n\n\n\n\nmax_connections\n\n\n16000\n\n\n\n\n\n\nmax_user_connections\n\n\n4294967295\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave Changes\n blue button.\n\n\n\n\n\n\n\n\nSet Cluster Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nClusters.\n\n\nClick check box on your new cluster.\n\n\nClick \nModify Cluster\n gray button on top.\n\n\nUnder Database Options, set \nDB Cluster Parameter Group\n to \nauroraclusterallowawsaccess\n.\n\n\nClick check box \nApply Immediately\n and click \nContinue\n blue button.\n\n\nReview changes and click \nModify Cluster\n blue button.\n\n\n\n\nSet Instance Parameter Group\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick check box on your new instance.\n\n\nClick \nInstance Actions \\ Modify\n gray button on top.\n\n\nUnder Database Options, set \nDB Parameter Group\n to \naurorainstanceallowawsaccess\n\n\nYou may also notice that \nDB Cluster Parameter Group\n is set to \nauroraclusterallowawsaccess\n\n\nClick check box \nApply Immediately\n and click \nContinue\n blue button.\n\n\nReview changes and click \nModify DB Instance\n blue button.\n\n\nClick \nInstance Actions \\ Reboot\n gray button on top.\n\n\nConfirm reboot with blue button.\n\n\n\n\n\n\nVerify Instance Configuration\n\n\n\n\nOpen \nRDS console.\n\n\nOn left pane go to \nInstances.\n\n\nClick check box on your new instance.\n\n\nClick \nInstance Actions \\ See Details\n gray button on top.\n\n\n\n\nVerify the following:\n\n\n\n\nEnpoint: \n( authorized )\n\n\nParameter Group: \naurorainstanceallowawsaccess ( in-sync )\n\n\nDB Cluster Parameter Group: \nauroraclusterallowawsaccess ( in-sync )\n\n\nSecurity Groups: \ndefault (sg-XXXXXXXX) ( active )\n\n\nPublicly Accessible: \nYes\n\n\nDB Instance Status: \navailable\n\n\n\n\n\n\n\n\n\n\nTest connection to your RDS Aurora\n\n\n\n\n\n\nDownload and install any \nMySQL client\n of your preference: \n\n\nFor Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"\n\n\n\n\n\n\n\nOn your AWS Console go to \nRDS Dashboard\n, select your instance and copy the \nCluster Endpoint\n, which is a blue string with more than 60 characters. \n\n\n\n\n\n\nLaunch your MySQL client and configure a new connection:\n\n\n\n\nName:\n type any name of your preference.\n\n\nHost:\n Paste the Cluster Endpoint and delete the suffix :3306\n\n\nUsername:\n root\n\n\nPassword:\n type the Master Password\n\n\nPort:\n 3306\n\n\nDatabase:\n Leave blank\n\n\nConnect using SSL:\n No\n\n\n\n\n\n\n\n\nClick Connect and verify that you can successfully connect to your RDS instance.\n\n\n\n\n\n\n\n\nSend Instance Connection Details to Factor BI\n\n\nEmail all the information you used to \nTest connection to your RDS Aurora\n to \njaime@factorbi.com\n so we can add your instance to our Bipost API.\n\n\n\n\nSecurity of your RDS Instance for Production\n\n\nIf your are ready to use Bipost API for production, we highly recommend the following:\n\n\n\n\nUse \nMySQL client\n to create a new user.\n\n\nSet a strong password.\n\n\n\n\nSet the following Global Privileges:\n\n\n\n\n\n\n\n\n\n\nConsole Access to Bucket\n\n\nBipost synchronization uses S3 to upload the data that is extracted from the on-premises database. The bucket is located within a Factor BI AWS account so we can efficiently handle  API calls, patches and new releases. \n\n\nRemember, we create a unique S3 bucket for each one of our customers, so nothing gets mixed up.\n\n\nSometimes you may want to access this bucket and review files and folders.\n\n\nTo accomplish this we provide an \nAWS Console access\n with a user, password and a direct link to your bucket.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#link-your-aws-account", 
            "text": "Follow these instructions to link your AWS Account and RDS Aurora instance to Bipost API.  IMPORTANT NOTICE: Many settings suggested here are for testing purposes. If you are to use the following AWS services for production you may want to follow your company policies and understand how to use AWS security according to your needs.", 
            "title": "Link your AWS Account"
        }, 
        {
            "location": "/setupaws/#dont-have-an-aws-account", 
            "text": "Create an AWS account here  aws.amazon.com     AWS usually makes an automated verification phone call, we suggest to provide a land line.   Provide payment information.  Select Basic Support (free plan).  Check if you can open  RDS Dashboard , by searching under  AWS services .  Congrats you have an AWS account!", 
            "title": "Don't have an AWS account?"
        }, 
        {
            "location": "/setupaws/#check-closest-aws-region-to-you-location", 
            "text": "cloudping.info  Click the link above and hit  HTTP Ping  and look for the lowest latency.  Maybe you want to try this at different times of the day.  Take note of the closest region.", 
            "title": "Check closest AWS Region to you location"
        }, 
        {
            "location": "/setupaws/#get-canonical-user-id-from-your-iam-home", 
            "text": "Upper right corner of your AWS console, click your account name (or follow next link).  My Security Credentials.  Click  Continue to Security Credentials  if dialog appears.  Account Identifiers.   Copy AWS Account ID and Canonical User ID.   Copy the entire string that starts with  AWS Account ID  up to the very end of a long hex string of 64 characters.      Email the above to  jaime@factorbi.com  so we can configure a dedicated bucket for your API calls.    Q: Is it secure to provide these numbers?  Yes, we use your  Canonical User ID  to create and provide access to a new and dedicated S3 bucket for your AWS Account. Further on you will link this bucket to you RDS instance.", 
            "title": "Get Canonical User ID from your IAM Home"
        }, 
        {
            "location": "/setupaws/#create-aurora-instance", 
            "text": "", 
            "title": "Create Aurora Instance"
        }, 
        {
            "location": "/setupaws/#aurora-db-details", 
            "text": "From AWS Console Home, upper right corner (next to you name) be sure to select the  closest region to your location.  From AWS Console Home, search RDS.  From  RDS Dashboard , click Instances.  Launch DB Instance, blue button.  Select Engine:  Amazon Aurora , click select.  DB Instance Class: for testing purposes select the smallest available, currently  t2.small  Multi-AZ Deployment: for testing purposes select  No  DB Instance Identifier: assign a name, lower-case and no special characters.  Master Username:  root  Master Password: assign a hard password and store it in a secure place.  Confirm Password.  On your left pane it is displayed an estimated monthly cost. For further information check On-Demand Pricing:  RDS Pricing  Click Next Step, blue button.", 
            "title": "Aurora DB Details"
        }, 
        {
            "location": "/setupaws/#aurora-network-security", 
            "text": "VPC:  Create new VPC  Subnet Group:  Create new DB Subnet Group  Publicly Accessible:  Yes  Availability Zone:  No Preference  VPC Security Group:  Create new Security Group", 
            "title": "Aurora Network &amp; Security"
        }, 
        {
            "location": "/setupaws/#aurora-database-options", 
            "text": "DB Cluster Identifier:  leave blank  Database Name:  leave blank  Database Port:  3306  DB Parameter Group:  default.aurora5.6  DB Cluster Parameter Group:  default.aurora5.6  Option Group:  leave default  Enable Encryption:  No", 
            "title": "Aurora Database Options"
        }, 
        {
            "location": "/setupaws/#aurora-failover", 
            "text": "Priority:  tier-0", 
            "title": "Aurora Failover"
        }, 
        {
            "location": "/setupaws/#aurora-backup", 
            "text": "Backup Retention Period:  1 day", 
            "title": "Aurora Backup"
        }, 
        {
            "location": "/setupaws/#aurora-monitoring", 
            "text": "Enable Enhanced Monitoring:  No", 
            "title": "Aurora Monitoring"
        }, 
        {
            "location": "/setupaws/#aurora-maintenance", 
            "text": "Auto Minor Version Upgrade:  Yes  Maintenance Windows:  No Preference", 
            "title": "Aurora Maintenance"
        }, 
        {
            "location": "/setupaws/#launch-db-instance", 
            "text": "Click Launch DB Instance blue button.    This process may take a while, sometimes 30 minutes or more.    You can check  Status  of your instance by going to  Instances  on left navigation pane.", 
            "title": "Launch DB Instance"
        }, 
        {
            "location": "/setupaws/#rds-instance-security-group", 
            "text": "Once the instance has  Status:   available  proceed:   Click check box way left of your DB Instance name.  Click  Instance Actions \\ See Details  gray button, on top.   Lookup for  Security Groups  and click the blue string to the right, it may appear as    default (sg-XXXXXXXX)      You are now on EC2 Dashboard and Security Group ID is already selected.   Click  Actions \\ Edit inbound rules  Click  Add Rule , under Type select  MYSQL/Aurora  Source  Custom  and type value:  0.0.0.0/0   Repeat steps 6   7, and type value  ::/0     Click  Save  blue button.   Click  Actions \\ Edit outbound rules  Verify if Type:  All traffic , Destination:  Custom  and value:  0.0.0.0/0  is already set, if not, add the rule.   Go back to  RDS Dashboard , select your instance, click  Instance Actions \\ Reboot , confirm with blue button on the right.  Wait until  Status  is  available  and check if  Security Groups  are  ( active )", 
            "title": "RDS Instance Security Group"
        }, 
        {
            "location": "/setupaws/#create-iam-policy-to-grant-access-to-s3", 
            "text": "From this point on you need the newly S3 bucket ARN that we provided over email.   If you haven't emailed us with your Canonical User ID, please follow  these steps.   Open  IAM Console.  In the left navigation pane choose  Policies.  Create policy  blue button.   Select  Policy Generator   Effect:  Allow  AWS Service:  Amazon S3  Actions: check  GetObject  and  GetObjectVersion  Amazon Resource Name (ARN):  arn you received over email , example:   arn:aws:s3:::bipost-000111222     Click  Add Statement    Repeat step 4 adding  /*  at the end of ARN bucket string, as follows:   Effect:  Allow  AWS Service:  Amazon S3  Actions: check  GetObject  and  GetObjectVersion  Amazon Resource Name (ARN): example:  arn:aws:s3:::bipost-000111222/*      Click  Next Step  blue button.   Policy Name:   AllowAuroraToS3  Optionally add  Description .   Policy Document : double check that JSON looks like this:  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt9999999999999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222\"\n            ]\n        },\n        {\n            \"Sid\": \"Stmt9999999777999\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::bipost-000111222/*\"\n            ]\n        }\n    ]\n}    Click  Create Policy    Further information from AWS go to:  Allowing Amazon Aurora to Access Amazon S3 Resources", 
            "title": "Create IAM Policy to Grant Access to S3"
        }, 
        {
            "location": "/setupaws/#create-iam-role-to-allow-rds-access-to-s3", 
            "text": "Open  IAM Console.  In the left navigation pane choose  Roles.  Create New Role  blue button.   Choose  AWS Service Role,  scroll down and select  Amazon RDS     Attach Policy,  leave blank and click  Next Step  blue button.   Role name:  RDSLoadFromS3  Click  Create role  blue button.  Click on your newly created role. This will open a Summary.   Under Permissions, click  Attach Policy  blue button.     Use Filter and select  Policy Type: Customer Managed   Click the check box of your newly created Policy:  AllowAuroraToS3  Click  Attach Policy  blue button.  Copy  Role ARN  string and save it for further use. It may look like this:  arn:aws:iam::123456789012:role/RDSLoadFromS3   Further information from AWS go to:  Creating an IAM Role to Allow Amazon Aurora to Access AWS Services", 
            "title": "Create IAM Role to Allow RDS Access to S3"
        }, 
        {
            "location": "/setupaws/#set-iam-role-to-aurora-cluster", 
            "text": "Open  RDS console.  Choose  Clusters  on left pane.  Click check box of your newly cluster.  Click  Manage IAM Roles  gray button, on top.  Select the role you just created:  RDSLoadFromS3  and click  Done , blue button.", 
            "title": "Set IAM Role to Aurora Cluster"
        }, 
        {
            "location": "/setupaws/#create-cluster-parameter-group", 
            "text": "If you are already using a custom DB Cluster Parameter Group, you can select that group instead of creating a new DB Cluster Parameter Group.   Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create Parameter Group  blue button on top.   Parameter Group Family:  aurora5.6  Type:  DB Cluster Parameter Group  Group Name:  AuroraClusterAllowAWSAccess  Description:  Allow cluster access to Amazon S3     Click  Create  blue button.   Click check box on your new  auroraclusterallowawsaccess  parameter group and click  Edit Parameters  gray button on top.   Set the following:     Name  Edit Values  Example      aurora_load_from_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aurora_select_into_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3    aws_default_s3_role  paste  Role ARN string  arn:aws:iam::123456789012:role/RDSLoadFromS3       Click  Save Changes  blue button.    Further information from AWS go to:  Associating an IAM Role with a DB Cluster", 
            "title": "Create Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#create-db-parameter-group", 
            "text": "If you are already using a custom DB Parameter Group, you can select that group instead of creating a new DB Parameter Group.   Open  RDS console.  On left pane go to  Parameter Groups.   Click  Create Parameter Group  blue button on top.   Parameter Group Family:  aurora5.6  Type:  DB Parameter Group  Group Name:  AuroraInstanceAllowAWSAccess  Description:  Allow instance access to Amazon S3     Click  Create  blue button.   Click check box on your new  aurorainstanceallowawsaccess  parameter group and click  Edit Parameters  gray button on top.   Set the following:     Name  Edit Values      log_bin_trust_function_creators  1    max_allowed_packet  1073741824    max_connections  16000    max_user_connections  4294967295       Click  Save Changes  blue button.", 
            "title": "Create DB Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-cluster-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Clusters.  Click check box on your new cluster.  Click  Modify Cluster  gray button on top.  Under Database Options, set  DB Cluster Parameter Group  to  auroraclusterallowawsaccess .  Click check box  Apply Immediately  and click  Continue  blue button.  Review changes and click  Modify Cluster  blue button.", 
            "title": "Set Cluster Parameter Group"
        }, 
        {
            "location": "/setupaws/#set-instance-parameter-group", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click check box on your new instance.  Click  Instance Actions \\ Modify  gray button on top.  Under Database Options, set  DB Parameter Group  to  aurorainstanceallowawsaccess  You may also notice that  DB Cluster Parameter Group  is set to  auroraclusterallowawsaccess  Click check box  Apply Immediately  and click  Continue  blue button.  Review changes and click  Modify DB Instance  blue button.  Click  Instance Actions \\ Reboot  gray button on top.  Confirm reboot with blue button.", 
            "title": "Set Instance Parameter Group"
        }, 
        {
            "location": "/setupaws/#verify-instance-configuration", 
            "text": "Open  RDS console.  On left pane go to  Instances.  Click check box on your new instance.  Click  Instance Actions \\ See Details  gray button on top.   Verify the following:   Enpoint:  ( authorized )  Parameter Group:  aurorainstanceallowawsaccess ( in-sync )  DB Cluster Parameter Group:  auroraclusterallowawsaccess ( in-sync )  Security Groups:  default (sg-XXXXXXXX) ( active )  Publicly Accessible:  Yes  DB Instance Status:  available", 
            "title": "Verify Instance Configuration"
        }, 
        {
            "location": "/setupaws/#test-connection-to-your-rds-aurora", 
            "text": "Download and install any  MySQL client  of your preference:   For Mac you may use \"Sequel Pro\" or \"MySQL Workbench\"\nFor Windows you may use \"MySQL Workbench\" or \"HeidiSQL\"    On your AWS Console go to  RDS Dashboard , select your instance and copy the  Cluster Endpoint , which is a blue string with more than 60 characters.     Launch your MySQL client and configure a new connection:   Name:  type any name of your preference.  Host:  Paste the Cluster Endpoint and delete the suffix :3306  Username:  root  Password:  type the Master Password  Port:  3306  Database:  Leave blank  Connect using SSL:  No     Click Connect and verify that you can successfully connect to your RDS instance.", 
            "title": "Test connection to your RDS Aurora"
        }, 
        {
            "location": "/setupaws/#send-instance-connection-details-to-factor-bi", 
            "text": "Email all the information you used to  Test connection to your RDS Aurora  to  jaime@factorbi.com  so we can add your instance to our Bipost API.", 
            "title": "Send Instance Connection Details to Factor BI"
        }, 
        {
            "location": "/setupaws/#security-of-your-rds-instance-for-production", 
            "text": "If your are ready to use Bipost API for production, we highly recommend the following:   Use  MySQL client  to create a new user.  Set a strong password.   Set the following Global Privileges:", 
            "title": "Security of your RDS Instance for Production"
        }, 
        {
            "location": "/setupaws/#console-access-to-bucket", 
            "text": "Bipost synchronization uses S3 to upload the data that is extracted from the on-premises database. The bucket is located within a Factor BI AWS account so we can efficiently handle  API calls, patches and new releases.   Remember, we create a unique S3 bucket for each one of our customers, so nothing gets mixed up.  Sometimes you may want to access this bucket and review files and folders.  To accomplish this we provide an  AWS Console access  with a user, password and a direct link to your bucket.", 
            "title": "Console Access to Bucket"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nIf you haven't linked your RDS Instance to Bipost API, please follow these steps: \nLink your AWS account.\n\n\nPrerequisites\n\n\nFor most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.\n\n\nWindow XP \n Vista\n\n\nDownload and install: \n\n\n\n\nWindowsInstaller.\n\n\nPrerequisites for Windows 7 and 8.\n\n\n\n\nWindows 7 and 8\n\n\nDownload and install:\n\n\n\n\ndotNetFx40\n\n\nvcredist\n\n\n\n\nDownload \n Install\n\n\n\n\n\n\nGet latest version here.\n\n\n\n\n\n\nUnzip \nbiPost.zip\n to any folder on your Windows.\n\n\n\n\n\n\nCheck if you can successfully launch \nbiPost.exe\n\n\n\n\n\n\nConfigure biPost.exe", 
            "title": "Download & Install"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "If you haven't linked your RDS Instance to Bipost API, please follow these steps:  Link your AWS account.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#prerequisites", 
            "text": "For most Windows 7 and up there is no need to install the prerequisites. Nevertheless if you face any trouble running biPost.exe, please try with the following.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/installation/#window-xp-vista", 
            "text": "Download and install:    WindowsInstaller.  Prerequisites for Windows 7 and 8.", 
            "title": "Window XP &amp; Vista"
        }, 
        {
            "location": "/installation/#windows-7-and-8", 
            "text": "Download and install:   dotNetFx40  vcredist", 
            "title": "Windows 7 and 8"
        }, 
        {
            "location": "/installation/#download-install", 
            "text": "Get latest version here.    Unzip  biPost.zip  to any folder on your Windows.    Check if you can successfully launch  biPost.exe    Configure biPost.exe", 
            "title": "Download &amp; Install"
        }, 
        {
            "location": "/bipostexe/", 
            "text": "Configuration\n\n\nIMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat configuration settings as sensitive information.\n\n\nFrom this point on you need your \nService No.\n and \nActivation No.\n that we provided over email. \n\n\nIf you haven't emailed us with your Instance Connection Details, please follow \nthese steps.\n\n\n\n\nClick \nConfiguration\n and set:\n\n\n\n\nService No.:\n 36 digit hex number, it may look like this: \na1bcd23e-4fa5-67b8-cd9e-f0123abc4567\n\n\nActivation No.:\n 24 digit hex number, it may look like this: \n5990ab12c3de45f6a78bc90d\n\n\nEngine:\n Select \nFirebird\n or \nSQL\n (Microsoft SQL Server).\n\n\n\n\nSystem:\n Select \nCustom...\n \n\n\nIf you use \nIntelisis ERP\n, \nMicrosip ERP\n or \nAspel SAE\n then select the System according. \n\n\n\n\n\n\n\n\nFirebird Connection\n\n\n\n\n\n\nRemote Connection:\n Enable when you are using biPost.exe over a LAN network, this is, biPost.exe is not running on the same server as the Firebird server.\n\n\n\n\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nPassword:\n Set your Firebird password, sometimes \nmasterkey\n\n\n\n\n\n\nDatabase:\n Location of your \n.FDB\n file.\n\n\n\n\n\n\n\n\nSQL Connection\n\n\n\n\n\n\n\n\nServer:\n IP or name of the server on your LAN network.\n\n\n\n\n\n\nUser:\n Login for your SQL server. It only needs read permissions.\n\n\n\n\n\n\nPassword:\n Password for the Login provided.\n\n\n\n\n\n\nDatabase:\n Name of your database.\n\n\n\n\n\n\n\n\nGeneral Settings\n\n\n\n\n\n\n\n\nSpecific Bucket:\n Enable to use your own AWS Account.\n\n\n\n\nEnter your \nBucket Name\n that we provided over email.\n\n\n\n\n\n\n\n\nInclude Catalogs:\n For use only with \nIntelisis ERP\n system, see \ndetails here.\n\n\n\n\n\n\nRecursive Sync\n\n\nCurrently supported only for SQL Server, when enabled it optimizes upload by extracting and uploading one day at a time for the given date range. \n\n\nIt is very useful to upload historic data.\n\n\nIt is used in combination with \ncustomData.json\n so you can configure the date field to use for each table.\n\n\nWhen \nturned off\n it automatically sets start and end date to yesterday.\n\n\n\n\ncustomData.json\n\n\nThis file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data.\n\n\nAll parameters are supported for SQL Server. For Firebird SQL please review supported parameters \nhere.\n\n\nExample 1, using recursiveDateField:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  }\n]\n\n\n\nExample 2, for catalogs:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\nThe above statements will send all data for the specified tables, as \njoin\n and \nfilter\n are not in use.\n\n\nNote that you can use \n\"active\": \"\",\n\n\n\n\nExample 3, using special filter when no datetime is available:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym \",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) \",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\njoin\n and \nfilter\n can use any syntax supported on SQL Server.\n\n\n\n\nExample 4 with Microsip ERP:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"ATRIBUTO_ID, NOMBRE, NOMBRE_COLUMNA, CLAVE_OBJETO, POSICION, TIPO, LONGITUD, ESCALA, VALOR_MINIMO, VALOR_MAXIMO, VALOR_DEFAULT_NUMERICO, VALOR_DEFAULT_CARACTER, DESCRIPCION \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"IMPUESTO_ART_ID, ARTICULO_ID, IMPUESTO_ID, UNIDADES_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"DOCTO_CM_ID, IMPUESTO_ID, COMPRA_NETA, OTROS_IMPUESTOS, PCTJE_IMPUESTO, IMPORTE_IMPUESTO, UNIDADES_IMPUESTO, IMPORTE_UNITARIO_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CC_IMPTO_ID, IMPTE_DOCTO_CC_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CP_IMPTO_ID, IMPTE_DOCTO_CP_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\n\n\nIn the above example biPost.exe is using \nSystem: Microsip\n and on every sync it sends all the factory embedded tables plus the tables set in customData.json \n\n\n\n\nTables \n Primary Keys\n\n\nTake note of the following:\n\n\n\n\nEvery table specified in \ncustomData.json\n must have a \nPrimary Key\n and these must be listed on \n\"fields\"\n\n\nYou can only setup customData.json with \nbase tables.\n If you use views it may work but it is not supported.\n\n\n\"fields\"\n parameter must only include fields that exist on \n\"table\":\n.\n\n\n\n\nrecursiveDateField\n\n\nThis parameter is used when \nRecursive Sync\n check box is enabled.\n\n\nIt only supports date fields without hour.\n\n\nOn SQL Server using \ndatetime\n data type, it only supports dates ending in \n00:00:00.000\n\n\ncustomData.json supported parameters for Firebird SQL.\n\n\n\n\nactive\n\n\ntable\n\n\nfields\n\n\n\n\nRest of the parameters do not work for Firebird.\n\n\nHandle several customData.json settings\n\n\nIt is very common to make changes to customData.json to upload different sets of data.\n\n\nUse Cases:\n\n\n\n\n\n\nSome tables may be uploaded once since its data is rarely changed, e.g. config \n company tables.\n\n\n\n\n\n\nHistoric data may be uploaded once, e.g. transactions from previous years.\n\n\n\n\n\n\nRecently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.\n\n\n\n\n\n\nRecently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.\n\n\n\n\n\n\nFor all the reasons above, it may be very useful to create two or more folders and copy entire biPost files and just change \ncustomData.json.\n Moreover you may want to have different sync schedules, which are explained next.\n\n\n\n\nSchedule\n\n\n\n\nIf you want automated execution of biPost.exe, then set the \nHour\n desired and click \nSchedule\n.\n\n\nThis will create a Windows Task that will run daily. If you want a different schedule, then open \nWindows Task Scheduler\n as follows.\n\n\nControl Panel \\ Administrative Tools:\n\n\n\n\n\n\nIf you manually create a task to run biPost then use \nargument: post\n\n\n\n\n\n\nCheck for Updates\n\n\nNew versions of biPost can be checked using \nHelp \\ Check for Updates.\n\n\n\n\n\n\n\n\nHandling Different Settings with Folders\n\n\nWe already explained why creating two or more folders is a good workaround for \nhandling different customData.json settings.\n\n\nIt is also possible that you may want to sync more than one database. For this matter proceed creating several folders so each one has it's own settings.", 
            "title": "Bipost.exe"
        }, 
        {
            "location": "/bipostexe/#configuration", 
            "text": "IMPORTANT NOTICE: Many configuration settings including Service No, Activation No and Specific Bucket are linked exclusively to your account. Treat configuration settings as sensitive information.  From this point on you need your  Service No.  and  Activation No.  that we provided over email.   If you haven't emailed us with your Instance Connection Details, please follow  these steps.   Click  Configuration  and set:   Service No.:  36 digit hex number, it may look like this:  a1bcd23e-4fa5-67b8-cd9e-f0123abc4567  Activation No.:  24 digit hex number, it may look like this:  5990ab12c3de45f6a78bc90d  Engine:  Select  Firebird  or  SQL  (Microsoft SQL Server).   System:  Select  Custom...    If you use  Intelisis ERP ,  Microsip ERP  or  Aspel SAE  then select the System according.", 
            "title": "Configuration"
        }, 
        {
            "location": "/bipostexe/#firebird-connection", 
            "text": "Remote Connection:  Enable when you are using biPost.exe over a LAN network, this is, biPost.exe is not running on the same server as the Firebird server.    Server:  IP or name of the server on your LAN network.    Password:  Set your Firebird password, sometimes  masterkey    Database:  Location of your  .FDB  file.", 
            "title": "Firebird Connection"
        }, 
        {
            "location": "/bipostexe/#sql-connection", 
            "text": "Server:  IP or name of the server on your LAN network.    User:  Login for your SQL server. It only needs read permissions.    Password:  Password for the Login provided.    Database:  Name of your database.", 
            "title": "SQL Connection"
        }, 
        {
            "location": "/bipostexe/#general-settings", 
            "text": "Specific Bucket:  Enable to use your own AWS Account.   Enter your  Bucket Name  that we provided over email.     Include Catalogs:  For use only with  Intelisis ERP  system, see  details here.", 
            "title": "General Settings"
        }, 
        {
            "location": "/bipostexe/#recursive-sync", 
            "text": "Currently supported only for SQL Server, when enabled it optimizes upload by extracting and uploading one day at a time for the given date range.   It is very useful to upload historic data.  It is used in combination with  customData.json  so you can configure the date field to use for each table.  When  turned off  it automatically sets start and end date to yesterday.", 
            "title": "Recursive Sync"
        }, 
        {
            "location": "/bipostexe/#customdatajson", 
            "text": "This file allows you to specify the tables, fields and filter criteria to apply on the select statement that extracts data.  All parameters are supported for SQL Server. For Firebird SQL please review supported parameters  here.  Example 1, using recursiveDateField:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"Venta\",\n    \"fields\": \"Venta.ID, Venta.Empresa, Venta.Mov, Venta.MovID, Venta.FechaEmision, Venta.Concepto, Venta.Moneda, Venta.TipoCambio, Venta.Estatus, Venta.Cliente, Venta.Almacen, Venta.Agente, Venta.Condicion, Venta.Vencimiento, Venta.DescuentoLineal, Venta.Sucursal, Venta.SubModulo, Venta.Importe, Venta.Impuestos, Venta.CostoTotal, Venta.FechaRegistro, Venta.DescuentoGlobal, Venta.Proyecto\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"VentaD\",\n    \"fields\": \"VentaD.ID, VentaD.Renglon, VentaD.RenglonSub, VentaD.Articulo, VentaD.Cantidad, VentaD.Almacen, VentaD.Precio, VentaD.DescuentoLinea, VentaD.DescuentoImporte, VentaD.Impuesto1, VentaD.Costo, VentaD.ContUso, VentaD.Unidad, VentaD.Factor, VentaD.Agente \",\n    \"join\": \"Venta WITH (NOLOCK) ON Venta.ID = VentaD.ID JOIN MovTipo WITH (NOLOCK) ON Venta.Mov = MovTipo.Mov AND MovTipo.Modulo = 'VTAS' \",\n    \"filter\": \"Venta.Estatus NOT IN ('SINAFECTAR','BORRADOR') AND MovTipo.Clave IN ('VTAS.F','VTAS.D','VTAS.N','VTAS.FM','VTAS.FC') \",\n    \"recursiveDateField\": \"Venta.FechaEmision\"\n  }\n]  Example 2, for catalogs:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]   The above statements will send all data for the specified tables, as  join  and  filter  are not in use.  Note that you can use  \"active\": \"\",   Example 3, using special filter when no datetime is available:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"bibliaAlmacen\",\n    \"fields\": \"id, empresa, articulo, almacen, y, m, venta, devolucion, compra, devolucionCompra, trasladoRecepcion, trasladoSalida, otraEntrada, otraSalida, invInicial, inventario, valor, valorUSD, costo, costoUSD, costoVenta, costoVentaUSD, total1, peso, acum, abc, idym \",\n    \"join\": \"\",\n    \"filter\": \"y = datepart(yy,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) AND m = datepart(m,DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0))) \",\n    \"recursiveDateField\": \"\"\n  }\n]   join  and  filter  can use any syntax supported on SQL Server.   Example 4 with Microsip ERP:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"ATRIBUTO_ID, NOMBRE, NOMBRE_COLUMNA, CLAVE_OBJETO, POSICION, TIPO, LONGITUD, ESCALA, VALOR_MINIMO, VALOR_MAXIMO, VALOR_DEFAULT_NUMERICO, VALOR_DEFAULT_CARACTER, DESCRIPCION \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_ARTICULOS\",\n    \"fields\": \"IMPUESTO_ART_ID, ARTICULO_ID, IMPUESTO_ID, UNIDADES_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPUESTOS_DOCTOS_CM\",\n    \"fields\": \"DOCTO_CM_ID, IMPUESTO_ID, COMPRA_NETA, OTROS_IMPUESTOS, PCTJE_IMPUESTO, IMPORTE_IMPUESTO, UNIDADES_IMPUESTO, IMPORTE_UNITARIO_IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CC_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CC_IMPTO_ID, IMPTE_DOCTO_CC_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"IMPORTES_DOCTOS_CP_IMPTOS\",\n    \"fields\": \"IMPTE_DOCTO_CP_IMPTO_ID, IMPTE_DOCTO_CP_ID, IMPUESTO_ID, IMPORTE, PCTJE_IMPUESTO, IMPUESTO \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]   In the above example biPost.exe is using  System: Microsip  and on every sync it sends all the factory embedded tables plus the tables set in customData.json", 
            "title": "customData.json"
        }, 
        {
            "location": "/bipostexe/#tables-primary-keys", 
            "text": "Take note of the following:   Every table specified in  customData.json  must have a  Primary Key  and these must be listed on  \"fields\"  You can only setup customData.json with  base tables.  If you use views it may work but it is not supported.  \"fields\"  parameter must only include fields that exist on  \"table\": .", 
            "title": "Tables &amp; Primary Keys"
        }, 
        {
            "location": "/bipostexe/#recursivedatefield", 
            "text": "This parameter is used when  Recursive Sync  check box is enabled.  It only supports date fields without hour.  On SQL Server using  datetime  data type, it only supports dates ending in  00:00:00.000", 
            "title": "recursiveDateField"
        }, 
        {
            "location": "/bipostexe/#customdatajson-supported-parameters-for-firebird-sql", 
            "text": "active  table  fields   Rest of the parameters do not work for Firebird.", 
            "title": "customData.json supported parameters for Firebird SQL."
        }, 
        {
            "location": "/bipostexe/#handle-several-customdatajson-settings", 
            "text": "It is very common to make changes to customData.json to upload different sets of data.  Use Cases:    Some tables may be uploaded once since its data is rarely changed, e.g. config   company tables.    Historic data may be uploaded once, e.g. transactions from previous years.    Recently changed data may be uploaded monthly or daily, e.g. invoices, quotes, purchase orders, etc.    Recently created and updated catalogs may be uploaded monthly or daily, e.g. customers, items, vendors, etc.    For all the reasons above, it may be very useful to create two or more folders and copy entire biPost files and just change  customData.json.  Moreover you may want to have different sync schedules, which are explained next.", 
            "title": "Handle several customData.json settings"
        }, 
        {
            "location": "/bipostexe/#schedule", 
            "text": "If you want automated execution of biPost.exe, then set the  Hour  desired and click  Schedule .  This will create a Windows Task that will run daily. If you want a different schedule, then open  Windows Task Scheduler  as follows.  Control Panel \\ Administrative Tools:    If you manually create a task to run biPost then use  argument: post", 
            "title": "Schedule"
        }, 
        {
            "location": "/bipostexe/#check-for-updates", 
            "text": "New versions of biPost can be checked using  Help \\ Check for Updates.", 
            "title": "Check for Updates"
        }, 
        {
            "location": "/bipostexe/#handling-different-settings-with-folders", 
            "text": "We already explained why creating two or more folders is a good workaround for  handling different customData.json settings.  It is also possible that you may want to sync more than one database. For this matter proceed creating several folders so each one has it's own settings.", 
            "title": "Handling Different Settings with Folders"
        }, 
        {
            "location": "/bipostapi/", 
            "text": "Bipost API\n\n\nBipost API runs on AWS with a Lambda function.\nIt has 4 main stages which are described next.\n\n\nCreate Schemas\n\n\n\n\n\n\nIf it doesn't exist, database is created with:\n\n\n\n\n\n\nName: Over email we ask for this name for every \nService No.\n provided.\n\n\n\n\n\n\nEncoding: \ncp1252 West European (latin1)\n\n\n\n\n\n\nCollation: \nlatin1_spanish_ci\n\n\n\n\n\n\n\n\n\n\nTables are created with the full set of fields found on source db. \n\n\n\n\n\n\nFields will appear on a different position as the source db.\n\n\n\n\n\n\nOnly the fields specified in \ncustomData.json\n will be populated.\n\n\n\n\n\n\n\n\nInitial Statement\n\n\nAfter the schemas are created (or checked if they exist) and \nbefore\n new data is loaded, you can specify to run a query on your database.\n\n\nThis is very useful if you need to delete, truncate or make any changes before data is loaded.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostInitial\n and must not have parameters.\n\n\nPlease let us know over email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  TRUNCATE TABLE `mytesttable`;\n\nEND$$\n\n\n\n\n\nLoad Data\n\n\nData loading is performed by Aurora using \nLOAD DATA FROM S3\n statement with \nREPLACE\n parameter. This technique ensures that all data is loaded with great performance.\n\n\nYou can verify which tables where loaded by querying the \naurora_s3_load_history\n table like this:\n\n\nselect * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';\n\n\n\nOptionally convert \nload_timestamp\n to your local time: \nCONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')\n\n\nAfter data is loaded, it always checks to create and populate these objects:\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\n\n\n\n\n\n\n\n\nT\n\n\nTable\n\n\n\n\n\n\ntime\n\n\nTable\n\n\n\n\n\n\nym\n\n\nTable\n\n\n\n\n\n\ndowhile\n\n\nStored Procedure\n\n\n\n\n\n\nspCreateTime\n\n\nStored Procedure\n\n\n\n\n\n\nspCreateYM\n\n\nStored Procedure\n\n\n\n\n\n\nsp_createIndex\n\n\nStored Procedure\n\n\n\n\n\n\n\n\n\n\nFinal Statement\n\n\nAfter new data is loaded to your database, you can specify to run a query.\n\n\nThis is very handy if you need to execute several routines and, for example, populate new tables.\n\n\nCurrently you need to include all desired statements in a stored procedure with the name \nspPostFinal\n and must not have parameters. \n\n\nPlease let us know over email to activate this stored procedure for a given \nService No.\n\n\nExample:\n\n\nDELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  INSERT INTO mytable (message) VALUES ('my_message');\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\nEND$$", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#bipost-api", 
            "text": "Bipost API runs on AWS with a Lambda function.\nIt has 4 main stages which are described next.", 
            "title": "Bipost API"
        }, 
        {
            "location": "/bipostapi/#create-schemas", 
            "text": "If it doesn't exist, database is created with:    Name: Over email we ask for this name for every  Service No.  provided.    Encoding:  cp1252 West European (latin1)    Collation:  latin1_spanish_ci      Tables are created with the full set of fields found on source db.     Fields will appear on a different position as the source db.    Only the fields specified in  customData.json  will be populated.", 
            "title": "Create Schemas"
        }, 
        {
            "location": "/bipostapi/#initial-statement", 
            "text": "After the schemas are created (or checked if they exist) and  before  new data is loaded, you can specify to run a query on your database.  This is very useful if you need to delete, truncate or make any changes before data is loaded.  Currently you need to include all desired statements in a stored procedure with the name  spPostInitial  and must not have parameters.  Please let us know over email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostInitial`$$\nCREATE PROCEDURE `spPostInitial`()\npostinitial:BEGIN\n\n  TRUNCATE TABLE `mytesttable`;\n\nEND$$", 
            "title": "Initial Statement"
        }, 
        {
            "location": "/bipostapi/#load-data", 
            "text": "Data loading is performed by Aurora using  LOAD DATA FROM S3  statement with  REPLACE  parameter. This technique ensures that all data is loaded with great performance.  You can verify which tables where loaded by querying the  aurora_s3_load_history  table like this:  select * from mysql.aurora_s3_load_history where file_name regexp 'mytablename';  Optionally convert  load_timestamp  to your local time:  CONVERT_TZ(load_timestamp,'UTC','America/Mexico_City')  After data is loaded, it always checks to create and populate these objects:     Name  Type      T  Table    time  Table    ym  Table    dowhile  Stored Procedure    spCreateTime  Stored Procedure    spCreateYM  Stored Procedure    sp_createIndex  Stored Procedure", 
            "title": "Load Data"
        }, 
        {
            "location": "/bipostapi/#final-statement", 
            "text": "After new data is loaded to your database, you can specify to run a query.  This is very handy if you need to execute several routines and, for example, populate new tables.  Currently you need to include all desired statements in a stored procedure with the name  spPostFinal  and must not have parameters.   Please let us know over email to activate this stored procedure for a given  Service No.  Example:  DELIMITER $$\nDROP PROCEDURE IF EXISTS `spPostFinal`$$\nCREATE PROCEDURE `spPostFinal`()\npostfinal:BEGIN\n\n  INSERT INTO mytable (message) VALUES ('my_message');\n\n  REPLACE INTO ymInfo (tag, y, m)\n  SELECT 'current', YEAR(fnDateInfo('yesterday',fnServiceDate())), MONTH(fnDateInfo('yesterday',fnServiceDate()));\n\nEND$$", 
            "title": "Final Statement"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting\n\n\nWhen you use manual sync and it is completed successfully, the following dialog appears.\n\n\n\n\nIf you face any errors, try looking here.\n\n\n\n\nCheck Log\n\n\nAfter you make a sync, you are able to check on you PC the compressed file uploaded.\n\n\n\n\n\n\nOpen a new Windows Explorer window and enter \n%LocalAppData%\\biPost\n\n\n\n\n\n\nOpen corresponding ZIP file and review contents.\n\n\n\n\n\n\nIt might me also useful to check after a \nSync Completed\n process, and \nbefore\n you press \nOK\n, open \n%LocalAppData%\\biPost\n and check the files that were just created.\n\n\nIf you need to check which files where loaded to Aurora, check \naurora_s3_load_history.\n\n\n\n\ncustomData.json syntax error\n\n\nMisspelled fields or tables on customData.json may appear as:\n\n\n\n\n\n\nDate format inside post.json\n\n\nIf immediately after launching biPost.exe this error appears \nString was not recognized as valid DateTime\n, try the following:\n\n\nOpen \npost.json\n file and delete all datetime strings found inside \n\" \"\n. Do not delete the double quotation marks.\n\n\n\n\n\n\nNo information to Sync\n\n\nIf \nNo information to Sync\n message appears, verify that customData.json is set to send at least one table.\n\n\n\n\nTime to sync and load to Aurora\n\n\nWhen a new Sync is initiated on biPost.exe, it may take a few minutes to extract, compress and upload to information to S3. While this is happening, no messages/icons will show that biPost.exe is working and maybe you will see \n(Not responding)\n on the top of the window, this is normal.\n\n\nIf you launch Windows Task Manager probably you'll see that \nbiPost.exe *32\n is running and consuming a considerable amount of CPU.\n\n\nOnce the compressed package it uploaded to S3, it may take 1 to 3 minutes to process and load your data to MySQL.\n\n\nIf you need to check which tables where loaded, check \naurora_s3_load_history.\n\n\n\n\nUpload Limit\n\n\nDepending on the number of columns on each table and the amount of data of each row, it is possible that a large amount of data sent on a single sync may not load on your Aurora DB.\n\n\nWe have tested different scenarios and a 100,000 row limit for a single sync works fine.\n\n\nWe recommend using \nRecursive Sync\n for large tables that have a datetime field available.\n\n\n\n\nSpecial Characters\n\n\nSome special characters on \nchar\n and \nvarchar\n fields are not supported and thus removed by biPost.exe\nFor example:\n\n\n\n\nEnter\n\n\n()\n\n\n\n\nOn \nSQL Server\n, all special characters on strings of 100 length or more are removed, leaving only letters and numbers.\n\n\n\n\nSchema Limitations\n\n\n\n\nNULL\n values on char and varchar are converted to \n''\n on MySQL.\n\n\nNULL\n values on float, money and int datatypes are converted to value \n0\n (zero) on MySQL.\n\n\nNULL\n values on datetime are converted to \n0000-00-00 00:00:00\n on MySQL.\n\n\nbit\n datatype is converted to \nVARCHAR(1)\n on MySQL.\n\n\nTables without a \nPRIMARY KEY\n will not load data on Aurora.\n\n\n\n\n\n\nNo Internet Connection\n\n\nIf there is no internet connection available, biPost will show the following message:\n\n\n\n\n\n\nFirewall Restrictions\n\n\nIf your internet connection has a firewall, it may show different errors like: \n\n\n\n\nThe remote name could not be resolved.\n\n\nA WebException with status SendFailure was thrown.\n\n\nA WebException with status NameResolutionFailure was thrown.\n\n\nError making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.\n\n\n\n\n\n\nGrant Firewall to reach Amazon S3:\n\n\nCreate a policy to Allow to:\n\n\n\n\n54.230.0.0/15\n\n\n52.192.0.0/11\n\n\n\n\n\n\n\n\nAppData\\Local folder\n\n\nSometimes it is necessary to manually delete the content of \n\\AppData\\Local\\biPost\n folder.\n\n\nOpen a new Windows Explorer window and enter \n%LocalAppData%\\biPost\n. Select all and delete.\n\n\n************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "When you use manual sync and it is completed successfully, the following dialog appears.   If you face any errors, try looking here.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#check-log", 
            "text": "After you make a sync, you are able to check on you PC the compressed file uploaded.    Open a new Windows Explorer window and enter  %LocalAppData%\\biPost    Open corresponding ZIP file and review contents.    It might me also useful to check after a  Sync Completed  process, and  before  you press  OK , open  %LocalAppData%\\biPost  and check the files that were just created.  If you need to check which files where loaded to Aurora, check  aurora_s3_load_history.", 
            "title": "Check Log"
        }, 
        {
            "location": "/troubleshooting/#customdatajson-syntax-error", 
            "text": "Misspelled fields or tables on customData.json may appear as:", 
            "title": "customData.json syntax error"
        }, 
        {
            "location": "/troubleshooting/#date-format-inside-postjson", 
            "text": "If immediately after launching biPost.exe this error appears  String was not recognized as valid DateTime , try the following:  Open  post.json  file and delete all datetime strings found inside  \" \" . Do not delete the double quotation marks.", 
            "title": "Date format inside post.json"
        }, 
        {
            "location": "/troubleshooting/#no-information-to-sync", 
            "text": "If  No information to Sync  message appears, verify that customData.json is set to send at least one table.", 
            "title": "No information to Sync"
        }, 
        {
            "location": "/troubleshooting/#time-to-sync-and-load-to-aurora", 
            "text": "When a new Sync is initiated on biPost.exe, it may take a few minutes to extract, compress and upload to information to S3. While this is happening, no messages/icons will show that biPost.exe is working and maybe you will see  (Not responding)  on the top of the window, this is normal.  If you launch Windows Task Manager probably you'll see that  biPost.exe *32  is running and consuming a considerable amount of CPU.  Once the compressed package it uploaded to S3, it may take 1 to 3 minutes to process and load your data to MySQL.  If you need to check which tables where loaded, check  aurora_s3_load_history.", 
            "title": "Time to sync and load to Aurora"
        }, 
        {
            "location": "/troubleshooting/#upload-limit", 
            "text": "Depending on the number of columns on each table and the amount of data of each row, it is possible that a large amount of data sent on a single sync may not load on your Aurora DB.  We have tested different scenarios and a 100,000 row limit for a single sync works fine.  We recommend using  Recursive Sync  for large tables that have a datetime field available.", 
            "title": "Upload Limit"
        }, 
        {
            "location": "/troubleshooting/#special-characters", 
            "text": "Some special characters on  char  and  varchar  fields are not supported and thus removed by biPost.exe\nFor example:   Enter  ()   On  SQL Server , all special characters on strings of 100 length or more are removed, leaving only letters and numbers.", 
            "title": "Special Characters"
        }, 
        {
            "location": "/troubleshooting/#schema-limitations", 
            "text": "NULL  values on char and varchar are converted to  ''  on MySQL.  NULL  values on float, money and int datatypes are converted to value  0  (zero) on MySQL.  NULL  values on datetime are converted to  0000-00-00 00:00:00  on MySQL.  bit  datatype is converted to  VARCHAR(1)  on MySQL.  Tables without a  PRIMARY KEY  will not load data on Aurora.", 
            "title": "Schema Limitations"
        }, 
        {
            "location": "/troubleshooting/#no-internet-connection", 
            "text": "If there is no internet connection available, biPost will show the following message:", 
            "title": "No Internet Connection"
        }, 
        {
            "location": "/troubleshooting/#firewall-restrictions", 
            "text": "If your internet connection has a firewall, it may show different errors like:    The remote name could not be resolved.  A WebException with status SendFailure was thrown.  A WebException with status NameResolutionFailure was thrown.  Error making request with Error Code ExpectationFailed and Http Status Code ExpectationFailed.", 
            "title": "Firewall Restrictions"
        }, 
        {
            "location": "/troubleshooting/#grant-firewall-to-reach-amazon-s3", 
            "text": "Create a policy to Allow to:   54.230.0.0/15  52.192.0.0/11", 
            "title": "Grant Firewall to reach Amazon S3:"
        }, 
        {
            "location": "/troubleshooting/#appdatalocal-folder", 
            "text": "Sometimes it is necessary to manually delete the content of  \\AppData\\Local\\biPost  folder.  Open a new Windows Explorer window and enter  %LocalAppData%\\biPost . Select all and delete.  ************** Exception Text **************\nSystem.IO.IOException: The process cannot access the file '012a3b4c-56d7-8ef9-0123-456789a012bc_post.zip' because it is being used by another process.", 
            "title": "AppData\\Local folder"
        }, 
        {
            "location": "/businessintelligence/", 
            "text": "Business Intelligence Use Case\n\n\nAt \nFactor BI\n we use Bipost to power small and medium companies with B.I. tools such as \nGoogle Data Studio\n and \nMicrosoft Power BI.\n\n\nHere is an example of the architected solution.\n\n\n\n\nIn this example some dashboards are built with Microsoft Power BI because it has a great native mobile app for \nAndroid\n and \niPhone\n, so it is used by high level executives on the go.\n\n\nGoogle Data Studio is used for desktop users and detailed analytical situations, because it has great date \n string filters, excellent UX, and dashboards are shared using \nGoogle accounts,\n which is a great way to avoid managing user creation, password reset, etc.", 
            "title": "Business Intelligence"
        }, 
        {
            "location": "/businessintelligence/#business-intelligence-use-case", 
            "text": "At  Factor BI  we use Bipost to power small and medium companies with B.I. tools such as  Google Data Studio  and  Microsoft Power BI.  Here is an example of the architected solution.   In this example some dashboards are built with Microsoft Power BI because it has a great native mobile app for  Android  and  iPhone , so it is used by high level executives on the go.  Google Data Studio is used for desktop users and detailed analytical situations, because it has great date   string filters, excellent UX, and dashboards are shared using  Google accounts,  which is a great way to avoid managing user creation, password reset, etc.", 
            "title": "Business Intelligence Use Case"
        }, 
        {
            "location": "/intelisis/", 
            "text": "Intelisis ERP\n\n\nIntelisis es un ERP que fabrica la empresa \nIntelisis Software, S.A. de C.V.\n\n\nAl ser \u00e9ste un sistema grande, las soluciones de Business Intelligence se desarrollan a la medida de cada necesidad.\n\n\nEn nuestro \nrepositorio de GitHub\n existen proyectos de B.I. para otros sistemas y se pueden usar como gu\u00eda de referencia y adaptarse. Estos objetos se proveen bajo la licencia \nGNU GPLv3\n GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.\n\n\n\n\nTablas Intelisis\n\n\nAl configurar biPost.exe con \nSystem: Intelisis\n, de f\u00e1brica se incluye la tabla \nVenta\n en la sincronizaci\u00f3n.\n\n\n\n\nCuando se incluye el check \nInclude Catalogs,\n de f\u00e1brica se incluen las siguientes tablas:\n\n\nAgente\nAlm\nArt\nCondicion\nCta\nCte\nEmpresa\nEmpresaCfg\nEmpresaCfg2\nEmpresaGral\nMon\nMonHist\nProv\nSucursal\nUEN\nVersion\n\n\n\n\n\nRecomendaciones\n\n\nCrear dos carpetas\n\n\nCrea siempre dos carpetas de Bipost, esto te permitir\u00e1 sincronizar cat\u00e1logos y movimientos en forma independiente.\n\n\nCarpeta Cat\u00e1logos\n\n\nTu primer carpeta donde est\u00e1 Bipost la puedes nombrar \nbiPost_catalogos\n y utilizar\u00e1s \ncustomData.json\n para enviar \u00fanicamente cat\u00e1logos. Este archivo puede verse de la siguiente forma:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\nAbre biPost.exe y aseg\u00farate de quitar el check \nRecursive Sync\n.\n\n\nUsando el JSON del ejemplo anterior, se enviar\u00e1 en cada sincronizaci\u00f3n \ntodos los renglones\n de las tablas \nMovTipo\n, \nCta\n y \nCentroCostos\n. Debido a que Bipost API utiliza una \nsentencia \nREPLACE\n en la base destino de MySQL, los registros se reemplazan utilizando la llave primaria de la tabla origen.\n\n\nNOTA:\n Si la tabla que vas a enviar tiene m\u00e1s de 100,000 registros, es recomendable que filtres el query utilizando la opci\u00f3n \n\"filter\":\n.\n\n\nCarpeta Recursivo\n\n\nGenera una copia de la carpeta \nbiPost_catalogos\n, puedes nombrarla \nbiPost_recursivo\n. Abre el archivo \ncustomData.json\n y especifica las tablas de movimientos que vas a enviar, por ejemplo:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"Cont\",\n    \"fields\": \"Cont.ID, Cont.Empresa, Cont.Mov, Cont.MovID, Cont.FechaEmision, Cont.FechaContable, Cont.Proyecto, Cont.Moneda, Cont.TipoCambio, Cont.Estatus, Cont.Ejercicio, Cont.Periodo, Cont.Moneda2, Cont.TipoCambio2\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus \n 'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"ContD\",\n    \"fields\": \"ContD.ID, ContD.Renglon, ContD.RenglonSub, ContD.Cuenta, ContD.SubCuenta, ContD.SubCuenta2, ContD.SubCuenta3, ContD.Debe, ContD.Debe2, ContD.Haber, ContD.Haber2, ContD.Sucursal, ContD.SucursalContable \",\n    \"join\": \"Cont WITH (NOLOCK) ON Cont.ID = ContD.ID JOIN MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus \n 'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  }\n]\n\n\n\nEs importante que incluyas el par\u00e1metro \n\"recursiveDateField\":\n ya que lo utiliza Bipost para optimizar la extracci\u00f3n y subida de informaci\u00f3n, por ejemplo: Supongamos que la tabla ContD tiene millones de registros, por tanto al momento de configurar \n\"recursiveDateField\":\n lo utilizar\u00e1s en conjunto con el check \nRecursive Sync\n, ubicado en la pesta\u00f1a \nGeneral Settings\n de biPost.exe:\n\n\n\n\nEl rango de fechas que especifiques en \nGeneral Settings\n se utilizar\u00e1 para generar una extracci\u00f3n y subida individual al bucket de S3 por cada, reduciendo el tama\u00f1o de los datos y el tiempo de carga hacia MySQL.\n\n\nMediante esta opci\u00f3n se pueden cargar hist\u00f3ricos de varios a\u00f1os.\n\n\nUso del Filter\n\n\nEn el ejemplo customData.json anterior, se utiliz\u00f3 el siguiente filtro para la tabla \nContD\n:\n\n\n\"filter\": \"Cont.Estatus \n 'SINAFECTAR'\n\n\nSi un registro del m\u00f3dulo Contabilidad se cancela, cambia su estatus a \nCANCELADO\n por tanto el query a la base de datos incluye los estatus \nCANCELADO\n para que estos cambios se vean reflejados en la base de MySQL.\n\n\nSi por el contrario se utiliza el filtro \n\"Cont.Estatus = 'CONCLUIDO'\n entonces el query omite los estatus \nCANCELADO\n y al cambiar un ID a estatus \nCANCELADO\n no se reflejar\u00e1 el cambio en la base de MySQL, causando inconsistencias.", 
            "title": "Intelisis"
        }, 
        {
            "location": "/intelisis/#intelisis-erp", 
            "text": "Intelisis es un ERP que fabrica la empresa  Intelisis Software, S.A. de C.V.  Al ser \u00e9ste un sistema grande, las soluciones de Business Intelligence se desarrollan a la medida de cada necesidad.  En nuestro  repositorio de GitHub  existen proyectos de B.I. para otros sistemas y se pueden usar como gu\u00eda de referencia y adaptarse. Estos objetos se proveen bajo la licencia  GNU GPLv3  GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.", 
            "title": "Intelisis ERP"
        }, 
        {
            "location": "/intelisis/#tablas-intelisis", 
            "text": "Al configurar biPost.exe con  System: Intelisis , de f\u00e1brica se incluye la tabla  Venta  en la sincronizaci\u00f3n.   Cuando se incluye el check  Include Catalogs,  de f\u00e1brica se incluen las siguientes tablas:  Agente\nAlm\nArt\nCondicion\nCta\nCte\nEmpresa\nEmpresaCfg\nEmpresaCfg2\nEmpresaGral\nMon\nMonHist\nProv\nSucursal\nUEN\nVersion", 
            "title": "Tablas Intelisis"
        }, 
        {
            "location": "/intelisis/#recomendaciones", 
            "text": "", 
            "title": "Recomendaciones"
        }, 
        {
            "location": "/intelisis/#crear-dos-carpetas", 
            "text": "Crea siempre dos carpetas de Bipost, esto te permitir\u00e1 sincronizar cat\u00e1logos y movimientos en forma independiente.", 
            "title": "Crear dos carpetas"
        }, 
        {
            "location": "/intelisis/#carpeta-catalogos", 
            "text": "Tu primer carpeta donde est\u00e1 Bipost la puedes nombrar  biPost_catalogos  y utilizar\u00e1s  customData.json  para enviar \u00fanicamente cat\u00e1logos. Este archivo puede verse de la siguiente forma:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"MovTipo\",\n    \"fields\": \"Modulo, Mov, Clave \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"Cta\",\n    \"fields\": \"Cuenta, Rama, Descripcion, Tipo, EsAcreedora, EsAcumulativa, CentrosCostos, Estatus, ClaveSAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"CentroCostos\",\n    \"fields\": \"CentroCostos, Rama, Descripcion, EsAcumulativo, Grupo, SubGrupo, SubSubGrupo, Estatus \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]  Abre biPost.exe y aseg\u00farate de quitar el check  Recursive Sync .  Usando el JSON del ejemplo anterior, se enviar\u00e1 en cada sincronizaci\u00f3n  todos los renglones  de las tablas  MovTipo ,  Cta  y  CentroCostos . Debido a que Bipost API utiliza una  sentencia  REPLACE  en la base destino de MySQL, los registros se reemplazan utilizando la llave primaria de la tabla origen.  NOTA:  Si la tabla que vas a enviar tiene m\u00e1s de 100,000 registros, es recomendable que filtres el query utilizando la opci\u00f3n  \"filter\": .", 
            "title": "Carpeta Cat\u00e1logos"
        }, 
        {
            "location": "/intelisis/#carpeta-recursivo", 
            "text": "Genera una copia de la carpeta  biPost_catalogos , puedes nombrarla  biPost_recursivo . Abre el archivo  customData.json  y especifica las tablas de movimientos que vas a enviar, por ejemplo:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"Cont\",\n    \"fields\": \"Cont.ID, Cont.Empresa, Cont.Mov, Cont.MovID, Cont.FechaEmision, Cont.FechaContable, Cont.Proyecto, Cont.Moneda, Cont.TipoCambio, Cont.Estatus, Cont.Ejercicio, Cont.Periodo, Cont.Moneda2, Cont.TipoCambio2\",\n    \"join\": \"MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus   'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"ContD\",\n    \"fields\": \"ContD.ID, ContD.Renglon, ContD.RenglonSub, ContD.Cuenta, ContD.SubCuenta, ContD.SubCuenta2, ContD.SubCuenta3, ContD.Debe, ContD.Debe2, ContD.Haber, ContD.Haber2, ContD.Sucursal, ContD.SucursalContable \",\n    \"join\": \"Cont WITH (NOLOCK) ON Cont.ID = ContD.ID JOIN MovTipo WITH (NOLOCK) ON Cont.Mov = MovTipo.Mov AND MovTipo.Modulo = 'CONT' \",\n    \"filter\": \"Cont.Estatus   'SINAFECTAR' AND MovTipo.clave IN ('CONT.P','CONT.C') \",\n    \"recursiveDateField\": \"Cont.FechaEmision\"\n  }\n]  Es importante que incluyas el par\u00e1metro  \"recursiveDateField\":  ya que lo utiliza Bipost para optimizar la extracci\u00f3n y subida de informaci\u00f3n, por ejemplo: Supongamos que la tabla ContD tiene millones de registros, por tanto al momento de configurar  \"recursiveDateField\":  lo utilizar\u00e1s en conjunto con el check  Recursive Sync , ubicado en la pesta\u00f1a  General Settings  de biPost.exe:   El rango de fechas que especifiques en  General Settings  se utilizar\u00e1 para generar una extracci\u00f3n y subida individual al bucket de S3 por cada, reduciendo el tama\u00f1o de los datos y el tiempo de carga hacia MySQL.  Mediante esta opci\u00f3n se pueden cargar hist\u00f3ricos de varios a\u00f1os.", 
            "title": "Carpeta Recursivo"
        }, 
        {
            "location": "/intelisis/#uso-del-filter", 
            "text": "En el ejemplo customData.json anterior, se utiliz\u00f3 el siguiente filtro para la tabla  ContD :  \"filter\": \"Cont.Estatus   'SINAFECTAR'  Si un registro del m\u00f3dulo Contabilidad se cancela, cambia su estatus a  CANCELADO  por tanto el query a la base de datos incluye los estatus  CANCELADO  para que estos cambios se vean reflejados en la base de MySQL.  Si por el contrario se utiliza el filtro  \"Cont.Estatus = 'CONCLUIDO'  entonces el query omite los estatus  CANCELADO  y al cambiar un ID a estatus  CANCELADO  no se reflejar\u00e1 el cambio en la base de MySQL, causando inconsistencias.", 
            "title": "Uso del Filter"
        }, 
        {
            "location": "/microsip/", 
            "text": "Microsip ERP\n\n\nMicrosip es un sistema administrativo ERP que fabrica la empresa \nAplicaciones y Proyectos Computacionales S.A. de C.V.\n\n\nFactor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.\n\n\nEn nuestro \nrepositorio de GitHub\n podr\u00e1s encontrar estos objetos bajo la licencia \nGNU GPLv3\n GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.\n\n\n\n\nTablas Microsip\n\n\n\n\nAl configurar biPost.exe con \nSystem: Microsip\n, de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:\n\n\nagentes\nalmacenes\nanticipos_cc\narticulos\natributos\nbancos\nbeneficiarios\ncajas\ncajeros\ncentros_costo\nciudades\nclaves_articulos\nclientes\ncobradores\ncomprom_articulos\nconceptos_ba\nconceptos_cc\nconceptos_cp\nconceptos_in\ncondiciones_pago\ncondiciones_pago_cp\ncuentas_bancarias\ncuentas_co\ndepositos_cc\ndepositos_cc_det\ndeptos_co\ndirs_clientes\ndoctos_ba\ndoctos_cc\ndoctos_cm\ndoctos_cm_det\ndoctos_cp\ndoctos_in\ndoctos_in_det\ndoctos_ve\ndoctos_ve_det\nestados\nexis_discretos\nformas_cobro_cc\nformas_cobro_doctos\ngrupos_lineas\nhistoria_cambiaria\nimportes_doctos_cc\nimportes_doctos_cc_imptos\nimportes_doctos_cp\nimportes_doctos_cp_imptos\nimpuestos\nimpuestos_articulos\nimpuestos_doctos_cm\nimpuestos_doctos_ve\nlibres_articulos\nlibres_cargos_cc\nlibres_cargos_cp\nlibres_com_cm\nlibres_cot_ve\nlibres_creditos_cc\nlibres_creditos_cp\nlibres_ctas_ban\nlibres_cuentas_co\nlibres_devcom_cm\nlibres_devfac_ve\nlibres_fac_ve\nlibres_ped_ve\nlibres_pol_co\nlineas_articulos\nmonedas\npaises\nplazos_cond_pag\nplazos_cond_pag_cp\npoliticas_comisiones_vendedores\nproveedores\nroles_claves_articulos\nsaldos_ba\nsaldos_cc\nsaldos_co\nsaldos_cp\nsaldos_in\nsucursales\ntipos_clientes\ntipos_impuestos\ntipos_polizas\ntipos_prov\ntraspasos_ba\nusos_anticipos_cc\nvencimientos_cargos_cc\nvencimientos_cargos_cm\nvencimientos_cargos_cp\nvencimientos_cargos_ve\nvendedores\nvias_embarque\nzonas_clientes\n\n\n\nEn el siguiente link est\u00e1 el listado de tablas de Microsip.\n\n\nTablas Microsip, archivo Google Sheets.\n\n\nNOTA:\n La lista del link anterior puede no estar completa. \n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nA\u00f1adir tablas a la Sincronizaci\u00f3n\n\n\nPara incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo \ncustomData.json\n, por ejemplo:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"ATRIBUTO_ID, NOMBRE, NOMBRE_COLUMNA, CLAVE_OBJETO, POSICION, TIPO, LONGITUD, ESCALA, VALOR_MINIMO, VALOR_MAXIMO, VALOR_DEFAULT_NUMERICO, VALOR_DEFAULT_CARACTER, DESCRIPCION \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"UNIDADES_VENTA\",\n    \"fields\": \"UNIDAD_VENTA_ID, UNIDAD_VENTA, CLAVE_SAT, SIMBOLO_SAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\nLos par\u00e1metros \njoin\n, \nfilter\n y \nrecursiveDateField\n no est\u00e1n soportados hasta el momento para Firebird SQL.\n\n\nObservar que puede estar \n\"active\": \"\"\n.\n\n\nPara m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver \naqu\u00ed.\n\n\n\n\nTablas B.I.\n\n\nAl correr los scripts del \nrepositorio de GitHub,\n se crean las siguientes tablas.\n\n\n\n\n\n\n\n\nTabla\n\n\nDescripci\u00f3n\n\n\n\n\n\n\n\n\n\n\nT\n\n\nTiraje de numeraci\u00f3n consecutiva.\n\n\n\n\n\n\ntime\n\n\nUtil para usarse con dimensiones de tiempo en el B.I.\n\n\n\n\n\n\nym\n\n\nUtil para usarse con dimensiones a\u00f1o y mes en el B.I.\n\n\n\n\n\n\nbiabc\n\n\nClasificaci\u00f3n ABC de los inventarios.\n\n\n\n\n\n\nbiblia\n\n\nMatriz de estad\u00edsticas de inventario por mes.\n\n\n\n\n\n\nmodulosgrupo1\n\n\nUtil para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.\n\n\n\n\n\n\nbusinessDay\n\n\nD\u00edas laborales de la empresa.\n\n\n\n\n\n\ncustomDates\n\n\nPasa una fecha espec\u00edfica a las funciones \nfnSyncDate()\n, \nfnAgingDate\n y \nfnServiceDate\n\n\n\n\n\n\ndateInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nymInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nsyncInfo\n\n\nContiene informaci\u00f3n de la sincronizaci\u00f3n.\n\n\n\n\n\n\nlogPostInitial\n\n\nLog de sincronizaciones, se ejecuta al inicio (antes de cargar la data).\n\n\n\n\n\n\nlogPostFinal\n\n\nLog de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).\n\n\n\n\n\n\nmatrixrep1\n\n\nEstructura matricial \u00fatil para reportes en Google Data Studio.\n\n\n\n\n\n\ncontagrupo1\n\n\nResumen contable por a\u00f1o y mes.\n\n\n\n\n\n\ncontagrupo2\n\n\nResumen contable por fecha.\n\n\n\n\n\n\neqtabla\n\n\nTabla para agregar equivalencias.", 
            "title": "Microsip"
        }, 
        {
            "location": "/microsip/#microsip-erp", 
            "text": "Microsip es un sistema administrativo ERP que fabrica la empresa  Aplicaciones y Proyectos Computacionales S.A. de C.V.  Factor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.  En nuestro  repositorio de GitHub  podr\u00e1s encontrar estos objetos bajo la licencia  GNU GPLv3  GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.", 
            "title": "Microsip ERP"
        }, 
        {
            "location": "/microsip/#tablas-microsip", 
            "text": "Al configurar biPost.exe con  System: Microsip , de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:  agentes\nalmacenes\nanticipos_cc\narticulos\natributos\nbancos\nbeneficiarios\ncajas\ncajeros\ncentros_costo\nciudades\nclaves_articulos\nclientes\ncobradores\ncomprom_articulos\nconceptos_ba\nconceptos_cc\nconceptos_cp\nconceptos_in\ncondiciones_pago\ncondiciones_pago_cp\ncuentas_bancarias\ncuentas_co\ndepositos_cc\ndepositos_cc_det\ndeptos_co\ndirs_clientes\ndoctos_ba\ndoctos_cc\ndoctos_cm\ndoctos_cm_det\ndoctos_cp\ndoctos_in\ndoctos_in_det\ndoctos_ve\ndoctos_ve_det\nestados\nexis_discretos\nformas_cobro_cc\nformas_cobro_doctos\ngrupos_lineas\nhistoria_cambiaria\nimportes_doctos_cc\nimportes_doctos_cc_imptos\nimportes_doctos_cp\nimportes_doctos_cp_imptos\nimpuestos\nimpuestos_articulos\nimpuestos_doctos_cm\nimpuestos_doctos_ve\nlibres_articulos\nlibres_cargos_cc\nlibres_cargos_cp\nlibres_com_cm\nlibres_cot_ve\nlibres_creditos_cc\nlibres_creditos_cp\nlibres_ctas_ban\nlibres_cuentas_co\nlibres_devcom_cm\nlibres_devfac_ve\nlibres_fac_ve\nlibres_ped_ve\nlibres_pol_co\nlineas_articulos\nmonedas\npaises\nplazos_cond_pag\nplazos_cond_pag_cp\npoliticas_comisiones_vendedores\nproveedores\nroles_claves_articulos\nsaldos_ba\nsaldos_cc\nsaldos_co\nsaldos_cp\nsaldos_in\nsucursales\ntipos_clientes\ntipos_impuestos\ntipos_polizas\ntipos_prov\ntraspasos_ba\nusos_anticipos_cc\nvencimientos_cargos_cc\nvencimientos_cargos_cm\nvencimientos_cargos_cp\nvencimientos_cargos_ve\nvendedores\nvias_embarque\nzonas_clientes  En el siguiente link est\u00e1 el listado de tablas de Microsip.  Tablas Microsip, archivo Google Sheets.  NOTA:  La lista del link anterior puede no estar completa.   Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Tablas Microsip"
        }, 
        {
            "location": "/microsip/#anadir-tablas-a-la-sincronizacion", 
            "text": "Para incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo  customData.json , por ejemplo:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"ATRIBUTOS\",\n    \"fields\": \"ATRIBUTO_ID, NOMBRE, NOMBRE_COLUMNA, CLAVE_OBJETO, POSICION, TIPO, LONGITUD, ESCALA, VALOR_MINIMO, VALOR_MAXIMO, VALOR_DEFAULT_NUMERICO, VALOR_DEFAULT_CARACTER, DESCRIPCION \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"UNIDADES_VENTA\",\n    \"fields\": \"UNIDAD_VENTA_ID, UNIDAD_VENTA, CLAVE_SAT, SIMBOLO_SAT \",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"\",\n    \"table\": \"\",\n    \"fields\": \"\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]  Los par\u00e1metros  join ,  filter  y  recursiveDateField  no est\u00e1n soportados hasta el momento para Firebird SQL.  Observar que puede estar  \"active\": \"\" .  Para m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver  aqu\u00ed.", 
            "title": "A\u00f1adir tablas a la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/microsip/#tablas-bi", 
            "text": "Al correr los scripts del  repositorio de GitHub,  se crean las siguientes tablas.     Tabla  Descripci\u00f3n      T  Tiraje de numeraci\u00f3n consecutiva.    time  Util para usarse con dimensiones de tiempo en el B.I.    ym  Util para usarse con dimensiones a\u00f1o y mes en el B.I.    biabc  Clasificaci\u00f3n ABC de los inventarios.    biblia  Matriz de estad\u00edsticas de inventario por mes.    modulosgrupo1  Util para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.    businessDay  D\u00edas laborales de la empresa.    customDates  Pasa una fecha espec\u00edfica a las funciones  fnSyncDate() ,  fnAgingDate  y  fnServiceDate    dateInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    ymInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    syncInfo  Contiene informaci\u00f3n de la sincronizaci\u00f3n.    logPostInitial  Log de sincronizaciones, se ejecuta al inicio (antes de cargar la data).    logPostFinal  Log de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).    matrixrep1  Estructura matricial \u00fatil para reportes en Google Data Studio.    contagrupo1  Resumen contable por a\u00f1o y mes.    contagrupo2  Resumen contable por fecha.    eqtabla  Tabla para agregar equivalencias.", 
            "title": "Tablas B.I."
        }, 
        {
            "location": "/aspel/", 
            "text": "Aspel SAE\n\n\nAspel SAE es un sistema administrativo que fabrica la empresa \nAspel de M\u00e9xico, S.A. de C.V.\n\n\nFactor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.\n\n\nEn nuestro \nrepositorio de GitHub\n podr\u00e1s encontrar estos objetos bajo la licencia \nGNU GPLv3\n GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.\n\n\n\n\nTablas Aspel\n\n\n\n\nAl configurar biPost.exe con \nSystem: SAE\n, de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:\n\n\nalmacenes\nclie\nclin\ncolor\ncompc\ncompd\ncompo\nconc\nconm\nconp\ncuen_det\ncuen_m\nfactd\nfactf\nfactp\ninve\nminve\nmoned\npaga_det\npaga_m\npar_compc\npar_compd\npar_compo\npar_factd\npar_factf\npar_factp\nprov\nprvprod\ntalla\nvend\n\n\n\nEn la base de datos del sistema Aspel SAE cada tabla tiene un sufijo \n01\n, \n02\n, etc., de acuerdo a la empresa que se ha creado. Por ejemplo la tabla de las facturas podr\u00edas encontrarla como \nfactf01\n y para otra raz\u00f3n social como \nfactf02\n.\n\n\nBipost autom\u00e1ticamente elimina estos sufijos por lo que las tablas en MySQL ser\u00e1n creadas sin el sufijo. Esta funcionalidad aplica incluso cuando se a\u00f1aden tablas con \ncustomData.json\n.\n\n\nEn el siguiente link est\u00e1 el listado de tablas de Aspel SAE.\n\n\nTablas Aspel SAE, archivo Google Sheets.\n\n\nNOTA:\n La lista del link anterior puede no estar completa. \n\n\nPara obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:\n\n\nselect rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;\n\n\n\n\n\nA\u00f1adir tablas a la Sincronizaci\u00f3n\n\n\nPara incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo \ncustomData.json\n, por ejemplo:\n\n\n[\n  {\n    \"active\": \"true\",\n    \"table\": \"CLIE_CLIB\",\n    \"fields\": \"CVE_CLIE, CAMPLIB1, CAMPLIB2, CAMPLIB3, CAMPLIB4, CAMPLIB5, CAMPLIB6\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"AFACT\",\n    \"fields\": \"CVE_AFACT, FVTA_COM, FDESCTO, FDES_FIN, FIMP, FCOMI, RVTA_COM, RDESCTO, RDES_FIN, RIMP, RCOMI, DVTA_COM, DDESCTO, DDES_FIN, DIMP, DCOMI, PVTA_COM, PDESCTO, PDES_FIN, PIMP, PCOMI, CVTA_COM, CDESCTO, CDES_FIN, CIMP, CCOMI, VVTA_COM, VDESCTO, VDES_FIN, VIMP, VCOMI, WVTA_COM, WDESCTO, WDES_FIN, WIMP, WCOMI, PER_ACUM\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]\n\n\n\nNotar que no se incluye el sufijo \n01\n, \n02\n, etc. de las tablas.\n\n\nLos par\u00e1metros \njoin\n, \nfilter\n y \nrecursiveDateField\n no est\u00e1n soportados hasta el momento para Firebird SQL.\n\n\nObservar que puede estar \n\"active\": \"\"\n.\n\n\nPara m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver \naqu\u00ed.\n\n\n\n\nTablas B.I.\n\n\nAl correr los scripts del \nrepositorio de GitHub,\n se crean las siguientes tablas.\n\n\n\n\n\n\n\n\nTabla\n\n\nDescripci\u00f3n\n\n\n\n\n\n\n\n\n\n\nT\n\n\nTiraje de numeraci\u00f3n consecutiva.\n\n\n\n\n\n\ntime\n\n\nUtil para usarse con dimensiones de tiempo en el B.I.\n\n\n\n\n\n\nym\n\n\nUtil para usarse con dimensiones a\u00f1o y mes en el B.I.\n\n\n\n\n\n\nbiabc\n\n\nClasificaci\u00f3n ABC de los inventarios.\n\n\n\n\n\n\nbiblia\n\n\nMatriz de estad\u00edsticas de inventario por mes.\n\n\n\n\n\n\nmodulosgrupo1\n\n\nUtil para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.\n\n\n\n\n\n\nbusinessDay\n\n\nD\u00edas laborales de la empresa.\n\n\n\n\n\n\ncustomDates\n\n\nPasa una fecha espec\u00edfica a las funciones \nfnSyncDate()\n, \nfnAgingDate\n y \nfnServiceDate\n\n\n\n\n\n\ndateInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nymInfo\n\n\nSe llena al ejecutar \nspPostFinal\n y se utiliza en diferentes vistas.\n\n\n\n\n\n\nsyncInfo\n\n\nContiene informaci\u00f3n de la sincronizaci\u00f3n.\n\n\n\n\n\n\nlogPostInitial\n\n\nLog de sincronizaciones, se ejecuta al inicio (antes de cargar la data).\n\n\n\n\n\n\nlogPostFinal\n\n\nLog de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).\n\n\n\n\n\n\nmatrixrep1\n\n\nEstructura matricial \u00fatil para reportes en Google Data Studio.\n\n\n\n\n\n\ncontagrupo1\n\n\nResumen contable por a\u00f1o y mes.\n\n\n\n\n\n\ncontagrupo2\n\n\nResumen contable por fecha.\n\n\n\n\n\n\neqtabla\n\n\nTabla para agregar equivalencias.", 
            "title": "Aspel"
        }, 
        {
            "location": "/aspel/#aspel-sae", 
            "text": "Aspel SAE es un sistema administrativo que fabrica la empresa  Aspel de M\u00e9xico, S.A. de C.V.  Factor BI ofrece una serie de objetos en MySQL que facilitan la creaci\u00f3n de un Business Intelligence.  En nuestro  repositorio de GitHub  podr\u00e1s encontrar estos objetos bajo la licencia  GNU GPLv3  GNU GENERAL PUBLIC LICENSE Versi\u00f3n 3, 29 de Junio 2007.", 
            "title": "Aspel SAE"
        }, 
        {
            "location": "/aspel/#tablas-aspel", 
            "text": "Al configurar biPost.exe con  System: SAE , de f\u00e1brica se incluye un listado de tablas en la sincronizaci\u00f3n, las cuales son:  almacenes\nclie\nclin\ncolor\ncompc\ncompd\ncompo\nconc\nconm\nconp\ncuen_det\ncuen_m\nfactd\nfactf\nfactp\ninve\nminve\nmoned\npaga_det\npaga_m\npar_compc\npar_compd\npar_compo\npar_factd\npar_factf\npar_factp\nprov\nprvprod\ntalla\nvend  En la base de datos del sistema Aspel SAE cada tabla tiene un sufijo  01 ,  02 , etc., de acuerdo a la empresa que se ha creado. Por ejemplo la tabla de las facturas podr\u00edas encontrarla como  factf01  y para otra raz\u00f3n social como  factf02 .  Bipost autom\u00e1ticamente elimina estos sufijos por lo que las tablas en MySQL ser\u00e1n creadas sin el sufijo. Esta funcionalidad aplica incluso cuando se a\u00f1aden tablas con  customData.json .  En el siguiente link est\u00e1 el listado de tablas de Aspel SAE.  Tablas Aspel SAE, archivo Google Sheets.  NOTA:  La lista del link anterior puede no estar completa.   Para obtener el listado completo de tablas de acuerdo a tu base de datos, puedes usar el siguiente query en Firebird:  select rdb$relation_name\nfrom rdb$relations\nwhere rdb$view_blr is null\nand (rdb$system_flag is null or rdb$system_flag = 0)\norder by rdb$relation_name;", 
            "title": "Tablas Aspel"
        }, 
        {
            "location": "/aspel/#anadir-tablas-a-la-sincronizacion", 
            "text": "Para incluir tablas adicionales en la sincronizaci\u00f3n, se utiliza el archivo  customData.json , por ejemplo:  [\n  {\n    \"active\": \"true\",\n    \"table\": \"CLIE_CLIB\",\n    \"fields\": \"CVE_CLIE, CAMPLIB1, CAMPLIB2, CAMPLIB3, CAMPLIB4, CAMPLIB5, CAMPLIB6\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  },\n  {\n    \"active\": \"true\",\n    \"table\": \"AFACT\",\n    \"fields\": \"CVE_AFACT, FVTA_COM, FDESCTO, FDES_FIN, FIMP, FCOMI, RVTA_COM, RDESCTO, RDES_FIN, RIMP, RCOMI, DVTA_COM, DDESCTO, DDES_FIN, DIMP, DCOMI, PVTA_COM, PDESCTO, PDES_FIN, PIMP, PCOMI, CVTA_COM, CDESCTO, CDES_FIN, CIMP, CCOMI, VVTA_COM, VDESCTO, VDES_FIN, VIMP, VCOMI, WVTA_COM, WDESCTO, WDES_FIN, WIMP, WCOMI, PER_ACUM\",\n    \"join\": \"\",\n    \"filter\": \"\",\n    \"recursiveDateField\": \"\"\n  }\n]  Notar que no se incluye el sufijo  01 ,  02 , etc. de las tablas.  Los par\u00e1metros  join ,  filter  y  recursiveDateField  no est\u00e1n soportados hasta el momento para Firebird SQL.  Observar que puede estar  \"active\": \"\" .  Para m\u00e1s informaci\u00f3n sobre la configuraci\u00f3n de customData.json ver  aqu\u00ed.", 
            "title": "A\u00f1adir tablas a la Sincronizaci\u00f3n"
        }, 
        {
            "location": "/aspel/#tablas-bi", 
            "text": "Al correr los scripts del  repositorio de GitHub,  se crean las siguientes tablas.     Tabla  Descripci\u00f3n      T  Tiraje de numeraci\u00f3n consecutiva.    time  Util para usarse con dimensiones de tiempo en el B.I.    ym  Util para usarse con dimensiones a\u00f1o y mes en el B.I.    biabc  Clasificaci\u00f3n ABC de los inventarios.    biblia  Matriz de estad\u00edsticas de inventario por mes.    modulosgrupo1  Util para un flujo de efectivo. Contiene un resumen de varios m\u00f3dulos del sistema.    businessDay  D\u00edas laborales de la empresa.    customDates  Pasa una fecha espec\u00edfica a las funciones  fnSyncDate() ,  fnAgingDate  y  fnServiceDate    dateInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    ymInfo  Se llena al ejecutar  spPostFinal  y se utiliza en diferentes vistas.    syncInfo  Contiene informaci\u00f3n de la sincronizaci\u00f3n.    logPostInitial  Log de sincronizaciones, se ejecuta al inicio (antes de cargar la data).    logPostFinal  Log de sincronizaciones, se ejecuta al final (despu\u00e9s de cargar la data).    matrixrep1  Estructura matricial \u00fatil para reportes en Google Data Studio.    contagrupo1  Resumen contable por a\u00f1o y mes.    contagrupo2  Resumen contable por fecha.    eqtabla  Tabla para agregar equivalencias.", 
            "title": "Tablas B.I."
        }, 
        {
            "location": "/about/", 
            "text": "History\n\n\nBipost was once designed to ease the use of \nCloud Business Intelligence\n tools.\n\n\nOver time, we started using Bipost as an extension of on-premises systems (like ERP's) to AWS cloud. Using MySQL Aurora and other AWS services, we build tailored made enterprise web applications.\n\n\n\n\nEnterprise Applications\n\n\nWe build tailored made enterprise web applications with integration to on-premises Relational Database Systems (RDBMS) using Bipost.\n\n\nWe use \nForm.io\n along with other technologies.\n\n\n\n\n\n\nFeedback\n\n\nWe are always happy to hear about our users.\n\n\nPlease send us an email to: \njaime@factorbi.com", 
            "title": "About"
        }, 
        {
            "location": "/about/#history", 
            "text": "Bipost was once designed to ease the use of  Cloud Business Intelligence  tools.  Over time, we started using Bipost as an extension of on-premises systems (like ERP's) to AWS cloud. Using MySQL Aurora and other AWS services, we build tailored made enterprise web applications.", 
            "title": "History"
        }, 
        {
            "location": "/about/#enterprise-applications", 
            "text": "We build tailored made enterprise web applications with integration to on-premises Relational Database Systems (RDBMS) using Bipost.  We use  Form.io  along with other technologies.", 
            "title": "Enterprise Applications"
        }, 
        {
            "location": "/about/#feedback", 
            "text": "We are always happy to hear about our users.  Please send us an email to:  jaime@factorbi.com", 
            "title": "Feedback"
        }
    ]
}